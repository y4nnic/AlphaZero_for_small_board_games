{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "from pipeline import Pipeline, agent_vs_player, agent_vs_agent\n",
    "import memory\n",
    "import model\n",
    "import agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TicTacToe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "az_pipeline = Pipeline(\"tictactoe_50\", \"TicTacToe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 | self-play\n",
      "saving memory position_memory_tictactoe_50_ep_0\n",
      "iteration 0 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 3, 3, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 9)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 4s 954us/step - loss: 6.7259 - value_loss: 1.0025 - policy_loss: 2.3650 - val_loss: 6.6363 - val_value_loss: 0.9450 - val_policy_loss: 2.2437\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.5432 - value_loss: 0.8365 - policy_loss: 2.1662 - val_loss: 6.6084 - val_value_loss: 0.9956 - val_policy_loss: 2.1381\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.4468 - value_loss: 0.7436 - policy_loss: 2.0670 - val_loss: 6.5070 - val_value_loss: 0.8648 - val_policy_loss: 2.0666\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.4142 - value_loss: 0.7481 - policy_loss: 1.9981 - val_loss: 6.4332 - val_value_loss: 0.7682 - val_policy_loss: 2.0162\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.3568 - value_loss: 0.6836 - policy_loss: 1.9485 - val_loss: 6.4511 - val_value_loss: 0.8441 - val_policy_loss: 1.9768\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.3472 - value_loss: 0.7038 - policy_loss: 1.9096 - val_loss: 6.4514 - val_value_loss: 0.8763 - val_policy_loss: 1.9459\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.3020 - value_loss: 0.6450 - policy_loss: 1.8785 - val_loss: 6.3807 - val_value_loss: 0.7609 - val_policy_loss: 1.9205\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.2615 - value_loss: 0.5902 - policy_loss: 1.8530 - val_loss: 6.4833 - val_value_loss: 0.9884 - val_policy_loss: 1.8988\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.2823 - value_loss: 0.6523 - policy_loss: 1.8331 - val_loss: 6.3711 - val_value_loss: 0.7821 - val_policy_loss: 1.8814\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.2371 - value_loss: 0.5801 - policy_loss: 1.8156 - val_loss: 6.3603 - val_value_loss: 0.7769 - val_policy_loss: 1.8655\n",
      "Saved model  tictactoe_50_0\n",
      "iteration 0 | evaluation\n",
      "agent vs random - win ratio 0.62 - draw ratio 0.06\n",
      "Number of seen trajectories: 100\n",
      "Number of unique trajectories: 100\n",
      "iteration 1 | self-play\n",
      "saving memory position_memory_tictactoe_50_ep_1\n",
      "iteration 1 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 3, 3, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 9)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 1s 176us/step - loss: 6.3661 - value_loss: 0.8077 - policy_loss: 1.8467 - val_loss: 6.3212 - val_value_loss: 0.7223 - val_policy_loss: 1.8428\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.3244 - value_loss: 0.7485 - policy_loss: 1.8231 - val_loss: 6.2959 - val_value_loss: 0.6880 - val_policy_loss: 1.8270\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.3002 - value_loss: 0.7188 - policy_loss: 1.8051 - val_loss: 6.2910 - val_value_loss: 0.6923 - val_policy_loss: 1.8135\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.2402 - value_loss: 0.6149 - policy_loss: 1.7896 - val_loss: 6.2983 - val_value_loss: 0.7200 - val_policy_loss: 1.8012\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.2544 - value_loss: 0.6566 - policy_loss: 1.7769 - val_loss: 6.2647 - val_value_loss: 0.6622 - val_policy_loss: 1.7922\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.2289 - value_loss: 0.6170 - policy_loss: 1.7662 - val_loss: 6.3007 - val_value_loss: 0.7431 - val_policy_loss: 1.7841\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.2108 - value_loss: 0.5930 - policy_loss: 1.7547 - val_loss: 6.2715 - val_value_loss: 0.6939 - val_policy_loss: 1.7756\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.2230 - value_loss: 0.6266 - policy_loss: 1.7462 - val_loss: 6.2526 - val_value_loss: 0.6629 - val_policy_loss: 1.7692\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1902 - value_loss: 0.5699 - policy_loss: 1.7378 - val_loss: 6.2553 - val_value_loss: 0.6759 - val_policy_loss: 1.7625\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1537 - value_loss: 0.5041 - policy_loss: 1.7313 - val_loss: 6.2471 - val_value_loss: 0.6672 - val_policy_loss: 1.7554\n",
      "Saved model  tictactoe_50_1\n",
      "iteration 1 | evaluation\n",
      "agent vs random - win ratio 0.69 - draw ratio 0.05\n",
      "Number of seen trajectories: 200\n",
      "Number of unique trajectories: 200\n",
      "iteration 2 | self-play\n",
      "saving memory position_memory_tictactoe_50_ep_2\n",
      "iteration 2 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 3, 3, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 9)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 1s 177us/step - loss: 6.2900 - value_loss: 0.7457 - policy_loss: 1.7629 - val_loss: 6.2947 - val_value_loss: 0.7686 - val_policy_loss: 1.7498\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.2533 - value_loss: 0.6855 - policy_loss: 1.7503 - val_loss: 6.2808 - val_value_loss: 0.7484 - val_policy_loss: 1.7429\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.2228 - value_loss: 0.6351 - policy_loss: 1.7405 - val_loss: 6.2657 - val_value_loss: 0.7245 - val_policy_loss: 1.7372\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.2116 - value_loss: 0.6212 - policy_loss: 1.7325 - val_loss: 6.2871 - val_value_loss: 0.7725 - val_policy_loss: 1.7326\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.2239 - value_loss: 0.6537 - policy_loss: 1.7252 - val_loss: 6.2559 - val_value_loss: 0.7163 - val_policy_loss: 1.7270\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1866 - value_loss: 0.5857 - policy_loss: 1.7192 - val_loss: 6.2630 - val_value_loss: 0.7351 - val_policy_loss: 1.7232\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1674 - value_loss: 0.5538 - policy_loss: 1.7134 - val_loss: 6.3065 - val_value_loss: 0.8257 - val_policy_loss: 1.7202\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1578 - value_loss: 0.5406 - policy_loss: 1.7082 - val_loss: 6.2412 - val_value_loss: 0.7005 - val_policy_loss: 1.7154\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1423 - value_loss: 0.5158 - policy_loss: 1.7026 - val_loss: 6.2407 - val_value_loss: 0.7034 - val_policy_loss: 1.7121\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1417 - value_loss: 0.5196 - policy_loss: 1.6982 - val_loss: 6.2459 - val_value_loss: 0.7172 - val_policy_loss: 1.7093\n",
      "Saved model  tictactoe_50_2\n",
      "iteration 2 | evaluation\n",
      "agent vs random - win ratio 0.7 - draw ratio 0.04\n",
      "Number of seen trajectories: 300\n",
      "Number of unique trajectories: 299\n",
      "iteration 3 | self-play\n",
      "saving memory position_memory_tictactoe_50_ep_3\n",
      "iteration 3 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 3, 3, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 9)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 1s 176us/step - loss: 6.2659 - value_loss: 0.7422 - policy_loss: 1.7246 - val_loss: 6.2355 - val_value_loss: 0.7047 - val_policy_loss: 1.7016\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.2140 - value_loss: 0.6475 - policy_loss: 1.7162 - val_loss: 6.2147 - val_value_loss: 0.6685 - val_policy_loss: 1.6969\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1793 - value_loss: 0.5848 - policy_loss: 1.7101 - val_loss: 6.2057 - val_value_loss: 0.6546 - val_policy_loss: 1.6934\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1787 - value_loss: 0.5887 - policy_loss: 1.7057 - val_loss: 6.2052 - val_value_loss: 0.6584 - val_policy_loss: 1.6893\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1742 - value_loss: 0.5855 - policy_loss: 1.7005 - val_loss: 6.2095 - val_value_loss: 0.6695 - val_policy_loss: 1.6874\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1493 - value_loss: 0.5405 - policy_loss: 1.6963 - val_loss: 6.2089 - val_value_loss: 0.6720 - val_policy_loss: 1.6843\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1490 - value_loss: 0.5436 - policy_loss: 1.6932 - val_loss: 6.2013 - val_value_loss: 0.6601 - val_policy_loss: 1.6817\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1283 - value_loss: 0.5068 - policy_loss: 1.6893 - val_loss: 6.1927 - val_value_loss: 0.6463 - val_policy_loss: 1.6790\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1211 - value_loss: 0.4962 - policy_loss: 1.6860 - val_loss: 6.1809 - val_value_loss: 0.6250 - val_policy_loss: 1.6772\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1118 - value_loss: 0.4811 - policy_loss: 1.6834 - val_loss: 6.1803 - val_value_loss: 0.6262 - val_policy_loss: 1.6756\n",
      "Saved model  tictactoe_50_3\n",
      "iteration 3 | evaluation\n",
      "agent vs random - win ratio 0.72 - draw ratio 0.06\n",
      "Number of seen trajectories: 400\n",
      "Number of unique trajectories: 397\n",
      "iteration 4 | self-play\n",
      "saving memory position_memory_tictactoe_50_ep_4\n",
      "iteration 4 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 3, 3, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 9)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 1s 176us/step - loss: 6.1995 - value_loss: 0.6245 - policy_loss: 1.7158 - val_loss: 6.1950 - val_value_loss: 0.6205 - val_policy_loss: 1.7111\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1636 - value_loss: 0.5597 - policy_loss: 1.7096 - val_loss: 6.1822 - val_value_loss: 0.5989 - val_policy_loss: 1.7079\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1422 - value_loss: 0.5227 - policy_loss: 1.7043 - val_loss: 6.1770 - val_value_loss: 0.5909 - val_policy_loss: 1.7061\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1302 - value_loss: 0.5032 - policy_loss: 1.7006 - val_loss: 6.1942 - val_value_loss: 0.6280 - val_policy_loss: 1.7042\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1110 - value_loss: 0.4694 - policy_loss: 1.6965 - val_loss: 6.1752 - val_value_loss: 0.5920 - val_policy_loss: 1.7027\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1127 - value_loss: 0.4770 - policy_loss: 1.6930 - val_loss: 6.1907 - val_value_loss: 0.6254 - val_policy_loss: 1.7009\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1013 - value_loss: 0.4576 - policy_loss: 1.6903 - val_loss: 6.1794 - val_value_loss: 0.6049 - val_policy_loss: 1.6995\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.0899 - value_loss: 0.4383 - policy_loss: 1.6873 - val_loss: 6.1741 - val_value_loss: 0.5970 - val_policy_loss: 1.6974\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.0845 - value_loss: 0.4311 - policy_loss: 1.6845 - val_loss: 6.1663 - val_value_loss: 0.5832 - val_policy_loss: 1.6962\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.0851 - value_loss: 0.4352 - policy_loss: 1.6823 - val_loss: 6.1638 - val_value_loss: 0.5803 - val_policy_loss: 1.6949\n",
      "Saved model  tictactoe_50_4\n",
      "iteration 4 | evaluation\n",
      "agent vs random - win ratio 0.72 - draw ratio 0.08\n",
      "Number of seen trajectories: 500\n",
      "Number of unique trajectories: 494\n",
      "iteration 5 | self-play\n",
      "saving memory position_memory_tictactoe_50_ep_5\n",
      "iteration 5 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 3, 3, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 9)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 1s 176us/step - loss: 6.2290 - value_loss: 0.6699 - policy_loss: 1.7357 - val_loss: 6.2101 - val_value_loss: 0.6577 - val_policy_loss: 1.7104\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.2024 - value_loss: 0.6208 - policy_loss: 1.7319 - val_loss: 6.2065 - val_value_loss: 0.6515 - val_policy_loss: 1.7097\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1870 - value_loss: 0.5928 - policy_loss: 1.7295 - val_loss: 6.2060 - val_value_loss: 0.6518 - val_policy_loss: 1.7087\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1752 - value_loss: 0.5719 - policy_loss: 1.7272 - val_loss: 6.2044 - val_value_loss: 0.6494 - val_policy_loss: 1.7081\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1661 - value_loss: 0.5566 - policy_loss: 1.7246 - val_loss: 6.2034 - val_value_loss: 0.6487 - val_policy_loss: 1.7072\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1587 - value_loss: 0.5441 - policy_loss: 1.7226 - val_loss: 6.2028 - val_value_loss: 0.6485 - val_policy_loss: 1.7065\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1509 - value_loss: 0.5299 - policy_loss: 1.7214 - val_loss: 6.2007 - val_value_loss: 0.6457 - val_policy_loss: 1.7055\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1469 - value_loss: 0.5234 - policy_loss: 1.7202 - val_loss: 6.2007 - val_value_loss: 0.6461 - val_policy_loss: 1.7054\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1400 - value_loss: 0.5123 - policy_loss: 1.7180 - val_loss: 6.1990 - val_value_loss: 0.6438 - val_policy_loss: 1.7046\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1375 - value_loss: 0.5086 - policy_loss: 1.7170 - val_loss: 6.1964 - val_value_loss: 0.6394 - val_policy_loss: 1.7042\n",
      "Saved model  tictactoe_50_5\n",
      "iteration 5 | evaluation\n",
      "agent vs random - win ratio 0.75 - draw ratio 0.04\n",
      "Number of seen trajectories: 600\n",
      "Number of unique trajectories: 591\n",
      "iteration 6 | self-play\n",
      "saving memory position_memory_tictactoe_50_ep_6\n",
      "iteration 6 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 3, 3, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 9)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 1s 178us/step - loss: 6.2036 - value_loss: 0.6552 - policy_loss: 1.7029 - val_loss: 6.1990 - val_value_loss: 0.6401 - val_policy_loss: 1.7090\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1830 - value_loss: 0.6178 - policy_loss: 1.6994 - val_loss: 6.1950 - val_value_loss: 0.6335 - val_policy_loss: 1.7078\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1712 - value_loss: 0.5969 - policy_loss: 1.6971 - val_loss: 6.1912 - val_value_loss: 0.6276 - val_policy_loss: 1.7064\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1586 - value_loss: 0.5745 - policy_loss: 1.6945 - val_loss: 6.1890 - val_value_loss: 0.6243 - val_policy_loss: 1.7057\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1506 - value_loss: 0.5603 - policy_loss: 1.6930 - val_loss: 6.1914 - val_value_loss: 0.6307 - val_policy_loss: 1.7045\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1444 - value_loss: 0.5500 - policy_loss: 1.6912 - val_loss: 6.1895 - val_value_loss: 0.6280 - val_policy_loss: 1.7036\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1376 - value_loss: 0.5384 - policy_loss: 1.6897 - val_loss: 6.1840 - val_value_loss: 0.6182 - val_policy_loss: 1.7028\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1368 - value_loss: 0.5378 - policy_loss: 1.6889 - val_loss: 6.1879 - val_value_loss: 0.6272 - val_policy_loss: 1.7019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1268 - value_loss: 0.5202 - policy_loss: 1.6867 - val_loss: 6.1964 - val_value_loss: 0.6451 - val_policy_loss: 1.7013\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1233 - value_loss: 0.5153 - policy_loss: 1.6850 - val_loss: 6.1897 - val_value_loss: 0.6328 - val_policy_loss: 1.7005\n",
      "Saved model  tictactoe_50_6\n",
      "iteration 6 | evaluation\n",
      "agent vs random - win ratio 0.76 - draw ratio 0.02\n",
      "Number of seen trajectories: 700\n",
      "Number of unique trajectories: 687\n",
      "iteration 7 | self-play\n",
      "saving memory position_memory_tictactoe_50_ep_7\n",
      "iteration 7 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 3, 3, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 9)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 1s 178us/step - loss: 6.2041 - value_loss: 0.6696 - policy_loss: 1.6926 - val_loss: 6.1438 - val_value_loss: 0.5739 - val_policy_loss: 1.6679\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1810 - value_loss: 0.6267 - policy_loss: 1.6896 - val_loss: 6.1371 - val_value_loss: 0.5616 - val_policy_loss: 1.6671\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1684 - value_loss: 0.6043 - policy_loss: 1.6871 - val_loss: 6.1337 - val_value_loss: 0.5560 - val_policy_loss: 1.6664\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1537 - value_loss: 0.5777 - policy_loss: 1.6847 - val_loss: 6.1366 - val_value_loss: 0.5631 - val_policy_loss: 1.6654\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1449 - value_loss: 0.5619 - policy_loss: 1.6831 - val_loss: 6.1346 - val_value_loss: 0.5600 - val_policy_loss: 1.6647\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1378 - value_loss: 0.5500 - policy_loss: 1.6813 - val_loss: 6.1409 - val_value_loss: 0.5735 - val_policy_loss: 1.6640\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1299 - value_loss: 0.5361 - policy_loss: 1.6797 - val_loss: 6.1394 - val_value_loss: 0.5716 - val_policy_loss: 1.6633\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1260 - value_loss: 0.5296 - policy_loss: 1.6786 - val_loss: 6.1305 - val_value_loss: 0.5548 - val_policy_loss: 1.6627\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1226 - value_loss: 0.5241 - policy_loss: 1.6777 - val_loss: 6.1322 - val_value_loss: 0.5593 - val_policy_loss: 1.6619\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1180 - value_loss: 0.5171 - policy_loss: 1.6759 - val_loss: 6.1308 - val_value_loss: 0.5574 - val_policy_loss: 1.6614\n",
      "Saved model  tictactoe_50_7\n",
      "iteration 7 | evaluation\n",
      "agent vs random - win ratio 0.77 - draw ratio 0.02\n",
      "Number of seen trajectories: 800\n",
      "Number of unique trajectories: 780\n",
      "iteration 8 | self-play\n",
      "saving memory position_memory_tictactoe_50_ep_8\n",
      "iteration 8 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 3, 3, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 9)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 1s 173us/step - loss: 6.2080 - value_loss: 0.7130 - policy_loss: 1.6602 - val_loss: 6.1879 - val_value_loss: 0.6803 - val_policy_loss: 1.6528\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1851 - value_loss: 0.6700 - policy_loss: 1.6578 - val_loss: 6.1799 - val_value_loss: 0.6661 - val_policy_loss: 1.6515\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1689 - value_loss: 0.6402 - policy_loss: 1.6555 - val_loss: 6.1848 - val_value_loss: 0.6769 - val_policy_loss: 1.6508\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1544 - value_loss: 0.6135 - policy_loss: 1.6534 - val_loss: 6.1774 - val_value_loss: 0.6632 - val_policy_loss: 1.6500\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1468 - value_loss: 0.5996 - policy_loss: 1.6525 - val_loss: 6.1739 - val_value_loss: 0.6568 - val_policy_loss: 1.6497\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1366 - value_loss: 0.5815 - policy_loss: 1.6504 - val_loss: 6.1725 - val_value_loss: 0.6551 - val_policy_loss: 1.6489\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1349 - value_loss: 0.5798 - policy_loss: 1.6492 - val_loss: 6.1820 - val_value_loss: 0.6752 - val_policy_loss: 1.6482\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1241 - value_loss: 0.5596 - policy_loss: 1.6481 - val_loss: 6.1674 - val_value_loss: 0.6468 - val_policy_loss: 1.6478\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1204 - value_loss: 0.5546 - policy_loss: 1.6461 - val_loss: 6.1683 - val_value_loss: 0.6495 - val_policy_loss: 1.6470\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1161 - value_loss: 0.5472 - policy_loss: 1.6450 - val_loss: 6.1658 - val_value_loss: 0.6453 - val_policy_loss: 1.6466\n",
      "Saved model  tictactoe_50_8\n",
      "iteration 8 | evaluation\n",
      "agent vs random - win ratio 0.83 - draw ratio 0.01\n",
      "Number of seen trajectories: 900\n",
      "Number of unique trajectories: 872\n",
      "iteration 9 | self-play\n",
      "saving memory position_memory_tictactoe_50_ep_9\n",
      "iteration 9 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 3, 3, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 9)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 1s 177us/step - loss: 6.1983 - value_loss: 0.6691 - policy_loss: 1.6880 - val_loss: 6.2069 - val_value_loss: 0.7083 - val_policy_loss: 1.6661\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1783 - value_loss: 0.6319 - policy_loss: 1.6854 - val_loss: 6.1968 - val_value_loss: 0.6897 - val_policy_loss: 1.6648\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1626 - value_loss: 0.6030 - policy_loss: 1.6832 - val_loss: 6.1937 - val_value_loss: 0.6847 - val_policy_loss: 1.6640\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1519 - value_loss: 0.5837 - policy_loss: 1.6815 - val_loss: 6.1904 - val_value_loss: 0.6790 - val_policy_loss: 1.6633\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1439 - value_loss: 0.5694 - policy_loss: 1.6802 - val_loss: 6.1890 - val_value_loss: 0.6775 - val_policy_loss: 1.6625\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1355 - value_loss: 0.5545 - policy_loss: 1.6786 - val_loss: 6.1846 - val_value_loss: 0.6696 - val_policy_loss: 1.6619\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1297 - value_loss: 0.5440 - policy_loss: 1.6777 - val_loss: 6.1853 - val_value_loss: 0.6721 - val_policy_loss: 1.6611\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1225 - value_loss: 0.5313 - policy_loss: 1.6764 - val_loss: 6.1840 - val_value_loss: 0.6707 - val_policy_loss: 1.6603\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1201 - value_loss: 0.5278 - policy_loss: 1.6754 - val_loss: 6.1840 - val_value_loss: 0.6712 - val_policy_loss: 1.6599\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1147 - value_loss: 0.5183 - policy_loss: 1.6744 - val_loss: 6.1835 - val_value_loss: 0.6712 - val_policy_loss: 1.6593\n",
      "Saved model  tictactoe_50_9\n",
      "iteration 9 | evaluation\n",
      "agent vs random - win ratio 0.86 - draw ratio 0.01\n",
      "Number of seen trajectories: 1000\n",
      "Number of unique trajectories: 958\n",
      "iteration 10 | self-play\n",
      "saving memory position_memory_tictactoe_50_ep_10\n",
      "iteration 10 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 3, 3, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 9)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 1s 177us/step - loss: 6.1772 - value_loss: 0.6409 - policy_loss: 1.6771 - val_loss: 6.1484 - val_value_loss: 0.6062 - val_policy_loss: 1.6543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1652 - value_loss: 0.6179 - policy_loss: 1.6762 - val_loss: 6.1428 - val_value_loss: 0.5960 - val_policy_loss: 1.6535\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1541 - value_loss: 0.5980 - policy_loss: 1.6740 - val_loss: 6.1396 - val_value_loss: 0.5902 - val_policy_loss: 1.6529\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1467 - value_loss: 0.5841 - policy_loss: 1.6733 - val_loss: 6.1384 - val_value_loss: 0.5886 - val_policy_loss: 1.6524\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1403 - value_loss: 0.5725 - policy_loss: 1.6723 - val_loss: 6.1368 - val_value_loss: 0.5860 - val_policy_loss: 1.6519\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1355 - value_loss: 0.5635 - policy_loss: 1.6718 - val_loss: 6.1354 - val_value_loss: 0.5840 - val_policy_loss: 1.6513\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1306 - value_loss: 0.5552 - policy_loss: 1.6704 - val_loss: 6.1340 - val_value_loss: 0.5817 - val_policy_loss: 1.6509\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1266 - value_loss: 0.5479 - policy_loss: 1.6700 - val_loss: 6.1334 - val_value_loss: 0.5811 - val_policy_loss: 1.6505\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1224 - value_loss: 0.5403 - policy_loss: 1.6693 - val_loss: 6.1325 - val_value_loss: 0.5798 - val_policy_loss: 1.6501\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1187 - value_loss: 0.5338 - policy_loss: 1.6687 - val_loss: 6.1351 - val_value_loss: 0.5857 - val_policy_loss: 1.6497\n",
      "Saved model  tictactoe_50_10\n",
      "iteration 10 | evaluation\n",
      "agent vs random - win ratio 0.85 - draw ratio 0.04\n",
      "Number of seen trajectories: 1100\n",
      "Number of unique trajectories: 1044\n",
      "iteration 11 | self-play\n",
      "saving memory position_memory_tictactoe_50_ep_11\n",
      "iteration 11 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 3, 3, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 9)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 1s 175us/step - loss: 6.1845 - value_loss: 0.6611 - policy_loss: 1.6730 - val_loss: 6.1889 - val_value_loss: 0.6770 - val_policy_loss: 1.6661\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1697 - value_loss: 0.6327 - policy_loss: 1.6721 - val_loss: 6.1845 - val_value_loss: 0.6690 - val_policy_loss: 1.6653\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1587 - value_loss: 0.6123 - policy_loss: 1.6706 - val_loss: 6.1809 - val_value_loss: 0.6627 - val_policy_loss: 1.6648\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1499 - value_loss: 0.5954 - policy_loss: 1.6700 - val_loss: 6.1789 - val_value_loss: 0.6592 - val_policy_loss: 1.6643\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1418 - value_loss: 0.5808 - policy_loss: 1.6687 - val_loss: 6.1768 - val_value_loss: 0.6556 - val_policy_loss: 1.6639\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1371 - value_loss: 0.5716 - policy_loss: 1.6685 - val_loss: 6.1754 - val_value_loss: 0.6533 - val_policy_loss: 1.6636\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1320 - value_loss: 0.5629 - policy_loss: 1.6673 - val_loss: 6.1739 - val_value_loss: 0.6508 - val_policy_loss: 1.6632\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1269 - value_loss: 0.5532 - policy_loss: 1.6669 - val_loss: 6.1733 - val_value_loss: 0.6501 - val_policy_loss: 1.6629\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1229 - value_loss: 0.5464 - policy_loss: 1.6658 - val_loss: 6.1717 - val_value_loss: 0.6474 - val_policy_loss: 1.6625\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1191 - value_loss: 0.5394 - policy_loss: 1.6653 - val_loss: 6.1714 - val_value_loss: 0.6472 - val_policy_loss: 1.6623\n",
      "Saved model  tictactoe_50_11\n",
      "iteration 11 | evaluation\n",
      "agent vs random - win ratio 0.81 - draw ratio 0.02\n",
      "Number of seen trajectories: 1200\n",
      "Number of unique trajectories: 1138\n",
      "iteration 12 | self-play\n",
      "saving memory position_memory_tictactoe_50_ep_12\n",
      "iteration 12 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 3, 3, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 9)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 1s 176us/step - loss: 6.2011 - value_loss: 0.6952 - policy_loss: 1.6738 - val_loss: 6.2348 - val_value_loss: 0.7424 - val_policy_loss: 1.6940\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1881 - value_loss: 0.6697 - policy_loss: 1.6735 - val_loss: 6.2318 - val_value_loss: 0.7374 - val_policy_loss: 1.6932\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1779 - value_loss: 0.6507 - policy_loss: 1.6722 - val_loss: 6.2296 - val_value_loss: 0.7338 - val_policy_loss: 1.6926\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1697 - value_loss: 0.6363 - policy_loss: 1.6704 - val_loss: 6.2278 - val_value_loss: 0.7309 - val_policy_loss: 1.6921\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1635 - value_loss: 0.6247 - policy_loss: 1.6697 - val_loss: 6.2297 - val_value_loss: 0.7353 - val_policy_loss: 1.6916\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1590 - value_loss: 0.6162 - policy_loss: 1.6694 - val_loss: 6.2276 - val_value_loss: 0.7315 - val_policy_loss: 1.6913\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1540 - value_loss: 0.6077 - policy_loss: 1.6681 - val_loss: 6.2251 - val_value_loss: 0.7270 - val_policy_loss: 1.6910\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1494 - value_loss: 0.5993 - policy_loss: 1.6673 - val_loss: 6.2253 - val_value_loss: 0.7281 - val_policy_loss: 1.6905\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1462 - value_loss: 0.5935 - policy_loss: 1.6670 - val_loss: 6.2236 - val_value_loss: 0.7251 - val_policy_loss: 1.6903\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1428 - value_loss: 0.5878 - policy_loss: 1.6660 - val_loss: 6.2234 - val_value_loss: 0.7253 - val_policy_loss: 1.6898\n",
      "Saved model  tictactoe_50_12\n",
      "iteration 12 | evaluation\n",
      "agent vs random - win ratio 0.81 - draw ratio 0.01\n",
      "Number of seen trajectories: 1300\n",
      "Number of unique trajectories: 1228\n",
      "iteration 13 | self-play\n",
      "saving memory position_memory_tictactoe_50_ep_13\n",
      "iteration 13 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 3, 3, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 9)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 1s 176us/step - loss: 6.1553 - value_loss: 0.6303 - policy_loss: 1.6486 - val_loss: 6.1601 - val_value_loss: 0.6250 - val_policy_loss: 1.6636\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1424 - value_loss: 0.6062 - policy_loss: 1.6471 - val_loss: 6.1579 - val_value_loss: 0.6213 - val_policy_loss: 1.6631\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1339 - value_loss: 0.5909 - policy_loss: 1.6457 - val_loss: 6.1547 - val_value_loss: 0.6156 - val_policy_loss: 1.6626\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1266 - value_loss: 0.5768 - policy_loss: 1.6452 - val_loss: 6.1530 - val_value_loss: 0.6129 - val_policy_loss: 1.6621\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1194 - value_loss: 0.5641 - policy_loss: 1.6438 - val_loss: 6.1511 - val_value_loss: 0.6095 - val_policy_loss: 1.6618\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1137 - value_loss: 0.5539 - policy_loss: 1.6426 - val_loss: 6.1497 - val_value_loss: 0.6072 - val_policy_loss: 1.6615\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1087 - value_loss: 0.5452 - policy_loss: 1.6416 - val_loss: 6.1481 - val_value_loss: 0.6046 - val_policy_loss: 1.6611\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1049 - value_loss: 0.5384 - policy_loss: 1.6409 - val_loss: 6.1474 - val_value_loss: 0.6037 - val_policy_loss: 1.6607\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.0998 - value_loss: 0.5292 - policy_loss: 1.6401 - val_loss: 6.1464 - val_value_loss: 0.6019 - val_policy_loss: 1.6606\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.0969 - value_loss: 0.5243 - policy_loss: 1.6393 - val_loss: 6.1462 - val_value_loss: 0.6019 - val_policy_loss: 1.6603\n",
      "Saved model  tictactoe_50_13\n",
      "iteration 13 | evaluation\n",
      "agent vs random - win ratio 0.84 - draw ratio 0.01\n",
      "Number of seen trajectories: 1400\n",
      "Number of unique trajectories: 1311\n",
      "iteration 14 | self-play\n",
      "saving memory position_memory_tictactoe_50_ep_14\n",
      "iteration 14 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 3, 3, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 9)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 1s 177us/step - loss: 6.1529 - value_loss: 0.6365 - policy_loss: 1.6392 - val_loss: 6.1718 - val_value_loss: 0.6554 - val_policy_loss: 1.6582\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1388 - value_loss: 0.6096 - policy_loss: 1.6382 - val_loss: 6.1677 - val_value_loss: 0.6476 - val_policy_loss: 1.6579\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1291 - value_loss: 0.5918 - policy_loss: 1.6366 - val_loss: 6.1648 - val_value_loss: 0.6423 - val_policy_loss: 1.6576\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1207 - value_loss: 0.5755 - policy_loss: 1.6363 - val_loss: 6.1625 - val_value_loss: 0.6382 - val_policy_loss: 1.6573\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1137 - value_loss: 0.5632 - policy_loss: 1.6349 - val_loss: 6.1597 - val_value_loss: 0.6329 - val_policy_loss: 1.6571\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1069 - value_loss: 0.5503 - policy_loss: 1.6342 - val_loss: 6.1578 - val_value_loss: 0.6296 - val_policy_loss: 1.6569\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1008 - value_loss: 0.5390 - policy_loss: 1.6335 - val_loss: 6.1566 - val_value_loss: 0.6274 - val_policy_loss: 1.6568\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.0971 - value_loss: 0.5328 - policy_loss: 1.6325 - val_loss: 6.1546 - val_value_loss: 0.6238 - val_policy_loss: 1.6566\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.0932 - value_loss: 0.5254 - policy_loss: 1.6322 - val_loss: 6.1538 - val_value_loss: 0.6225 - val_policy_loss: 1.6564\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.0879 - value_loss: 0.5159 - policy_loss: 1.6314 - val_loss: 6.1527 - val_value_loss: 0.6206 - val_policy_loss: 1.6563\n",
      "Saved model  tictactoe_50_14\n",
      "iteration 14 | evaluation\n",
      "agent vs random - win ratio 0.85 - draw ratio 0.0\n",
      "Number of seen trajectories: 1500\n",
      "Number of unique trajectories: 1395\n",
      "iteration 15 | self-play\n",
      "saving memory position_memory_tictactoe_50_ep_15\n",
      "iteration 15 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 3, 3, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 9)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 1s 177us/step - loss: 6.1315 - value_loss: 0.5929 - policy_loss: 1.6416 - val_loss: 6.1423 - val_value_loss: 0.5829 - val_policy_loss: 1.6733\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1227 - value_loss: 0.5766 - policy_loss: 1.6404 - val_loss: 6.1394 - val_value_loss: 0.5775 - val_policy_loss: 1.6729\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1175 - value_loss: 0.5671 - policy_loss: 1.6396 - val_loss: 6.1376 - val_value_loss: 0.5742 - val_policy_loss: 1.6727\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1116 - value_loss: 0.5560 - policy_loss: 1.6390 - val_loss: 6.1362 - val_value_loss: 0.5717 - val_policy_loss: 1.6725\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1082 - value_loss: 0.5497 - policy_loss: 1.6386 - val_loss: 6.1357 - val_value_loss: 0.5709 - val_policy_loss: 1.6723\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1041 - value_loss: 0.5418 - policy_loss: 1.6383 - val_loss: 6.1349 - val_value_loss: 0.5696 - val_policy_loss: 1.6721\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1005 - value_loss: 0.5355 - policy_loss: 1.6374 - val_loss: 6.1345 - val_value_loss: 0.5691 - val_policy_loss: 1.6719\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.0973 - value_loss: 0.5296 - policy_loss: 1.6370 - val_loss: 6.1340 - val_value_loss: 0.5684 - val_policy_loss: 1.6717\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.0942 - value_loss: 0.5240 - policy_loss: 1.6365 - val_loss: 6.1337 - val_value_loss: 0.5680 - val_policy_loss: 1.6716\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.0926 - value_loss: 0.5211 - policy_loss: 1.6363 - val_loss: 6.1335 - val_value_loss: 0.5679 - val_policy_loss: 1.6714\n",
      "Saved model  tictactoe_50_15\n",
      "iteration 15 | evaluation\n",
      "agent vs random - win ratio 0.82 - draw ratio 0.01\n",
      "Number of seen trajectories: 1600\n",
      "Number of unique trajectories: 1476\n",
      "iteration 16 | self-play\n",
      "saving memory position_memory_tictactoe_50_ep_16\n",
      "iteration 16 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 3, 3, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 9)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 1s 173us/step - loss: 6.1642 - value_loss: 0.6557 - policy_loss: 1.6450 - val_loss: 6.1625 - val_value_loss: 0.6410 - val_policy_loss: 1.6564\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1541 - value_loss: 0.6370 - policy_loss: 1.6436 - val_loss: 6.1590 - val_value_loss: 0.6344 - val_policy_loss: 1.6561\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1475 - value_loss: 0.6243 - policy_loss: 1.6431 - val_loss: 6.1567 - val_value_loss: 0.6300 - val_policy_loss: 1.6560\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1416 - value_loss: 0.6131 - policy_loss: 1.6426 - val_loss: 6.1553 - val_value_loss: 0.6273 - val_policy_loss: 1.6558\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1368 - value_loss: 0.6039 - policy_loss: 1.6423 - val_loss: 6.1530 - val_value_loss: 0.6230 - val_policy_loss: 1.6557\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1322 - value_loss: 0.5952 - policy_loss: 1.6419 - val_loss: 6.1516 - val_value_loss: 0.6204 - val_policy_loss: 1.6556\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1263 - value_loss: 0.5844 - policy_loss: 1.6410 - val_loss: 6.1508 - val_value_loss: 0.6191 - val_policy_loss: 1.6555\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1237 - value_loss: 0.5795 - policy_loss: 1.6408 - val_loss: 6.1496 - val_value_loss: 0.6168 - val_policy_loss: 1.6554\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1194 - value_loss: 0.5716 - policy_loss: 1.6403 - val_loss: 6.1485 - val_value_loss: 0.6147 - val_policy_loss: 1.6553\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1165 - value_loss: 0.5665 - policy_loss: 1.6395 - val_loss: 6.1480 - val_value_loss: 0.6140 - val_policy_loss: 1.6552\n",
      "Saved model  tictactoe_50_16\n",
      "iteration 16 | evaluation\n",
      "agent vs random - win ratio 0.83 - draw ratio 0.07\n",
      "Number of seen trajectories: 1700\n",
      "Number of unique trajectories: 1558\n",
      "iteration 17 | self-play\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving memory position_memory_tictactoe_50_ep_17\n",
      "iteration 17 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 3, 3, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 9)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 1s 173us/step - loss: 6.1552 - value_loss: 0.6260 - policy_loss: 1.6575 - val_loss: 6.1397 - val_value_loss: 0.6003 - val_policy_loss: 1.6523\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1468 - value_loss: 0.6101 - policy_loss: 1.6567 - val_loss: 6.1352 - val_value_loss: 0.5917 - val_policy_loss: 1.6519\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1412 - value_loss: 0.5993 - policy_loss: 1.6565 - val_loss: 6.1327 - val_value_loss: 0.5871 - val_policy_loss: 1.6516\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1348 - value_loss: 0.5868 - policy_loss: 1.6561 - val_loss: 6.1303 - val_value_loss: 0.5826 - val_policy_loss: 1.6514\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1297 - value_loss: 0.5777 - policy_loss: 1.6552 - val_loss: 6.1289 - val_value_loss: 0.5801 - val_policy_loss: 1.6511\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1271 - value_loss: 0.5729 - policy_loss: 1.6548 - val_loss: 6.1276 - val_value_loss: 0.5779 - val_policy_loss: 1.6509\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1232 - value_loss: 0.5653 - policy_loss: 1.6548 - val_loss: 6.1260 - val_value_loss: 0.5749 - val_policy_loss: 1.6507\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1211 - value_loss: 0.5612 - policy_loss: 1.6547 - val_loss: 6.1259 - val_value_loss: 0.5750 - val_policy_loss: 1.6505\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1168 - value_loss: 0.5543 - policy_loss: 1.6530 - val_loss: 6.1253 - val_value_loss: 0.5740 - val_policy_loss: 1.6503\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1149 - value_loss: 0.5506 - policy_loss: 1.6531 - val_loss: 6.1236 - val_value_loss: 0.5709 - val_policy_loss: 1.6501\n",
      "Saved model  tictactoe_50_17\n",
      "iteration 17 | evaluation\n",
      "agent vs random - win ratio 0.83 - draw ratio 0.0\n",
      "Number of seen trajectories: 1800\n",
      "Number of unique trajectories: 1631\n",
      "iteration 18 | self-play\n",
      "saving memory position_memory_tictactoe_50_ep_18\n",
      "iteration 18 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 3, 3, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 9)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 1s 177us/step - loss: 6.1441 - value_loss: 0.5938 - policy_loss: 1.6682 - val_loss: 6.1188 - val_value_loss: 0.5336 - val_policy_loss: 1.6780\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1354 - value_loss: 0.5779 - policy_loss: 1.6669 - val_loss: 6.1164 - val_value_loss: 0.5291 - val_policy_loss: 1.6777\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1311 - value_loss: 0.5691 - policy_loss: 1.6671 - val_loss: 6.1154 - val_value_loss: 0.5274 - val_policy_loss: 1.6775\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1255 - value_loss: 0.5587 - policy_loss: 1.6666 - val_loss: 6.1133 - val_value_loss: 0.5235 - val_policy_loss: 1.6772\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1225 - value_loss: 0.5535 - policy_loss: 1.6657 - val_loss: 6.1126 - val_value_loss: 0.5224 - val_policy_loss: 1.6770\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1185 - value_loss: 0.5459 - policy_loss: 1.6654 - val_loss: 6.1119 - val_value_loss: 0.5213 - val_policy_loss: 1.6769\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1143 - value_loss: 0.5386 - policy_loss: 1.6645 - val_loss: 6.1107 - val_value_loss: 0.5191 - val_policy_loss: 1.6767\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1126 - value_loss: 0.5351 - policy_loss: 1.6646 - val_loss: 6.1109 - val_value_loss: 0.5197 - val_policy_loss: 1.6766\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1095 - value_loss: 0.5294 - policy_loss: 1.6641 - val_loss: 6.1088 - val_value_loss: 0.5159 - val_policy_loss: 1.6764\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1062 - value_loss: 0.5235 - policy_loss: 1.6635 - val_loss: 6.1087 - val_value_loss: 0.5159 - val_policy_loss: 1.6763\n",
      "Saved model  tictactoe_50_18\n",
      "iteration 18 | evaluation\n",
      "agent vs random - win ratio 0.81 - draw ratio 0.03\n",
      "Number of seen trajectories: 1900\n",
      "Number of unique trajectories: 1713\n",
      "iteration 19 | self-play\n",
      "saving memory position_memory_tictactoe_50_ep_19\n",
      "iteration 19 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 3, 3, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 9)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 1s 176us/step - loss: 6.1355 - value_loss: 0.6005 - policy_loss: 1.6451 - val_loss: 6.1142 - val_value_loss: 0.5465 - val_policy_loss: 1.6566\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1282 - value_loss: 0.5867 - policy_loss: 1.6445 - val_loss: 6.1100 - val_value_loss: 0.5387 - val_policy_loss: 1.6562\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1224 - value_loss: 0.5758 - policy_loss: 1.6439 - val_loss: 6.1069 - val_value_loss: 0.5329 - val_policy_loss: 1.6559\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1166 - value_loss: 0.5651 - policy_loss: 1.6430 - val_loss: 6.1047 - val_value_loss: 0.5288 - val_policy_loss: 1.6556\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1139 - value_loss: 0.5596 - policy_loss: 1.6432 - val_loss: 6.1035 - val_value_loss: 0.5268 - val_policy_loss: 1.6553\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1092 - value_loss: 0.5510 - policy_loss: 1.6425 - val_loss: 6.1012 - val_value_loss: 0.5225 - val_policy_loss: 1.6550\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1057 - value_loss: 0.5447 - policy_loss: 1.6419 - val_loss: 6.1002 - val_value_loss: 0.5209 - val_policy_loss: 1.6548\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1032 - value_loss: 0.5401 - policy_loss: 1.6416 - val_loss: 6.0987 - val_value_loss: 0.5181 - val_policy_loss: 1.6545\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1003 - value_loss: 0.5351 - policy_loss: 1.6408 - val_loss: 6.0979 - val_value_loss: 0.5169 - val_policy_loss: 1.6543\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.0983 - value_loss: 0.5310 - policy_loss: 1.6411 - val_loss: 6.0964 - val_value_loss: 0.5141 - val_policy_loss: 1.6540\n",
      "Saved model  tictactoe_50_19\n",
      "iteration 19 | evaluation\n",
      "agent vs random - win ratio 0.81 - draw ratio 0.01\n",
      "Number of seen trajectories: 2000\n",
      "Number of unique trajectories: 1787\n",
      "iteration 20 | self-play\n",
      "saving memory position_memory_tictactoe_50_ep_20\n",
      "iteration 20 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 3, 3, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 9)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1513 - value_loss: 0.6254 - policy_loss: 1.6527 - val_loss: 6.1492 - val_value_loss: 0.6169 - val_policy_loss: 1.6570\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1462 - value_loss: 0.6152 - policy_loss: 1.6528 - val_loss: 6.1475 - val_value_loss: 0.6137 - val_policy_loss: 1.6568\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1419 - value_loss: 0.6070 - policy_loss: 1.6524 - val_loss: 6.1462 - val_value_loss: 0.6113 - val_policy_loss: 1.6567\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1375 - value_loss: 0.5991 - policy_loss: 1.6515 - val_loss: 6.1452 - val_value_loss: 0.6094 - val_policy_loss: 1.6566\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1339 - value_loss: 0.5921 - policy_loss: 1.6514 - val_loss: 6.1443 - val_value_loss: 0.6078 - val_policy_loss: 1.6566\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1294 - value_loss: 0.5837 - policy_loss: 1.6508 - val_loss: 6.1436 - val_value_loss: 0.6065 - val_policy_loss: 1.6565\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1267 - value_loss: 0.5784 - policy_loss: 1.6507 - val_loss: 6.1430 - val_value_loss: 0.6053 - val_policy_loss: 1.6564\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1238 - value_loss: 0.5729 - policy_loss: 1.6505 - val_loss: 6.1425 - val_value_loss: 0.6044 - val_policy_loss: 1.6564\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1211 - value_loss: 0.5676 - policy_loss: 1.6505 - val_loss: 6.1420 - val_value_loss: 0.6036 - val_policy_loss: 1.6563\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1176 - value_loss: 0.5612 - policy_loss: 1.6498 - val_loss: 6.1417 - val_value_loss: 0.6030 - val_policy_loss: 1.6563\n",
      "Saved model  tictactoe_50_20\n",
      "iteration 20 | evaluation\n",
      "agent vs random - win ratio 0.79 - draw ratio 0.02\n",
      "Number of seen trajectories: 2100\n",
      "Number of unique trajectories: 1867\n",
      "iteration 21 | self-play\n",
      "saving memory position_memory_tictactoe_50_ep_21\n",
      "iteration 21 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 3, 3, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 9)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 1s 178us/step - loss: 6.1134 - value_loss: 0.5707 - policy_loss: 1.6321 - val_loss: 6.1147 - val_value_loss: 0.5893 - val_policy_loss: 1.6160\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1097 - value_loss: 0.5640 - policy_loss: 1.6314 - val_loss: 6.1139 - val_value_loss: 0.5878 - val_policy_loss: 1.6158\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1063 - value_loss: 0.5576 - policy_loss: 1.6311 - val_loss: 6.1132 - val_value_loss: 0.5866 - val_policy_loss: 1.6158\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1029 - value_loss: 0.5510 - policy_loss: 1.6308 - val_loss: 6.1126 - val_value_loss: 0.5856 - val_policy_loss: 1.6157\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.0997 - value_loss: 0.5452 - policy_loss: 1.6303 - val_loss: 6.1120 - val_value_loss: 0.5846 - val_policy_loss: 1.6156\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.0986 - value_loss: 0.5421 - policy_loss: 1.6311 - val_loss: 6.1116 - val_value_loss: 0.5839 - val_policy_loss: 1.6155\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.0961 - value_loss: 0.5384 - policy_loss: 1.6299 - val_loss: 6.1111 - val_value_loss: 0.5830 - val_policy_loss: 1.6154\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.0952 - value_loss: 0.5361 - policy_loss: 1.6304 - val_loss: 6.1107 - val_value_loss: 0.5823 - val_policy_loss: 1.6154\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.0925 - value_loss: 0.5313 - policy_loss: 1.6300 - val_loss: 6.1102 - val_value_loss: 0.5814 - val_policy_loss: 1.6153\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.0909 - value_loss: 0.5288 - policy_loss: 1.6294 - val_loss: 6.1098 - val_value_loss: 0.5807 - val_policy_loss: 1.6152\n",
      "Saved model  tictactoe_50_21\n",
      "iteration 21 | evaluation\n",
      "agent vs random - win ratio 0.78 - draw ratio 0.02\n",
      "Number of seen trajectories: 2200\n",
      "Number of unique trajectories: 1940\n",
      "iteration 22 | self-play\n",
      "saving memory position_memory_tictactoe_50_ep_22\n",
      "iteration 22 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 3, 3, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 9)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 1s 177us/step - loss: 6.1229 - value_loss: 0.6038 - policy_loss: 1.6183 - val_loss: 6.1342 - val_value_loss: 0.6062 - val_policy_loss: 1.6386\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1184 - value_loss: 0.5953 - policy_loss: 1.6178 - val_loss: 6.1328 - val_value_loss: 0.6035 - val_policy_loss: 1.6385\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1149 - value_loss: 0.5886 - policy_loss: 1.6176 - val_loss: 6.1319 - val_value_loss: 0.6019 - val_policy_loss: 1.6384\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1120 - value_loss: 0.5831 - policy_loss: 1.6173 - val_loss: 6.1313 - val_value_loss: 0.6007 - val_policy_loss: 1.6383\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1091 - value_loss: 0.5778 - policy_loss: 1.6169 - val_loss: 6.1308 - val_value_loss: 0.5999 - val_policy_loss: 1.6382\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1077 - value_loss: 0.5747 - policy_loss: 1.6172 - val_loss: 6.1304 - val_value_loss: 0.5993 - val_policy_loss: 1.6381\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1053 - value_loss: 0.5708 - policy_loss: 1.6162 - val_loss: 6.1300 - val_value_loss: 0.5986 - val_policy_loss: 1.6380\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1034 - value_loss: 0.5668 - policy_loss: 1.6165 - val_loss: 6.1298 - val_value_loss: 0.5983 - val_policy_loss: 1.6379\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1021 - value_loss: 0.5647 - policy_loss: 1.6161 - val_loss: 6.1295 - val_value_loss: 0.5979 - val_policy_loss: 1.6379\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1003 - value_loss: 0.5613 - policy_loss: 1.6160 - val_loss: 6.1293 - val_value_loss: 0.5975 - val_policy_loss: 1.6378\n",
      "Saved model  tictactoe_50_22\n",
      "iteration 22 | evaluation\n",
      "agent vs random - win ratio 0.83 - draw ratio 0.01\n",
      "Number of seen trajectories: 2300\n",
      "Number of unique trajectories: 2012\n",
      "iteration 23 | self-play\n",
      "saving memory position_memory_tictactoe_50_ep_23\n",
      "iteration 23 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 3, 3, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 9)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 1s 175us/step - loss: 6.1335 - value_loss: 0.6259 - policy_loss: 1.6178 - val_loss: 6.1577 - val_value_loss: 0.6662 - val_policy_loss: 1.6260\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1281 - value_loss: 0.6151 - policy_loss: 1.6178 - val_loss: 6.1550 - val_value_loss: 0.6609 - val_policy_loss: 1.6259\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 1s 170us/step - loss: 6.1244 - value_loss: 0.6079 - policy_loss: 1.6177 - val_loss: 6.1530 - val_value_loss: 0.6569 - val_policy_loss: 1.6258\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1215 - value_loss: 0.6023 - policy_loss: 1.6175 - val_loss: 6.1513 - val_value_loss: 0.6537 - val_policy_loss: 1.6258\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1178 - value_loss: 0.5958 - policy_loss: 1.6166 - val_loss: 6.1495 - val_value_loss: 0.6502 - val_policy_loss: 1.6257\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1157 - value_loss: 0.5915 - policy_loss: 1.6168 - val_loss: 6.1482 - val_value_loss: 0.6478 - val_policy_loss: 1.6256\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1129 - value_loss: 0.5854 - policy_loss: 1.6173 - val_loss: 6.1470 - val_value_loss: 0.6454 - val_policy_loss: 1.6256\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1109 - value_loss: 0.5825 - policy_loss: 1.6163 - val_loss: 6.1459 - val_value_loss: 0.6432 - val_policy_loss: 1.6255\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1084 - value_loss: 0.5777 - policy_loss: 1.6161 - val_loss: 6.1450 - val_value_loss: 0.6415 - val_policy_loss: 1.6255\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1061 - value_loss: 0.5738 - policy_loss: 1.6155 - val_loss: 6.1439 - val_value_loss: 0.6395 - val_policy_loss: 1.6254\n",
      "Saved model  tictactoe_50_23\n",
      "iteration 23 | evaluation\n",
      "agent vs random - win ratio 0.78 - draw ratio 0.02\n",
      "Number of seen trajectories: 2400\n",
      "Number of unique trajectories: 2085\n",
      "iteration 24 | self-play\n",
      "saving memory position_memory_tictactoe_50_ep_24\n",
      "iteration 24 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 3, 3, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 9)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 1s 175us/step - loss: 6.1231 - value_loss: 0.5864 - policy_loss: 1.6368 - val_loss: 6.1225 - val_value_loss: 0.5722 - val_policy_loss: 1.6499\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1176 - value_loss: 0.5769 - policy_loss: 1.6354 - val_loss: 6.1199 - val_value_loss: 0.5673 - val_policy_loss: 1.6497\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1138 - value_loss: 0.5696 - policy_loss: 1.6353 - val_loss: 6.1181 - val_value_loss: 0.5639 - val_policy_loss: 1.6496\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1104 - value_loss: 0.5633 - policy_loss: 1.6346 - val_loss: 6.1167 - val_value_loss: 0.5612 - val_policy_loss: 1.6494\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1085 - value_loss: 0.5598 - policy_loss: 1.6345 - val_loss: 6.1155 - val_value_loss: 0.5590 - val_policy_loss: 1.6493\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1057 - value_loss: 0.5545 - policy_loss: 1.6341 - val_loss: 6.1146 - val_value_loss: 0.5573 - val_policy_loss: 1.6492\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1024 - value_loss: 0.5483 - policy_loss: 1.6339 - val_loss: 6.1137 - val_value_loss: 0.5557 - val_policy_loss: 1.6490\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1010 - value_loss: 0.5459 - policy_loss: 1.6334 - val_loss: 6.1129 - val_value_loss: 0.5544 - val_policy_loss: 1.6489\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.0984 - value_loss: 0.5410 - policy_loss: 1.6332 - val_loss: 6.1123 - val_value_loss: 0.5533 - val_policy_loss: 1.6488\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.0972 - value_loss: 0.5386 - policy_loss: 1.6333 - val_loss: 6.1117 - val_value_loss: 0.5523 - val_policy_loss: 1.6487\n",
      "Saved model  tictactoe_50_24\n",
      "iteration 24 | evaluation\n",
      "agent vs random - win ratio 0.81 - draw ratio 0.02\n",
      "Number of seen trajectories: 2500\n",
      "Number of unique trajectories: 2162\n",
      "iteration 25 | self-play\n",
      "saving memory position_memory_tictactoe_50_ep_25\n",
      "iteration 25 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 3, 3, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 9)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 1s 178us/step - loss: 6.1041 - value_loss: 0.5459 - policy_loss: 1.6397 - val_loss: 6.1115 - val_value_loss: 0.5513 - val_policy_loss: 1.6491\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1024 - value_loss: 0.5429 - policy_loss: 1.6394 - val_loss: 6.1105 - val_value_loss: 0.5496 - val_policy_loss: 1.6490\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1008 - value_loss: 0.5398 - policy_loss: 1.6392 - val_loss: 6.1097 - val_value_loss: 0.5481 - val_policy_loss: 1.6489\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.0994 - value_loss: 0.5372 - policy_loss: 1.6391 - val_loss: 6.1090 - val_value_loss: 0.5467 - val_policy_loss: 1.6488\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.0979 - value_loss: 0.5341 - policy_loss: 1.6392 - val_loss: 6.1083 - val_value_loss: 0.5454 - val_policy_loss: 1.6488\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.0960 - value_loss: 0.5309 - policy_loss: 1.6387 - val_loss: 6.1078 - val_value_loss: 0.5444 - val_policy_loss: 1.6487\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.0946 - value_loss: 0.5283 - policy_loss: 1.6384 - val_loss: 6.1073 - val_value_loss: 0.5435 - val_policy_loss: 1.6487\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.0936 - value_loss: 0.5261 - policy_loss: 1.6386 - val_loss: 6.1069 - val_value_loss: 0.5427 - val_policy_loss: 1.6487\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.0930 - value_loss: 0.5250 - policy_loss: 1.6387 - val_loss: 6.1064 - val_value_loss: 0.5418 - val_policy_loss: 1.6486\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.0911 - value_loss: 0.5215 - policy_loss: 1.6384 - val_loss: 6.1060 - val_value_loss: 0.5411 - val_policy_loss: 1.6486\n",
      "Saved model  tictactoe_50_25\n",
      "iteration 25 | evaluation\n",
      "agent vs random - win ratio 0.78 - draw ratio 0.01\n",
      "Number of seen trajectories: 2600\n",
      "Number of unique trajectories: 2238\n",
      "iteration 26 | self-play\n",
      "saving memory position_memory_tictactoe_50_ep_26\n",
      "iteration 26 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 3, 3, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 9)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 1s 175us/step - loss: 6.0754 - value_loss: 0.5083 - policy_loss: 1.6201 - val_loss: 6.0682 - val_value_loss: 0.4806 - val_policy_loss: 1.6335\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.0736 - value_loss: 0.5050 - policy_loss: 1.6199 - val_loss: 6.0671 - val_value_loss: 0.4784 - val_policy_loss: 1.6334\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.0723 - value_loss: 0.5022 - policy_loss: 1.6201 - val_loss: 6.0662 - val_value_loss: 0.4767 - val_policy_loss: 1.6334\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.0703 - value_loss: 0.4992 - policy_loss: 1.6193 - val_loss: 6.0655 - val_value_loss: 0.4755 - val_policy_loss: 1.6334\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.0690 - value_loss: 0.4966 - policy_loss: 1.6191 - val_loss: 6.0650 - val_value_loss: 0.4744 - val_policy_loss: 1.6333\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.0675 - value_loss: 0.4939 - policy_loss: 1.6190 - val_loss: 6.0645 - val_value_loss: 0.4735 - val_policy_loss: 1.6333\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.0677 - value_loss: 0.4931 - policy_loss: 1.6201 - val_loss: 6.0641 - val_value_loss: 0.4728 - val_policy_loss: 1.6333\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.0658 - value_loss: 0.4904 - policy_loss: 1.6191 - val_loss: 6.0637 - val_value_loss: 0.4721 - val_policy_loss: 1.6332\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.0645 - value_loss: 0.4879 - policy_loss: 1.6190 - val_loss: 6.0634 - val_value_loss: 0.4714 - val_policy_loss: 1.6332\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.0628 - value_loss: 0.4849 - policy_loss: 1.6185 - val_loss: 6.0631 - val_value_loss: 0.4708 - val_policy_loss: 1.6332\n",
      "Saved model  tictactoe_50_26\n",
      "iteration 26 | evaluation\n",
      "agent vs random - win ratio 0.84 - draw ratio 0.0\n",
      "Number of seen trajectories: 2700\n",
      "Number of unique trajectories: 2312\n",
      "iteration 27 | self-play\n",
      "saving memory position_memory_tictactoe_50_ep_27\n",
      "iteration 27 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 3, 3, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 9)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 1s 176us/step - loss: 6.1094 - value_loss: 0.5577 - policy_loss: 1.6389 - val_loss: 6.1160 - val_value_loss: 0.5539 - val_policy_loss: 1.6560\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1071 - value_loss: 0.5535 - policy_loss: 1.6386 - val_loss: 6.1155 - val_value_loss: 0.5529 - val_policy_loss: 1.6559\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1054 - value_loss: 0.5503 - policy_loss: 1.6384 - val_loss: 6.1150 - val_value_loss: 0.5521 - val_policy_loss: 1.6559\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1042 - value_loss: 0.5477 - policy_loss: 1.6387 - val_loss: 6.1146 - val_value_loss: 0.5514 - val_policy_loss: 1.6558\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1022 - value_loss: 0.5442 - policy_loss: 1.6382 - val_loss: 6.1143 - val_value_loss: 0.5508 - val_policy_loss: 1.6558\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1014 - value_loss: 0.5423 - policy_loss: 1.6386 - val_loss: 6.1140 - val_value_loss: 0.5503 - val_policy_loss: 1.6557\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.0994 - value_loss: 0.5390 - policy_loss: 1.6378 - val_loss: 6.1138 - val_value_loss: 0.5499 - val_policy_loss: 1.6557\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.0980 - value_loss: 0.5360 - policy_loss: 1.6380 - val_loss: 6.1136 - val_value_loss: 0.5496 - val_policy_loss: 1.6557\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.0970 - value_loss: 0.5341 - policy_loss: 1.6379 - val_loss: 6.1134 - val_value_loss: 0.5492 - val_policy_loss: 1.6556\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.0960 - value_loss: 0.5321 - policy_loss: 1.6380 - val_loss: 6.1133 - val_value_loss: 0.5490 - val_policy_loss: 1.6556\n",
      "Saved model  tictactoe_50_27\n",
      "iteration 27 | evaluation\n",
      "agent vs random - win ratio 0.87 - draw ratio 0.0\n",
      "Number of seen trajectories: 2800\n",
      "Number of unique trajectories: 2373\n",
      "iteration 28 | self-play\n",
      "saving memory position_memory_tictactoe_50_ep_28\n",
      "iteration 28 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 3, 3, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 9)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 1s 176us/step - loss: 6.1507 - value_loss: 0.6632 - policy_loss: 1.6162 - val_loss: 6.1492 - val_value_loss: 0.6679 - val_policy_loss: 1.6086\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1462 - value_loss: 0.6542 - policy_loss: 1.6163 - val_loss: 6.1460 - val_value_loss: 0.6616 - val_policy_loss: 1.6085\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1432 - value_loss: 0.6482 - policy_loss: 1.6164 - val_loss: 6.1440 - val_value_loss: 0.6577 - val_policy_loss: 1.6085\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1406 - value_loss: 0.6433 - policy_loss: 1.6159 - val_loss: 6.1425 - val_value_loss: 0.6547 - val_policy_loss: 1.6084\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1385 - value_loss: 0.6396 - policy_loss: 1.6156 - val_loss: 6.1413 - val_value_loss: 0.6525 - val_policy_loss: 1.6084\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1373 - value_loss: 0.6371 - policy_loss: 1.6156 - val_loss: 6.1404 - val_value_loss: 0.6507 - val_policy_loss: 1.6083\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1357 - value_loss: 0.6337 - policy_loss: 1.6159 - val_loss: 6.1395 - val_value_loss: 0.6489 - val_policy_loss: 1.6083\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1335 - value_loss: 0.6300 - policy_loss: 1.6152 - val_loss: 6.1386 - val_value_loss: 0.6472 - val_policy_loss: 1.6082\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1324 - value_loss: 0.6279 - policy_loss: 1.6151 - val_loss: 6.1379 - val_value_loss: 0.6458 - val_policy_loss: 1.6082\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1312 - value_loss: 0.6254 - policy_loss: 1.6154 - val_loss: 6.1371 - val_value_loss: 0.6444 - val_policy_loss: 1.6081\n",
      "Saved model  tictactoe_50_28\n",
      "iteration 28 | evaluation\n",
      "agent vs random - win ratio 0.83 - draw ratio 0.0\n",
      "Number of seen trajectories: 2900\n",
      "Number of unique trajectories: 2445\n",
      "iteration 29 | self-play\n",
      "saving memory position_memory_tictactoe_50_ep_29\n",
      "iteration 29 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 3, 3, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 9)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 1s 178us/step - loss: 6.1480 - value_loss: 0.6445 - policy_loss: 1.6298 - val_loss: 6.1464 - val_value_loss: 0.6605 - val_policy_loss: 1.6106\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1465 - value_loss: 0.6417 - policy_loss: 1.6296 - val_loss: 6.1455 - val_value_loss: 0.6588 - val_policy_loss: 1.6106\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1448 - value_loss: 0.6381 - policy_loss: 1.6299 - val_loss: 6.1448 - val_value_loss: 0.6575 - val_policy_loss: 1.6106\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1428 - value_loss: 0.6339 - policy_loss: 1.6300 - val_loss: 6.1443 - val_value_loss: 0.6564 - val_policy_loss: 1.6105\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1410 - value_loss: 0.6311 - policy_loss: 1.6293 - val_loss: 6.1437 - val_value_loss: 0.6553 - val_policy_loss: 1.6105\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1399 - value_loss: 0.6291 - policy_loss: 1.6291 - val_loss: 6.1433 - val_value_loss: 0.6546 - val_policy_loss: 1.6105\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1381 - value_loss: 0.6256 - policy_loss: 1.6290 - val_loss: 6.1430 - val_value_loss: 0.6539 - val_policy_loss: 1.6104\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1372 - value_loss: 0.6241 - policy_loss: 1.6288 - val_loss: 6.1427 - val_value_loss: 0.6534 - val_policy_loss: 1.6104\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1358 - value_loss: 0.6212 - policy_loss: 1.6288 - val_loss: 6.1424 - val_value_loss: 0.6529 - val_policy_loss: 1.6104\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1347 - value_loss: 0.6186 - policy_loss: 1.6293 - val_loss: 6.1421 - val_value_loss: 0.6524 - val_policy_loss: 1.6104\n",
      "Saved model  tictactoe_50_29\n",
      "iteration 29 | evaluation\n",
      "agent vs random - win ratio 0.9 - draw ratio 0.02\n",
      "Number of seen trajectories: 3000\n",
      "Number of unique trajectories: 2513\n",
      "iteration 30 | self-play\n",
      "saving memory position_memory_tictactoe_50_ep_30\n",
      "iteration 30 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 3, 3, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 9)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 1s 175us/step - loss: 6.1509 - value_loss: 0.6397 - policy_loss: 1.6407 - val_loss: 6.1135 - val_value_loss: 0.5964 - val_policy_loss: 1.6091\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1481 - value_loss: 0.6342 - policy_loss: 1.6404 - val_loss: 6.1127 - val_value_loss: 0.5949 - val_policy_loss: 1.6091\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1471 - value_loss: 0.6324 - policy_loss: 1.6403 - val_loss: 6.1123 - val_value_loss: 0.5941 - val_policy_loss: 1.6090\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1453 - value_loss: 0.6289 - policy_loss: 1.6402 - val_loss: 6.1120 - val_value_loss: 0.5937 - val_policy_loss: 1.6089\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1444 - value_loss: 0.6272 - policy_loss: 1.6401 - val_loss: 6.1118 - val_value_loss: 0.5933 - val_policy_loss: 1.6088\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1431 - value_loss: 0.6246 - policy_loss: 1.6401 - val_loss: 6.1117 - val_value_loss: 0.5931 - val_policy_loss: 1.6088\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1421 - value_loss: 0.6225 - policy_loss: 1.6401 - val_loss: 6.1116 - val_value_loss: 0.5929 - val_policy_loss: 1.6087\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1407 - value_loss: 0.6205 - policy_loss: 1.6395 - val_loss: 6.1115 - val_value_loss: 0.5928 - val_policy_loss: 1.6087\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1395 - value_loss: 0.6184 - policy_loss: 1.6393 - val_loss: 6.1114 - val_value_loss: 0.5928 - val_policy_loss: 1.6086\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1390 - value_loss: 0.6170 - policy_loss: 1.6396 - val_loss: 6.1113 - val_value_loss: 0.5927 - val_policy_loss: 1.6086\n",
      "Saved model  tictactoe_50_30\n",
      "iteration 30 | evaluation\n",
      "agent vs random - win ratio 0.84 - draw ratio 0.0\n",
      "Number of seen trajectories: 3100\n",
      "Number of unique trajectories: 2584\n",
      "iteration 31 | self-play\n",
      "saving memory position_memory_tictactoe_50_ep_31\n",
      "iteration 31 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 3, 3, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 9)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 1s 176us/step - loss: 6.1555 - value_loss: 0.6398 - policy_loss: 1.6498 - val_loss: 6.1451 - val_value_loss: 0.6535 - val_policy_loss: 1.6152\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1543 - value_loss: 0.6370 - policy_loss: 1.6502 - val_loss: 6.1446 - val_value_loss: 0.6528 - val_policy_loss: 1.6151\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1515 - value_loss: 0.6318 - policy_loss: 1.6499 - val_loss: 6.1442 - val_value_loss: 0.6521 - val_policy_loss: 1.6150\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1510 - value_loss: 0.6310 - policy_loss: 1.6496 - val_loss: 6.1439 - val_value_loss: 0.6514 - val_policy_loss: 1.6149\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1501 - value_loss: 0.6293 - policy_loss: 1.6495 - val_loss: 6.1435 - val_value_loss: 0.6508 - val_policy_loss: 1.6149\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1485 - value_loss: 0.6262 - policy_loss: 1.6493 - val_loss: 6.1432 - val_value_loss: 0.6503 - val_policy_loss: 1.6148\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1478 - value_loss: 0.6249 - policy_loss: 1.6492 - val_loss: 6.1429 - val_value_loss: 0.6497 - val_policy_loss: 1.6147\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1464 - value_loss: 0.6222 - policy_loss: 1.6493 - val_loss: 6.1426 - val_value_loss: 0.6492 - val_policy_loss: 1.6147\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1452 - value_loss: 0.6200 - policy_loss: 1.6491 - val_loss: 6.1423 - val_value_loss: 0.6487 - val_policy_loss: 1.6146\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1444 - value_loss: 0.6180 - policy_loss: 1.6494 - val_loss: 6.1421 - val_value_loss: 0.6483 - val_policy_loss: 1.6146\n",
      "Saved model  tictactoe_50_31\n",
      "iteration 31 | evaluation\n",
      "agent vs random - win ratio 0.79 - draw ratio 0.04\n",
      "Number of seen trajectories: 3200\n",
      "Number of unique trajectories: 2641\n",
      "iteration 32 | self-play\n",
      "saving memory position_memory_tictactoe_50_ep_32\n",
      "iteration 32 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 3, 3, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 9)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 1s 176us/step - loss: 6.1086 - value_loss: 0.5489 - policy_loss: 1.6471 - val_loss: 6.0883 - val_value_loss: 0.5327 - val_policy_loss: 1.6227\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1058 - value_loss: 0.5435 - policy_loss: 1.6468 - val_loss: 6.0872 - val_value_loss: 0.5304 - val_policy_loss: 1.6226\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1042 - value_loss: 0.5399 - policy_loss: 1.6471 - val_loss: 6.0865 - val_value_loss: 0.5291 - val_policy_loss: 1.6225\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1027 - value_loss: 0.5371 - policy_loss: 1.6470 - val_loss: 6.0860 - val_value_loss: 0.5282 - val_policy_loss: 1.6224\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1020 - value_loss: 0.5359 - policy_loss: 1.6469 - val_loss: 6.0856 - val_value_loss: 0.5275 - val_policy_loss: 1.6224\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1012 - value_loss: 0.5339 - policy_loss: 1.6471 - val_loss: 6.0853 - val_value_loss: 0.5269 - val_policy_loss: 1.6223\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.0994 - value_loss: 0.5310 - policy_loss: 1.6465 - val_loss: 6.0850 - val_value_loss: 0.5264 - val_policy_loss: 1.6222\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.0986 - value_loss: 0.5295 - policy_loss: 1.6464 - val_loss: 6.0847 - val_value_loss: 0.5260 - val_policy_loss: 1.6222\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.0989 - value_loss: 0.5299 - policy_loss: 1.6467 - val_loss: 6.0845 - val_value_loss: 0.5256 - val_policy_loss: 1.6221\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.0973 - value_loss: 0.5266 - policy_loss: 1.6468 - val_loss: 6.0842 - val_value_loss: 0.5251 - val_policy_loss: 1.6221\n",
      "Saved model  tictactoe_50_32\n",
      "iteration 32 | evaluation\n",
      "agent vs random - win ratio 0.8 - draw ratio 0.01\n",
      "Number of seen trajectories: 3300\n",
      "Number of unique trajectories: 2699\n",
      "iteration 33 | self-play\n",
      "saving memory position_memory_tictactoe_50_ep_33\n",
      "iteration 33 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 3, 3, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 9)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 1s 176us/step - loss: 6.0624 - value_loss: 0.4846 - policy_loss: 1.6189 - val_loss: 6.0791 - val_value_loss: 0.5010 - val_policy_loss: 1.6360\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.0618 - value_loss: 0.4831 - policy_loss: 1.6193 - val_loss: 6.0784 - val_value_loss: 0.4996 - val_policy_loss: 1.6360\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.0606 - value_loss: 0.4810 - policy_loss: 1.6190 - val_loss: 6.0778 - val_value_loss: 0.4984 - val_policy_loss: 1.6360\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.0588 - value_loss: 0.4779 - policy_loss: 1.6185 - val_loss: 6.0771 - val_value_loss: 0.4971 - val_policy_loss: 1.6360\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.0587 - value_loss: 0.4776 - policy_loss: 1.6187 - val_loss: 6.0765 - val_value_loss: 0.4959 - val_policy_loss: 1.6360\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.0573 - value_loss: 0.4748 - policy_loss: 1.6187 - val_loss: 6.0760 - val_value_loss: 0.4948 - val_policy_loss: 1.6359\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.0566 - value_loss: 0.4738 - policy_loss: 1.6183 - val_loss: 6.0754 - val_value_loss: 0.4938 - val_policy_loss: 1.6359\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.0565 - value_loss: 0.4731 - policy_loss: 1.6188 - val_loss: 6.0749 - val_value_loss: 0.4927 - val_policy_loss: 1.6359\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.0550 - value_loss: 0.4703 - policy_loss: 1.6186 - val_loss: 6.0744 - val_value_loss: 0.4917 - val_policy_loss: 1.6359\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.0544 - value_loss: 0.4688 - policy_loss: 1.6188 - val_loss: 6.0739 - val_value_loss: 0.4908 - val_policy_loss: 1.6359\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model  tictactoe_50_33\n",
      "iteration 33 | evaluation\n",
      "agent vs random - win ratio 0.87 - draw ratio 0.01\n",
      "Number of seen trajectories: 3400\n",
      "Number of unique trajectories: 2762\n",
      "iteration 34 | self-play\n",
      "saving memory position_memory_tictactoe_50_ep_34\n",
      "iteration 34 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 3, 3, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 9)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 1s 175us/step - loss: 6.0694 - value_loss: 0.5014 - policy_loss: 1.6162 - val_loss: 6.0891 - val_value_loss: 0.5223 - val_policy_loss: 1.6347\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.0680 - value_loss: 0.4990 - policy_loss: 1.6159 - val_loss: 6.0884 - val_value_loss: 0.5209 - val_policy_loss: 1.6347\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.0672 - value_loss: 0.4975 - policy_loss: 1.6158 - val_loss: 6.0878 - val_value_loss: 0.5199 - val_policy_loss: 1.6347\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.0674 - value_loss: 0.4975 - policy_loss: 1.6162 - val_loss: 6.0874 - val_value_loss: 0.5190 - val_policy_loss: 1.6347\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.0658 - value_loss: 0.4948 - policy_loss: 1.6157 - val_loss: 6.0870 - val_value_loss: 0.5182 - val_policy_loss: 1.6347\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.0655 - value_loss: 0.4943 - policy_loss: 1.6156 - val_loss: 6.0867 - val_value_loss: 0.5176 - val_policy_loss: 1.6347\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.0649 - value_loss: 0.4928 - policy_loss: 1.6159 - val_loss: 6.0864 - val_value_loss: 0.5171 - val_policy_loss: 1.6347\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.0636 - value_loss: 0.4907 - policy_loss: 1.6155 - val_loss: 6.0861 - val_value_loss: 0.5166 - val_policy_loss: 1.6347\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.0634 - value_loss: 0.4897 - policy_loss: 1.6161 - val_loss: 6.0859 - val_value_loss: 0.5161 - val_policy_loss: 1.6347\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.0616 - value_loss: 0.4866 - policy_loss: 1.6155 - val_loss: 6.0856 - val_value_loss: 0.5156 - val_policy_loss: 1.6347\n",
      "Saved model  tictactoe_50_34\n",
      "iteration 34 | evaluation\n",
      "agent vs random - win ratio 0.88 - draw ratio 0.01\n",
      "Number of seen trajectories: 3500\n",
      "Number of unique trajectories: 2828\n",
      "iteration 35 | self-play\n",
      "saving memory position_memory_tictactoe_50_ep_35\n",
      "iteration 35 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 3, 3, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 9)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 1s 176us/step - loss: 6.1150 - value_loss: 0.5864 - policy_loss: 1.6226 - val_loss: 6.1159 - val_value_loss: 0.5578 - val_policy_loss: 1.6529\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1133 - value_loss: 0.5834 - policy_loss: 1.6223 - val_loss: 6.1153 - val_value_loss: 0.5566 - val_policy_loss: 1.6530\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1124 - value_loss: 0.5819 - policy_loss: 1.6218 - val_loss: 6.1147 - val_value_loss: 0.5555 - val_policy_loss: 1.6530\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1115 - value_loss: 0.5800 - policy_loss: 1.6220 - val_loss: 6.1143 - val_value_loss: 0.5546 - val_policy_loss: 1.6530\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1103 - value_loss: 0.5780 - policy_loss: 1.6216 - val_loss: 6.1140 - val_value_loss: 0.5538 - val_policy_loss: 1.6531\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1092 - value_loss: 0.5758 - policy_loss: 1.6216 - val_loss: 6.1137 - val_value_loss: 0.5533 - val_policy_loss: 1.6531\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1094 - value_loss: 0.5758 - policy_loss: 1.6219 - val_loss: 6.1134 - val_value_loss: 0.5528 - val_policy_loss: 1.6531\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1085 - value_loss: 0.5741 - policy_loss: 1.6220 - val_loss: 6.1133 - val_value_loss: 0.5524 - val_policy_loss: 1.6532\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1080 - value_loss: 0.5730 - policy_loss: 1.6219 - val_loss: 6.1131 - val_value_loss: 0.5521 - val_policy_loss: 1.6532\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1075 - value_loss: 0.5722 - policy_loss: 1.6218 - val_loss: 6.1130 - val_value_loss: 0.5518 - val_policy_loss: 1.6532\n",
      "Saved model  tictactoe_50_35\n",
      "iteration 35 | evaluation\n",
      "agent vs random - win ratio 0.83 - draw ratio 0.0\n",
      "Number of seen trajectories: 3600\n",
      "Number of unique trajectories: 2891\n",
      "iteration 36 | self-play\n",
      "saving memory position_memory_tictactoe_50_ep_36\n",
      "iteration 36 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 3, 3, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 9)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 1s 177us/step - loss: 6.1202 - value_loss: 0.5863 - policy_loss: 1.6332 - val_loss: 6.1215 - val_value_loss: 0.5815 - val_policy_loss: 1.6406\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1197 - value_loss: 0.5847 - policy_loss: 1.6336 - val_loss: 6.1213 - val_value_loss: 0.5811 - val_policy_loss: 1.6406\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1198 - value_loss: 0.5848 - policy_loss: 1.6339 - val_loss: 6.1212 - val_value_loss: 0.5809 - val_policy_loss: 1.6405\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1187 - value_loss: 0.5832 - policy_loss: 1.6333 - val_loss: 6.1210 - val_value_loss: 0.5806 - val_policy_loss: 1.6405\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1192 - value_loss: 0.5834 - policy_loss: 1.6340 - val_loss: 6.1209 - val_value_loss: 0.5803 - val_policy_loss: 1.6405\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1177 - value_loss: 0.5812 - policy_loss: 1.6332 - val_loss: 6.1208 - val_value_loss: 0.5801 - val_policy_loss: 1.6405\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1175 - value_loss: 0.5811 - policy_loss: 1.6330 - val_loss: 6.1206 - val_value_loss: 0.5798 - val_policy_loss: 1.6405\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1169 - value_loss: 0.5795 - policy_loss: 1.6333 - val_loss: 6.1205 - val_value_loss: 0.5796 - val_policy_loss: 1.6405\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1161 - value_loss: 0.5783 - policy_loss: 1.6330 - val_loss: 6.1204 - val_value_loss: 0.5794 - val_policy_loss: 1.6405\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1157 - value_loss: 0.5773 - policy_loss: 1.6332 - val_loss: 6.1203 - val_value_loss: 0.5792 - val_policy_loss: 1.6405\n",
      "Saved model  tictactoe_50_36\n",
      "iteration 36 | evaluation\n",
      "agent vs random - win ratio 0.84 - draw ratio 0.01\n",
      "Number of seen trajectories: 3700\n",
      "Number of unique trajectories: 2949\n",
      "iteration 37 | self-play\n",
      "saving memory position_memory_tictactoe_50_ep_37\n",
      "iteration 37 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 3, 3, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 9)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 1s 175us/step - loss: 6.1003 - value_loss: 0.5667 - policy_loss: 1.6131 - val_loss: 6.0972 - val_value_loss: 0.5697 - val_policy_loss: 1.6038\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1007 - value_loss: 0.5675 - policy_loss: 1.6130 - val_loss: 6.0965 - val_value_loss: 0.5683 - val_policy_loss: 1.6038\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.0990 - value_loss: 0.5641 - policy_loss: 1.6129 - val_loss: 6.0959 - val_value_loss: 0.5671 - val_policy_loss: 1.6038\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.0989 - value_loss: 0.5637 - policy_loss: 1.6133 - val_loss: 6.0954 - val_value_loss: 0.5660 - val_policy_loss: 1.6038\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.0980 - value_loss: 0.5625 - policy_loss: 1.6127 - val_loss: 6.0949 - val_value_loss: 0.5650 - val_policy_loss: 1.6039\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.0970 - value_loss: 0.5595 - policy_loss: 1.6135 - val_loss: 6.0944 - val_value_loss: 0.5641 - val_policy_loss: 1.6039\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.0958 - value_loss: 0.5579 - policy_loss: 1.6129 - val_loss: 6.0940 - val_value_loss: 0.5633 - val_policy_loss: 1.6039\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.0963 - value_loss: 0.5586 - policy_loss: 1.6131 - val_loss: 6.0936 - val_value_loss: 0.5625 - val_policy_loss: 1.6039\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.0951 - value_loss: 0.5556 - policy_loss: 1.6136 - val_loss: 6.0933 - val_value_loss: 0.5618 - val_policy_loss: 1.6039\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.0939 - value_loss: 0.5545 - policy_loss: 1.6124 - val_loss: 6.0930 - val_value_loss: 0.5611 - val_policy_loss: 1.6039\n",
      "Saved model  tictactoe_50_37\n",
      "iteration 37 | evaluation\n",
      "agent vs random - win ratio 0.84 - draw ratio 0.02\n",
      "Number of seen trajectories: 3800\n",
      "Number of unique trajectories: 3014\n",
      "iteration 38 | self-play\n",
      "saving memory position_memory_tictactoe_50_ep_38\n",
      "iteration 38 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 3, 3, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 9)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 1s 178us/step - loss: 6.1264 - value_loss: 0.6110 - policy_loss: 1.6209 - val_loss: 6.1214 - val_value_loss: 0.6138 - val_policy_loss: 1.6080\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1258 - value_loss: 0.6096 - policy_loss: 1.6210 - val_loss: 6.1211 - val_value_loss: 0.6133 - val_policy_loss: 1.6081\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1258 - value_loss: 0.6094 - policy_loss: 1.6214 - val_loss: 6.1208 - val_value_loss: 0.6127 - val_policy_loss: 1.6081\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1240 - value_loss: 0.6064 - policy_loss: 1.6208 - val_loss: 6.1206 - val_value_loss: 0.6122 - val_policy_loss: 1.6081\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1239 - value_loss: 0.6062 - policy_loss: 1.6207 - val_loss: 6.1203 - val_value_loss: 0.6117 - val_policy_loss: 1.6081\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1238 - value_loss: 0.6056 - policy_loss: 1.6211 - val_loss: 6.1201 - val_value_loss: 0.6112 - val_policy_loss: 1.6081\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1235 - value_loss: 0.6051 - policy_loss: 1.6212 - val_loss: 6.1199 - val_value_loss: 0.6107 - val_policy_loss: 1.6082\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1232 - value_loss: 0.6044 - policy_loss: 1.6212 - val_loss: 6.1196 - val_value_loss: 0.6103 - val_policy_loss: 1.6082\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1216 - value_loss: 0.6018 - policy_loss: 1.6205 - val_loss: 6.1194 - val_value_loss: 0.6098 - val_policy_loss: 1.6082\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1213 - value_loss: 0.6008 - policy_loss: 1.6210 - val_loss: 6.1192 - val_value_loss: 0.6094 - val_policy_loss: 1.6082\n",
      "Saved model  tictactoe_50_38\n",
      "iteration 38 | evaluation\n",
      "agent vs random - win ratio 0.82 - draw ratio 0.03\n",
      "Number of seen trajectories: 3900\n",
      "Number of unique trajectories: 3079\n",
      "iteration 39 | self-play\n",
      "saving memory position_memory_tictactoe_50_ep_39\n",
      "iteration 39 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 3, 3, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 9)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 1s 175us/step - loss: 6.1639 - value_loss: 0.6931 - policy_loss: 1.6138 - val_loss: 6.1598 - val_value_loss: 0.6783 - val_policy_loss: 1.6205\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1638 - value_loss: 0.6931 - policy_loss: 1.6137 - val_loss: 6.1594 - val_value_loss: 0.6775 - val_policy_loss: 1.6205\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1643 - value_loss: 0.6930 - policy_loss: 1.6147 - val_loss: 6.1591 - val_value_loss: 0.6769 - val_policy_loss: 1.6205\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1618 - value_loss: 0.6899 - policy_loss: 1.6128 - val_loss: 6.1588 - val_value_loss: 0.6763 - val_policy_loss: 1.6204\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1621 - value_loss: 0.6899 - policy_loss: 1.6135 - val_loss: 6.1585 - val_value_loss: 0.6758 - val_policy_loss: 1.6204\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1612 - value_loss: 0.6882 - policy_loss: 1.6135 - val_loss: 6.1583 - val_value_loss: 0.6753 - val_policy_loss: 1.6204\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1609 - value_loss: 0.6880 - policy_loss: 1.6131 - val_loss: 6.1580 - val_value_loss: 0.6749 - val_policy_loss: 1.6204\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1599 - value_loss: 0.6861 - policy_loss: 1.6128 - val_loss: 6.1578 - val_value_loss: 0.6744 - val_policy_loss: 1.6204\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1590 - value_loss: 0.6840 - policy_loss: 1.6131 - val_loss: 6.1576 - val_value_loss: 0.6740 - val_policy_loss: 1.6204\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1582 - value_loss: 0.6825 - policy_loss: 1.6131 - val_loss: 6.1574 - val_value_loss: 0.6736 - val_policy_loss: 1.6204\n",
      "Saved model  tictactoe_50_39\n",
      "iteration 39 | evaluation\n",
      "agent vs random - win ratio 0.8 - draw ratio 0.01\n",
      "Number of seen trajectories: 4000\n",
      "Number of unique trajectories: 3141\n",
      "iteration 40 | self-play\n",
      "saving memory position_memory_tictactoe_50_ep_40\n",
      "iteration 40 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 3, 3, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 9)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 1s 178us/step - loss: 6.1490 - value_loss: 0.6720 - policy_loss: 1.6052 - val_loss: 6.1352 - val_value_loss: 0.6401 - val_policy_loss: 1.6094\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1494 - value_loss: 0.6725 - policy_loss: 1.6055 - val_loss: 6.1351 - val_value_loss: 0.6401 - val_policy_loss: 1.6094\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1484 - value_loss: 0.6707 - policy_loss: 1.6054 - val_loss: 6.1351 - val_value_loss: 0.6400 - val_policy_loss: 1.6095\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1476 - value_loss: 0.6692 - policy_loss: 1.6052 - val_loss: 6.1351 - val_value_loss: 0.6400 - val_policy_loss: 1.6095\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1475 - value_loss: 0.6688 - policy_loss: 1.6053 - val_loss: 6.1351 - val_value_loss: 0.6399 - val_policy_loss: 1.6095\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1470 - value_loss: 0.6678 - policy_loss: 1.6054 - val_loss: 6.1351 - val_value_loss: 0.6399 - val_policy_loss: 1.6095\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1479 - value_loss: 0.6693 - policy_loss: 1.6058 - val_loss: 6.1351 - val_value_loss: 0.6399 - val_policy_loss: 1.6095\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1467 - value_loss: 0.6671 - policy_loss: 1.6055 - val_loss: 6.1351 - val_value_loss: 0.6398 - val_policy_loss: 1.6095\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1464 - value_loss: 0.6666 - policy_loss: 1.6055 - val_loss: 6.1350 - val_value_loss: 0.6398 - val_policy_loss: 1.6096\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1461 - value_loss: 0.6662 - policy_loss: 1.6052 - val_loss: 6.1350 - val_value_loss: 0.6397 - val_policy_loss: 1.6096\n",
      "Saved model  tictactoe_50_40\n",
      "iteration 40 | evaluation\n",
      "agent vs random - win ratio 0.83 - draw ratio 0.02\n",
      "Number of seen trajectories: 4100\n",
      "Number of unique trajectories: 3202\n",
      "iteration 41 | self-play\n",
      "saving memory position_memory_tictactoe_50_ep_41\n",
      "iteration 41 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 3, 3, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 9)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 1s 175us/step - loss: 6.1488 - value_loss: 0.6359 - policy_loss: 1.6410 - val_loss: 6.1201 - val_value_loss: 0.6284 - val_policy_loss: 1.5911\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1482 - value_loss: 0.6350 - policy_loss: 1.6406 - val_loss: 6.1198 - val_value_loss: 0.6279 - val_policy_loss: 1.5910\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1472 - value_loss: 0.6328 - policy_loss: 1.6408 - val_loss: 6.1196 - val_value_loss: 0.6275 - val_policy_loss: 1.5909\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1469 - value_loss: 0.6325 - policy_loss: 1.6405 - val_loss: 6.1193 - val_value_loss: 0.6271 - val_policy_loss: 1.5908\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1469 - value_loss: 0.6325 - policy_loss: 1.6406 - val_loss: 6.1191 - val_value_loss: 0.6266 - val_policy_loss: 1.5907\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1469 - value_loss: 0.6327 - policy_loss: 1.6404 - val_loss: 6.1188 - val_value_loss: 0.6262 - val_policy_loss: 1.5906\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1462 - value_loss: 0.6311 - policy_loss: 1.6404 - val_loss: 6.1186 - val_value_loss: 0.6258 - val_policy_loss: 1.5906\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1459 - value_loss: 0.6307 - policy_loss: 1.6403 - val_loss: 6.1183 - val_value_loss: 0.6254 - val_policy_loss: 1.5905\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1462 - value_loss: 0.6311 - policy_loss: 1.6406 - val_loss: 6.1181 - val_value_loss: 0.6250 - val_policy_loss: 1.5905\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1447 - value_loss: 0.6284 - policy_loss: 1.6402 - val_loss: 6.1179 - val_value_loss: 0.6247 - val_policy_loss: 1.5904\n",
      "Saved model  tictactoe_50_41\n",
      "iteration 41 | evaluation\n",
      "agent vs random - win ratio 0.84 - draw ratio 0.0\n",
      "Number of seen trajectories: 4200\n",
      "Number of unique trajectories: 3248\n",
      "iteration 42 | self-play\n",
      "saving memory position_memory_tictactoe_50_ep_42\n",
      "iteration 42 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 3, 3, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 9)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 1s 178us/step - loss: 6.1432 - value_loss: 0.6201 - policy_loss: 1.6454 - val_loss: 6.1548 - val_value_loss: 0.6229 - val_policy_loss: 1.6660\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1425 - value_loss: 0.6194 - policy_loss: 1.6449 - val_loss: 6.1546 - val_value_loss: 0.6225 - val_policy_loss: 1.6660\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1425 - value_loss: 0.6192 - policy_loss: 1.6450 - val_loss: 6.1544 - val_value_loss: 0.6222 - val_policy_loss: 1.6659\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1420 - value_loss: 0.6177 - policy_loss: 1.6455 - val_loss: 6.1542 - val_value_loss: 0.6219 - val_policy_loss: 1.6659\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1421 - value_loss: 0.6184 - policy_loss: 1.6450 - val_loss: 6.1541 - val_value_loss: 0.6216 - val_policy_loss: 1.6659\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1411 - value_loss: 0.6168 - policy_loss: 1.6447 - val_loss: 6.1539 - val_value_loss: 0.6213 - val_policy_loss: 1.6658\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1412 - value_loss: 0.6170 - policy_loss: 1.6447 - val_loss: 6.1538 - val_value_loss: 0.6210 - val_policy_loss: 1.6658\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1412 - value_loss: 0.6167 - policy_loss: 1.6449 - val_loss: 6.1536 - val_value_loss: 0.6207 - val_policy_loss: 1.6658\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1421 - value_loss: 0.6180 - policy_loss: 1.6456 - val_loss: 6.1534 - val_value_loss: 0.6204 - val_policy_loss: 1.6658\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1402 - value_loss: 0.6151 - policy_loss: 1.6446 - val_loss: 6.1533 - val_value_loss: 0.6201 - val_policy_loss: 1.6658\n",
      "Saved model  tictactoe_50_42\n",
      "iteration 42 | evaluation\n",
      "agent vs random - win ratio 0.8 - draw ratio 0.05\n",
      "Number of seen trajectories: 4300\n",
      "Number of unique trajectories: 3305\n",
      "iteration 43 | self-play\n",
      "saving memory position_memory_tictactoe_50_ep_43\n",
      "iteration 43 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 3, 3, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 9)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 1s 178us/step - loss: 6.1287 - value_loss: 0.6122 - policy_loss: 1.6244 - val_loss: 6.1401 - val_value_loss: 0.6387 - val_policy_loss: 1.6208\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1272 - value_loss: 0.6096 - policy_loss: 1.6241 - val_loss: 6.1398 - val_value_loss: 0.6382 - val_policy_loss: 1.6207\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1276 - value_loss: 0.6103 - policy_loss: 1.6243 - val_loss: 6.1395 - val_value_loss: 0.6377 - val_policy_loss: 1.6207\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1279 - value_loss: 0.6107 - policy_loss: 1.6245 - val_loss: 6.1393 - val_value_loss: 0.6372 - val_policy_loss: 1.6207\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1272 - value_loss: 0.6089 - policy_loss: 1.6248 - val_loss: 6.1390 - val_value_loss: 0.6367 - val_policy_loss: 1.6207\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1261 - value_loss: 0.6071 - policy_loss: 1.6244 - val_loss: 6.1388 - val_value_loss: 0.6362 - val_policy_loss: 1.6207\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1263 - value_loss: 0.6075 - policy_loss: 1.6243 - val_loss: 6.1386 - val_value_loss: 0.6357 - val_policy_loss: 1.6207\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1262 - value_loss: 0.6072 - policy_loss: 1.6245 - val_loss: 6.1383 - val_value_loss: 0.6353 - val_policy_loss: 1.6207\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1261 - value_loss: 0.6071 - policy_loss: 1.6244 - val_loss: 6.1381 - val_value_loss: 0.6348 - val_policy_loss: 1.6207\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1251 - value_loss: 0.6049 - policy_loss: 1.6246 - val_loss: 6.1379 - val_value_loss: 0.6344 - val_policy_loss: 1.6207\n",
      "Saved model  tictactoe_50_43\n",
      "iteration 43 | evaluation\n",
      "agent vs random - win ratio 0.79 - draw ratio 0.0\n",
      "Number of seen trajectories: 4400\n",
      "Number of unique trajectories: 3359\n",
      "iteration 44 | self-play\n",
      "saving memory position_memory_tictactoe_50_ep_44\n",
      "iteration 44 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 3, 3, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 9)\n",
      "Train on 4096 samples, validate on 1024 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 1s 176us/step - loss: 6.1365 - value_loss: 0.6144 - policy_loss: 1.6379 - val_loss: 6.1717 - val_value_loss: 0.6649 - val_policy_loss: 1.6579\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1373 - value_loss: 0.6150 - policy_loss: 1.6389 - val_loss: 6.1716 - val_value_loss: 0.6647 - val_policy_loss: 1.6579\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1368 - value_loss: 0.6143 - policy_loss: 1.6387 - val_loss: 6.1715 - val_value_loss: 0.6644 - val_policy_loss: 1.6579\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1367 - value_loss: 0.6144 - policy_loss: 1.6384 - val_loss: 6.1714 - val_value_loss: 0.6641 - val_policy_loss: 1.6579\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1355 - value_loss: 0.6123 - policy_loss: 1.6380 - val_loss: 6.1712 - val_value_loss: 0.6639 - val_policy_loss: 1.6579\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1351 - value_loss: 0.6115 - policy_loss: 1.6380 - val_loss: 6.1711 - val_value_loss: 0.6636 - val_policy_loss: 1.6579\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1349 - value_loss: 0.6107 - policy_loss: 1.6385 - val_loss: 6.1710 - val_value_loss: 0.6634 - val_policy_loss: 1.6579\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1353 - value_loss: 0.6112 - policy_loss: 1.6387 - val_loss: 6.1709 - val_value_loss: 0.6632 - val_policy_loss: 1.6579\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1351 - value_loss: 0.6109 - policy_loss: 1.6387 - val_loss: 6.1708 - val_value_loss: 0.6629 - val_policy_loss: 1.6580\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1345 - value_loss: 0.6099 - policy_loss: 1.6384 - val_loss: 6.1706 - val_value_loss: 0.6627 - val_policy_loss: 1.6580\n",
      "Saved model  tictactoe_50_44\n",
      "iteration 44 | evaluation\n",
      "agent vs random - win ratio 0.82 - draw ratio 0.02\n",
      "Number of seen trajectories: 4500\n",
      "Number of unique trajectories: 3420\n",
      "iteration 45 | self-play\n",
      "saving memory position_memory_tictactoe_50_ep_45\n",
      "iteration 45 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 3, 3, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 9)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 1s 177us/step - loss: 6.1242 - value_loss: 0.5950 - policy_loss: 1.6329 - val_loss: 6.1228 - val_value_loss: 0.5798 - val_policy_loss: 1.6452\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1247 - value_loss: 0.5956 - policy_loss: 1.6331 - val_loss: 6.1226 - val_value_loss: 0.5795 - val_policy_loss: 1.6452\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1243 - value_loss: 0.5951 - policy_loss: 1.6327 - val_loss: 6.1225 - val_value_loss: 0.5792 - val_policy_loss: 1.6452\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1236 - value_loss: 0.5936 - policy_loss: 1.6330 - val_loss: 6.1223 - val_value_loss: 0.5789 - val_policy_loss: 1.6452\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1250 - value_loss: 0.5966 - policy_loss: 1.6327 - val_loss: 6.1222 - val_value_loss: 0.5786 - val_policy_loss: 1.6452\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1233 - value_loss: 0.5933 - policy_loss: 1.6327 - val_loss: 6.1221 - val_value_loss: 0.5783 - val_policy_loss: 1.6452\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1235 - value_loss: 0.5936 - policy_loss: 1.6328 - val_loss: 6.1219 - val_value_loss: 0.5781 - val_policy_loss: 1.6452\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1230 - value_loss: 0.5928 - policy_loss: 1.6325 - val_loss: 6.1218 - val_value_loss: 0.5778 - val_policy_loss: 1.6452\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1237 - value_loss: 0.5942 - policy_loss: 1.6325 - val_loss: 6.1217 - val_value_loss: 0.5776 - val_policy_loss: 1.6452\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1224 - value_loss: 0.5916 - policy_loss: 1.6326 - val_loss: 6.1216 - val_value_loss: 0.5774 - val_policy_loss: 1.6452\n",
      "Saved model  tictactoe_50_45\n",
      "iteration 45 | evaluation\n",
      "agent vs random - win ratio 0.87 - draw ratio 0.04\n",
      "Number of seen trajectories: 4600\n",
      "Number of unique trajectories: 3480\n",
      "iteration 46 | self-play\n",
      "saving memory position_memory_tictactoe_50_ep_46\n",
      "iteration 46 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 3, 3, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 9)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 1s 176us/step - loss: 6.1177 - value_loss: 0.5786 - policy_loss: 1.6361 - val_loss: 6.1222 - val_value_loss: 0.5622 - val_policy_loss: 1.6616\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1175 - value_loss: 0.5781 - policy_loss: 1.6363 - val_loss: 6.1221 - val_value_loss: 0.5620 - val_policy_loss: 1.6616\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1180 - value_loss: 0.5786 - policy_loss: 1.6368 - val_loss: 6.1220 - val_value_loss: 0.5617 - val_policy_loss: 1.6615\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1181 - value_loss: 0.5793 - policy_loss: 1.6362 - val_loss: 6.1219 - val_value_loss: 0.5615 - val_policy_loss: 1.6615\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1167 - value_loss: 0.5768 - policy_loss: 1.6359 - val_loss: 6.1218 - val_value_loss: 0.5613 - val_policy_loss: 1.6615\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1173 - value_loss: 0.5777 - policy_loss: 1.6363 - val_loss: 6.1217 - val_value_loss: 0.5611 - val_policy_loss: 1.6615\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1170 - value_loss: 0.5770 - policy_loss: 1.6364 - val_loss: 6.1216 - val_value_loss: 0.5609 - val_policy_loss: 1.6615\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1163 - value_loss: 0.5757 - policy_loss: 1.6363 - val_loss: 6.1215 - val_value_loss: 0.5608 - val_policy_loss: 1.6615\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1174 - value_loss: 0.5781 - policy_loss: 1.6361 - val_loss: 6.1214 - val_value_loss: 0.5606 - val_policy_loss: 1.6615\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1169 - value_loss: 0.5770 - policy_loss: 1.6362 - val_loss: 6.1213 - val_value_loss: 0.5604 - val_policy_loss: 1.6615\n",
      "Saved model  tictactoe_50_46\n",
      "iteration 46 | evaluation\n",
      "agent vs random - win ratio 0.85 - draw ratio 0.02\n",
      "Number of seen trajectories: 4700\n",
      "Number of unique trajectories: 3540\n",
      "iteration 47 | self-play\n",
      "saving memory position_memory_tictactoe_50_ep_47\n",
      "iteration 47 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 3, 3, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 9)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 1s 175us/step - loss: 6.1264 - value_loss: 0.5919 - policy_loss: 1.6403 - val_loss: 6.1045 - val_value_loss: 0.5910 - val_policy_loss: 1.5973\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1274 - value_loss: 0.5935 - policy_loss: 1.6407 - val_loss: 6.1044 - val_value_loss: 0.5907 - val_policy_loss: 1.5973\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1258 - value_loss: 0.5910 - policy_loss: 1.6399 - val_loss: 6.1042 - val_value_loss: 0.5905 - val_policy_loss: 1.5973\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1258 - value_loss: 0.5913 - policy_loss: 1.6397 - val_loss: 6.1041 - val_value_loss: 0.5903 - val_policy_loss: 1.5973\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1264 - value_loss: 0.5918 - policy_loss: 1.6403 - val_loss: 6.1040 - val_value_loss: 0.5901 - val_policy_loss: 1.5973\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1254 - value_loss: 0.5904 - policy_loss: 1.6397 - val_loss: 6.1039 - val_value_loss: 0.5899 - val_policy_loss: 1.5973\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1253 - value_loss: 0.5897 - policy_loss: 1.6402 - val_loss: 6.1038 - val_value_loss: 0.5897 - val_policy_loss: 1.5973\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1250 - value_loss: 0.5896 - policy_loss: 1.6398 - val_loss: 6.1037 - val_value_loss: 0.5895 - val_policy_loss: 1.5973\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1254 - value_loss: 0.5904 - policy_loss: 1.6399 - val_loss: 6.1036 - val_value_loss: 0.5894 - val_policy_loss: 1.5973\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1245 - value_loss: 0.5882 - policy_loss: 1.6402 - val_loss: 6.1035 - val_value_loss: 0.5892 - val_policy_loss: 1.5973\n",
      "Saved model  tictactoe_50_47\n",
      "iteration 47 | evaluation\n",
      "agent vs random - win ratio 0.79 - draw ratio 0.01\n",
      "Number of seen trajectories: 4800\n",
      "Number of unique trajectories: 3588\n",
      "iteration 48 | self-play\n",
      "saving memory position_memory_tictactoe_50_ep_48\n",
      "iteration 48 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 3, 3, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 9)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 1s 176us/step - loss: 6.1404 - value_loss: 0.6259 - policy_loss: 1.6343 - val_loss: 6.1484 - val_value_loss: 0.6199 - val_policy_loss: 1.6563\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1389 - value_loss: 0.6231 - policy_loss: 1.6341 - val_loss: 6.1484 - val_value_loss: 0.6199 - val_policy_loss: 1.6563\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1394 - value_loss: 0.6235 - policy_loss: 1.6347 - val_loss: 6.1484 - val_value_loss: 0.6200 - val_policy_loss: 1.6563\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1389 - value_loss: 0.6229 - policy_loss: 1.6344 - val_loss: 6.1485 - val_value_loss: 0.6200 - val_policy_loss: 1.6562\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1397 - value_loss: 0.6235 - policy_loss: 1.6353 - val_loss: 6.1485 - val_value_loss: 0.6201 - val_policy_loss: 1.6562\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1380 - value_loss: 0.6210 - policy_loss: 1.6345 - val_loss: 6.1485 - val_value_loss: 0.6201 - val_policy_loss: 1.6562\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1389 - value_loss: 0.6227 - policy_loss: 1.6346 - val_loss: 6.1485 - val_value_loss: 0.6202 - val_policy_loss: 1.6562\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1388 - value_loss: 0.6228 - policy_loss: 1.6343 - val_loss: 6.1485 - val_value_loss: 0.6202 - val_policy_loss: 1.6562\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1393 - value_loss: 0.6233 - policy_loss: 1.6347 - val_loss: 6.1485 - val_value_loss: 0.6203 - val_policy_loss: 1.6562\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1389 - value_loss: 0.6228 - policy_loss: 1.6343 - val_loss: 6.1486 - val_value_loss: 0.6203 - val_policy_loss: 1.6562\n",
      "Saved model  tictactoe_50_48\n",
      "iteration 48 | evaluation\n",
      "agent vs random - win ratio 0.76 - draw ratio 0.03\n",
      "Number of seen trajectories: 4900\n",
      "Number of unique trajectories: 3653\n",
      "iteration 49 | self-play\n",
      "saving memory position_memory_tictactoe_50_ep_49\n",
      "iteration 49 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 3, 3, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 9)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 1s 178us/step - loss: 6.1287 - value_loss: 0.6306 - policy_loss: 1.6062 - val_loss: 6.1163 - val_value_loss: 0.6123 - val_policy_loss: 1.5997\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1287 - value_loss: 0.6306 - policy_loss: 1.6062 - val_loss: 6.1163 - val_value_loss: 0.6121 - val_policy_loss: 1.5998\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1276 - value_loss: 0.6283 - policy_loss: 1.6063 - val_loss: 6.1162 - val_value_loss: 0.6120 - val_policy_loss: 1.5998\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1278 - value_loss: 0.6289 - policy_loss: 1.6061 - val_loss: 6.1161 - val_value_loss: 0.6118 - val_policy_loss: 1.5998\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1285 - value_loss: 0.6303 - policy_loss: 1.6061 - val_loss: 6.1161 - val_value_loss: 0.6117 - val_policy_loss: 1.5999\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1274 - value_loss: 0.6282 - policy_loss: 1.6060 - val_loss: 6.1160 - val_value_loss: 0.6115 - val_policy_loss: 1.5999\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1283 - value_loss: 0.6297 - policy_loss: 1.6064 - val_loss: 6.1160 - val_value_loss: 0.6114 - val_policy_loss: 1.6000\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1280 - value_loss: 0.6287 - policy_loss: 1.6067 - val_loss: 6.1160 - val_value_loss: 0.6113 - val_policy_loss: 1.6000\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 1s 172us/step - loss: 6.1273 - value_loss: 0.6275 - policy_loss: 1.6066 - val_loss: 6.1159 - val_value_loss: 0.6112 - val_policy_loss: 1.6000\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 1s 171us/step - loss: 6.1275 - value_loss: 0.6282 - policy_loss: 1.6062 - val_loss: 6.1159 - val_value_loss: 0.6111 - val_policy_loss: 1.6001\n",
      "Saved model  tictactoe_50_49\n",
      "iteration 49 | evaluation\n",
      "agent vs random - win ratio 0.88 - draw ratio 0.01\n",
      "Number of seen trajectories: 5000\n",
      "Number of unique trajectories: 3707\n"
     ]
    }
   ],
   "source": [
    "wins, draws, seen_trajectories, unique_trajectories = az_pipeline.run(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wins: [0.62, 0.69, 0.7, 0.72, 0.72, 0.75, 0.76, 0.77, 0.83, 0.86, 0.85, 0.81, 0.81, 0.84, 0.85, 0.82, 0.83, 0.83, 0.81, 0.81, 0.79, 0.78, 0.83, 0.78, 0.81, 0.78, 0.84, 0.87, 0.83, 0.9, 0.84, 0.79, 0.8, 0.87, 0.88, 0.83, 0.84, 0.84, 0.82, 0.8, 0.83, 0.84, 0.8, 0.79, 0.82, 0.87, 0.85, 0.79, 0.76, 0.88]\n",
      "draws: [0.06, 0.05, 0.04, 0.06, 0.08, 0.04, 0.02, 0.02, 0.01, 0.01, 0.04, 0.02, 0.01, 0.01, 0.0, 0.01, 0.07, 0.0, 0.03, 0.01, 0.02, 0.02, 0.01, 0.02, 0.02, 0.01, 0.0, 0.0, 0.0, 0.02, 0.0, 0.04, 0.01, 0.01, 0.01, 0.0, 0.01, 0.02, 0.03, 0.01, 0.02, 0.0, 0.05, 0.0, 0.02, 0.04, 0.02, 0.01, 0.03, 0.01]\n",
      "seen_trajectories: [ 100.  200.  300.  400.  500.  600.  700.  800.  900. 1000. 1100. 1200.\n",
      " 1300. 1400. 1500. 1600. 1700. 1800. 1900. 2000. 2100. 2200. 2300. 2400.\n",
      " 2500. 2600. 2700. 2800. 2900. 3000. 3100. 3200. 3300. 3400. 3500. 3600.\n",
      " 3700. 3800. 3900. 4000. 4100. 4200. 4300. 4400. 4500. 4600. 4700. 4800.\n",
      " 4900. 5000.]\n",
      "unique_trajectories: [ 100.  200.  299.  397.  494.  591.  687.  780.  872.  958. 1044. 1138.\n",
      " 1228. 1311. 1395. 1476. 1558. 1631. 1713. 1787. 1867. 1940. 2012. 2085.\n",
      " 2162. 2238. 2312. 2373. 2445. 2513. 2584. 2641. 2699. 2762. 2828. 2891.\n",
      " 2949. 3014. 3079. 3141. 3202. 3248. 3305. 3359. 3420. 3480. 3540. 3588.\n",
      " 3653. 3707.]\n"
     ]
    }
   ],
   "source": [
    "print(\"wins: {}\" .format(wins))\n",
    "print(\"draws: {}\" .format(draws))\n",
    "print(\"seen_trajectories: {}\" .format(seen_trajectories))\n",
    "print(\"unique_trajectories: {}\" .format(unique_trajectories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model tictactoe_200_199\n"
     ]
    }
   ],
   "source": [
    "position_memory = memory.PositionMemory(variant=\"TicTacToe\")\n",
    "last_model = model.AZModel(\n",
    "    memory=position_memory,\n",
    "    input_shape=[3,3,3],\n",
    "    num_possible_moves=9,\n",
    "    model_id=\"tictactoe_50\"\n",
    ")\n",
    "last_model.load(199)\n",
    "last_agent = agent.AlphaZeroAgent(variant=\"TicTacToe\", model=last_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========\n",
      " 1 | 0 | 0\n",
      " 0 | 0 | 0\n",
      " 0 | 0 | 0\n",
      "-----------\n",
      "value: 0.46559569239616394\n",
      "policy:\n",
      " 0.0 | 0.1281755417585373 | 0.12269657850265503\n",
      " 0.12649105489253998 | 0.127775639295578 | 0.12690158188343048\n",
      " 0.11899434030056 | 0.1240631490945816 | 0.12490214407444\n",
      "===========\n",
      "choose from [0 1 1 1 1 1 1 1 1] :4\n",
      "===========\n",
      " 1 | 0 | 0\n",
      " 0 | 2 | 0\n",
      " 0 | 0 | 0\n",
      "-----------\n",
      "value: -0.19915524125099182\n",
      "policy:\n",
      " 0.0 | 0.14660902321338654 | 0.14949288964271545\n",
      " 0.14317259192466736 | 0.0 | 0.13867177069187164\n",
      " 0.15232343971729279 | 0.1282029151916504 | 0.14152741432189941\n",
      "===========\n",
      "===========\n",
      " 1 | 1 | 0\n",
      " 0 | 2 | 0\n",
      " 0 | 0 | 0\n",
      "-----------\n",
      "value: 0.8018612861633301\n",
      "policy:\n",
      " 0.0 | 0.0 | 0.15995465219020844\n",
      " 0.22592492401599884 | 0.0 | 0.1442735642194748\n",
      " 0.1973903924226761 | 0.1485459804534912 | 0.12391046434640884\n",
      "===========\n",
      "choose from [0 0 1 1 0 1 1 1 1] :2\n",
      "===========\n",
      " 1 | 1 | 2\n",
      " 0 | 2 | 0\n",
      " 0 | 0 | 0\n",
      "-----------\n",
      "value: 0.6975383758544922\n",
      "policy:\n",
      " 0.0 | 0.0 | 0.0\n",
      " 0.4014134109020233 | 0.0 | 0.12569226324558258\n",
      " 0.24514320492744446 | 0.11772617697715759 | 0.11002487689256668\n",
      "===========\n",
      "===========\n",
      " 1 | 1 | 2\n",
      " 1 | 2 | 0\n",
      " 0 | 0 | 0\n",
      "-----------\n",
      "value: -0.9516979455947876\n",
      "policy:\n",
      " 0.0 | 0.0 | 0.0\n",
      " 0.0 | 0.0 | 0.0030483563896268606\n",
      " 0.9898093342781067 | 0.0041738273575901985 | 0.002968493616208434\n",
      "===========\n",
      "choose from [0 0 0 0 0 1 1 1 1] :6\n",
      "===========\n",
      " 1 | 1 | 2\n",
      " 1 | 2 | 0\n",
      " 2 | 0 | 0\n",
      "-----------\n",
      "value: 0.19888073205947876\n",
      "policy:\n",
      " 0.0 | 0.0 | 0.0\n",
      " 0.0 | 0.0 | 0.1732681542634964\n",
      " 0.0 | 0.3697223961353302 | 0.457009494304657\n",
      "===========\n",
      "Player -1 won the game after 6 turns.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_vs_player(last_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========\n",
      " 1 | 0 | 0\n",
      " 0 | 0 | 0\n",
      " 0 | 0 | 0\n",
      "-----------\n",
      "value: 0.46559569239616394\n",
      "policy:\n",
      " 0.0 | 0.1281755417585373 | 0.12269657850265503\n",
      " 0.12649105489253998 | 0.127775639295578 | 0.12690158188343048\n",
      " 0.11899434030056 | 0.1240631490945816 | 0.12490214407444\n",
      "===========\n",
      "choose from [0 1 1 1 1 1 1 1 1] :1\n",
      "===========\n",
      " 1 | 2 | 0\n",
      " 0 | 0 | 0\n",
      " 0 | 0 | 0\n",
      "-----------\n",
      "value: -0.6387047171592712\n",
      "policy:\n",
      " 0.0 | 0.0 | 0.1391703188419342\n",
      " 0.150371253490448 | 0.15748624503612518 | 0.1410897821187973\n",
      " 0.13433045148849487 | 0.1351022571325302 | 0.1424497663974762\n",
      "===========\n",
      "===========\n",
      " 1 | 2 | 0\n",
      " 1 | 0 | 0\n",
      " 0 | 0 | 0\n",
      "-----------\n",
      "value: 0.9341468811035156\n",
      "policy:\n",
      " 0.0 | 0.0 | 0.13794970512390137\n",
      " 0.0 | 0.19776815176010132 | 0.1498076617717743\n",
      " 0.14616642892360687 | 0.21535807847976685 | 0.15294994413852692\n",
      "===========\n",
      "choose from [0 0 1 0 1 1 1 1 1] :6\n",
      "===========\n",
      " 1 | 2 | 0\n",
      " 1 | 0 | 0\n",
      " 2 | 0 | 0\n",
      "-----------\n",
      "value: -0.6676097512245178\n",
      "policy:\n",
      " 0.0 | 0.0 | 0.10814708471298218\n",
      " 0.0 | 0.4900229871273041 | 0.18986505270004272\n",
      " 0.0 | 0.09798500686883926 | 0.11397992074489594\n",
      "===========\n",
      "===========\n",
      " 1 | 2 | 0\n",
      " 1 | 1 | 0\n",
      " 2 | 0 | 0\n",
      "-----------\n",
      "value: 0.967418372631073\n",
      "policy:\n",
      " 0.0 | 0.0 | 0.24063631892204285\n",
      " 0.0 | 0.0 | 0.09246817976236343\n",
      " 0.0 | 0.47813680768013 | 0.18875867128372192\n",
      "===========\n",
      "choose from [0 0 1 0 0 1 0 1 1] :8\n",
      "===========\n",
      " 1 | 2 | 0\n",
      " 1 | 1 | 0\n",
      " 2 | 0 | 2\n",
      "-----------\n",
      "value: -0.977812647819519\n",
      "policy:\n",
      " 0.0 | 0.0 | 0.00848178006708622\n",
      " 0.0 | 0.0 | 0.9829579591751099\n",
      " 0.0 | 0.00856025516986847 | 0.0\n",
      "===========\n",
      "===========\n",
      " 1 | 2 | 0\n",
      " 1 | 1 | 1\n",
      " 2 | 0 | 2\n",
      "-----------\n",
      "value: -0.7381231188774109\n",
      "policy:\n",
      " 0.0 | 0.0 | 0.002756534144282341\n",
      " 0.0 | 0.0 | 0.0\n",
      " 0.0 | 0.9972434639930725 | 0.0\n",
      "===========\n",
      "Player 1 won the game after 7 turns.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_vs_player(last_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model tictactoe_200_36\n"
     ]
    }
   ],
   "source": [
    "best_version = 36\n",
    "\n",
    "position_memory = memory.PositionMemory(variant=\"TicTacToe\")\n",
    "best_model = model.AZModel(\n",
    "    memory=position_memory,\n",
    "    input_shape=[3,3,3],\n",
    "    num_possible_moves=9,\n",
    "    model_id=\"tictactoe_50\"\n",
    ")\n",
    "best_model.load(best_version)\n",
    "best_agent = agent.AlphaZeroAgent(variant=\"TicTacToe\", model=best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========\n",
      " 1 | 0 | 0\n",
      " 0 | 0 | 0\n",
      " 0 | 0 | 0\n",
      "-----------\n",
      "value: 0.6146203279495239\n",
      "policy:\n",
      " 0.0 | 0.12209516018629074 | 0.12502822279930115\n",
      " 0.12881030142307281 | 0.12327451258897781 | 0.12835565209388733\n",
      " 0.12000666558742523 | 0.12225155532360077 | 0.13017794489860535\n",
      "===========\n",
      "choose from [0 1 1 1 1 1 1 1 1] :4\n",
      "===========\n",
      " 1 | 0 | 0\n",
      " 0 | 2 | 0\n",
      " 0 | 0 | 0\n",
      "-----------\n",
      "value: -0.5808872580528259\n",
      "policy:\n",
      " 0.0 | 0.15039192140102386 | 0.14112643897533417\n",
      " 0.1749834567308426 | 0.0 | 0.17335224151611328\n",
      " 0.13449956476688385 | 0.13255955278873444 | 0.09308679401874542\n",
      "===========\n",
      "===========\n",
      " 1 | 0 | 0\n",
      " 1 | 2 | 0\n",
      " 0 | 0 | 0\n",
      "-----------\n",
      "value: 0.8508213758468628\n",
      "policy:\n",
      " 0.0 | 0.22177645564079285 | 0.2331078201532364\n",
      " 0.0 | 0.0 | 0.11355749517679214\n",
      " 0.16143296658992767 | 0.15127594769001007 | 0.11884935945272446\n",
      "===========\n",
      "choose from [0 1 1 0 0 1 1 1 1] :6\n",
      "===========\n",
      " 1 | 0 | 0\n",
      " 1 | 2 | 0\n",
      " 2 | 0 | 0\n",
      "-----------\n",
      "value: 0.5356945991516113\n",
      "policy:\n",
      " 0.0 | 0.43316930532455444 | 0.20491187274456024\n",
      " 0.0 | 0.0 | 0.1537821739912033\n",
      " 0.0 | 0.14758042991161346 | 0.060556262731552124\n",
      "===========\n",
      "===========\n",
      " 1 | 1 | 0\n",
      " 1 | 2 | 0\n",
      " 2 | 0 | 0\n",
      "-----------\n",
      "value: -0.8744661211967468\n",
      "policy:\n",
      " 0.0 | 0.0 | 0.9808607697486877\n",
      " 0.0 | 0.0 | 0.006694367621093988\n",
      " 0.0 | 0.006103879772126675 | 0.006341004278510809\n",
      "===========\n",
      "choose from [0 0 1 0 0 1 0 1 1] :2\n",
      "===========\n",
      " 1 | 1 | 2\n",
      " 1 | 2 | 0\n",
      " 2 | 0 | 0\n",
      "-----------\n",
      "value: 0.4967239797115326\n",
      "policy:\n",
      " 0.0 | 0.0 | 0.0\n",
      " 0.0 | 0.0 | 0.18555238842964172\n",
      " 0.0 | 0.28752434253692627 | 0.5269232988357544\n",
      "===========\n",
      "Player -1 won the game after 6 turns.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_vs_player(best_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========\n",
      " 1 | 0 | 0\n",
      " 0 | 0 | 0\n",
      " 0 | 0 | 0\n",
      "-----------\n",
      "value: 0.6146203279495239\n",
      "policy:\n",
      " 0.0 | 0.12209516018629074 | 0.12502822279930115\n",
      " 0.12881030142307281 | 0.12327451258897781 | 0.12835565209388733\n",
      " 0.12000666558742523 | 0.12225155532360077 | 0.13017794489860535\n",
      "===========\n",
      "choose from [0 1 1 1 1 1 1 1 1] :2\n",
      "===========\n",
      " 1 | 0 | 2\n",
      " 0 | 0 | 0\n",
      " 0 | 0 | 0\n",
      "-----------\n",
      "value: -0.5626295804977417\n",
      "policy:\n",
      " 0.0 | 0.1140308678150177 | 0.0\n",
      " 0.21480697393417358 | 0.155502587556839 | 0.12961216270923615\n",
      " 0.12150993943214417 | 0.12859079241752625 | 0.13594666123390198\n",
      "===========\n",
      "===========\n",
      " 1 | 0 | 2\n",
      " 1 | 0 | 0\n",
      " 0 | 0 | 0\n",
      "-----------\n",
      "value: 0.9116694331169128\n",
      "policy:\n",
      " 0.0 | 0.1515212208032608 | 0.0\n",
      " 0.0 | 0.1473388969898224 | 0.17324647307395935\n",
      " 0.21073423326015472 | 0.17555497586727142 | 0.14160427451133728\n",
      "===========\n",
      "choose from [0 1 0 0 1 1 1 1 1] :6\n",
      "===========\n",
      " 1 | 0 | 2\n",
      " 1 | 0 | 0\n",
      " 2 | 0 | 0\n",
      "-----------\n",
      "value: -0.4170190393924713\n",
      "policy:\n",
      " 0.0 | 0.1741737425327301 | 0.0\n",
      " 0.0 | 0.42813751101493835 | 0.17048828303813934\n",
      " 0.0 | 0.09888597577810287 | 0.1283145248889923\n",
      "===========\n",
      "===========\n",
      " 1 | 0 | 2\n",
      " 1 | 1 | 0\n",
      " 2 | 0 | 0\n",
      "-----------\n",
      "value: 0.9142457246780396\n",
      "policy:\n",
      " 0.0 | 0.09538015723228455 | 0.0\n",
      " 0.0 | 0.0 | 0.13372491300106049\n",
      " 0.0 | 0.35632023215293884 | 0.4145747423171997\n",
      "===========\n",
      "choose from [0 1 0 0 0 1 0 1 1] :5\n",
      "===========\n",
      " 1 | 0 | 2\n",
      " 1 | 1 | 2\n",
      " 2 | 0 | 0\n",
      "-----------\n",
      "value: -0.616674542427063\n",
      "policy:\n",
      " 0.0 | 0.05063515529036522 | 0.0\n",
      " 0.0 | 0.0 | 0.0\n",
      " 0.0 | 0.03537009656429291 | 0.9139947295188904\n",
      "===========\n",
      "===========\n",
      " 1 | 0 | 2\n",
      " 1 | 1 | 2\n",
      " 2 | 0 | 1\n",
      "-----------\n",
      "value: -0.568318247795105\n",
      "policy:\n",
      " 0.0 | 0.5900267958641052 | 0.0\n",
      " 0.0 | 0.0 | 0.0\n",
      " 0.0 | 0.4099732041358948 | 0.0\n",
      "===========\n",
      "Player 1 won the game after 7 turns.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_vs_player(best_agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ConnectFour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "az_pipeline = Pipeline(\"connectfour_50\", \"Connect4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 | self-play\n",
      "saving memory position_memory_connectfour_50_ep_0\n",
      "iteration 0 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (2690, 6, 7, 3)\n",
      "model_y_outcomes: (2690,)\n",
      "model_y_probabilities: (2690, 7)\n",
      "Train on 2152 samples, validate on 538 samples\n",
      "Epoch 1/10\n",
      "2152/2152 [==============================] - 5s 2ms/step - loss: 6.8204 - value_loss: 1.2359 - policy_loss: 2.1690 - val_loss: 7.1597 - val_value_loss: 1.9773 - val_policy_loss: 2.1064\n",
      "Epoch 2/10\n",
      "2152/2152 [==============================] - 1s 467us/step - loss: 6.7592 - value_loss: 1.1985 - policy_loss: 2.0843 - val_loss: 7.2657 - val_value_loss: 2.1993 - val_policy_loss: 2.0968\n",
      "Epoch 3/10\n",
      "2152/2152 [==============================] - 1s 468us/step - loss: 6.6802 - value_loss: 1.0685 - policy_loss: 2.0567 - val_loss: 6.8715 - val_value_loss: 1.4274 - val_policy_loss: 2.0807\n",
      "Epoch 4/10\n",
      "2152/2152 [==============================] - 1s 470us/step - loss: 6.5967 - value_loss: 0.9253 - policy_loss: 2.0334 - val_loss: 7.3972 - val_value_loss: 2.4793 - val_policy_loss: 2.0806\n",
      "Epoch 5/10\n",
      "2152/2152 [==============================] - 1s 468us/step - loss: 6.5128 - value_loss: 0.7728 - policy_loss: 2.0184 - val_loss: 6.8847 - val_value_loss: 1.4573 - val_policy_loss: 2.0779\n",
      "Epoch 6/10\n",
      "2152/2152 [==============================] - 1s 469us/step - loss: 6.5774 - value_loss: 0.9166 - policy_loss: 2.0042 - val_loss: 7.1136 - val_value_loss: 1.9092 - val_policy_loss: 2.0843\n",
      "Epoch 7/10\n",
      "2152/2152 [==============================] - 1s 468us/step - loss: 6.3544 - value_loss: 0.4836 - policy_loss: 1.9916 - val_loss: 7.0114 - val_value_loss: 1.7223 - val_policy_loss: 2.0671\n",
      "Epoch 8/10\n",
      "2152/2152 [==============================] - 1s 468us/step - loss: 6.4975 - value_loss: 0.7807 - policy_loss: 1.9811 - val_loss: 7.2501 - val_value_loss: 2.2094 - val_policy_loss: 2.0579\n",
      "Epoch 9/10\n",
      "2152/2152 [==============================] - 1s 468us/step - loss: 6.3556 - value_loss: 0.5042 - policy_loss: 1.9741 - val_loss: 7.0489 - val_value_loss: 1.8075 - val_policy_loss: 2.0578\n",
      "Epoch 10/10\n",
      "2152/2152 [==============================] - 1s 468us/step - loss: 6.4145 - value_loss: 0.6313 - policy_loss: 1.9653 - val_loss: 7.0454 - val_value_loss: 1.7859 - val_policy_loss: 2.0728\n",
      "Saved model  connectfour_50_0\n",
      "iteration 0 | evaluation\n",
      "agent vs random - win ratio 0.66 - draw ratio 0.0\n",
      "Number of seen trajectories: 100\n",
      "Number of unique trajectories: 100\n",
      "iteration 1 | self-play\n",
      "saving memory position_memory_connectfour_50_ep_1\n",
      "iteration 1 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 6, 7, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 7)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 2s 455us/step - loss: 6.5815 - value_loss: 0.9123 - policy_loss: 2.0188 - val_loss: 6.6363 - val_value_loss: 1.0203 - val_policy_loss: 2.0209\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.4794 - value_loss: 0.7358 - policy_loss: 1.9918 - val_loss: 6.4892 - val_value_loss: 0.7408 - val_policy_loss: 2.0068\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.4855 - value_loss: 0.7690 - policy_loss: 1.9715 - val_loss: 6.4756 - val_value_loss: 0.7252 - val_policy_loss: 1.9959\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.3985 - value_loss: 0.6087 - policy_loss: 1.9585 - val_loss: 6.4238 - val_value_loss: 0.6294 - val_policy_loss: 1.9887\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.3415 - value_loss: 0.5107 - policy_loss: 1.9430 - val_loss: 6.4355 - val_value_loss: 0.6593 - val_policy_loss: 1.9829\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.3495 - value_loss: 0.5376 - policy_loss: 1.9328 - val_loss: 6.3848 - val_value_loss: 0.5667 - val_policy_loss: 1.9748\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.2556 - value_loss: 0.3598 - policy_loss: 1.9235 - val_loss: 6.3558 - val_value_loss: 0.5130 - val_policy_loss: 1.9711\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.2538 - value_loss: 0.3648 - policy_loss: 1.9156 - val_loss: 6.5113 - val_value_loss: 0.8278 - val_policy_loss: 1.9680\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.2876 - value_loss: 0.4420 - policy_loss: 1.9067 - val_loss: 6.4312 - val_value_loss: 0.6733 - val_policy_loss: 1.9629\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.2374 - value_loss: 0.3498 - policy_loss: 1.8990 - val_loss: 6.3404 - val_value_loss: 0.4977 - val_policy_loss: 1.9576\n",
      "Saved model  connectfour_50_1\n",
      "iteration 1 | evaluation\n",
      "agent vs random - win ratio 0.66 - draw ratio 0.0\n",
      "Number of seen trajectories: 200\n",
      "Number of unique trajectories: 200\n",
      "iteration 2 | self-play\n",
      "saving memory position_memory_connectfour_50_ep_2\n",
      "iteration 2 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 6, 7, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 7)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 2s 454us/step - loss: 6.5029 - value_loss: 0.8043 - policy_loss: 1.9762 - val_loss: 6.4645 - val_value_loss: 0.7273 - val_policy_loss: 1.9767\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 2s 450us/step - loss: 6.4686 - value_loss: 0.7511 - policy_loss: 1.9616 - val_loss: 6.4838 - val_value_loss: 0.7713 - val_policy_loss: 1.9719\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.4250 - value_loss: 0.6759 - policy_loss: 1.9502 - val_loss: 6.4524 - val_value_loss: 0.7136 - val_policy_loss: 1.9676\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.4236 - value_loss: 0.6826 - policy_loss: 1.9413 - val_loss: 6.4653 - val_value_loss: 0.7442 - val_policy_loss: 1.9635\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.3409 - value_loss: 0.5269 - policy_loss: 1.9321 - val_loss: 6.4346 - val_value_loss: 0.6874 - val_policy_loss: 1.9595\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.2839 - value_loss: 0.4214 - policy_loss: 1.9244 - val_loss: 6.4061 - val_value_loss: 0.6339 - val_policy_loss: 1.9566\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.2939 - value_loss: 0.4509 - policy_loss: 1.9156 - val_loss: 6.5408 - val_value_loss: 0.9035 - val_policy_loss: 1.9571\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 2s 453us/step - loss: 6.3198 - value_loss: 0.5086 - policy_loss: 1.9103 - val_loss: 6.5300 - val_value_loss: 0.8841 - val_policy_loss: 1.9555\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.2913 - value_loss: 0.4586 - policy_loss: 1.9039 - val_loss: 6.3732 - val_value_loss: 0.5763 - val_policy_loss: 1.9504\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.2286 - value_loss: 0.3400 - policy_loss: 1.8976 - val_loss: 6.3909 - val_value_loss: 0.6162 - val_policy_loss: 1.9466\n",
      "Saved model  connectfour_50_2\n",
      "iteration 2 | evaluation\n",
      "agent vs random - win ratio 0.69 - draw ratio 0.0\n",
      "Number of seen trajectories: 300\n",
      "Number of unique trajectories: 300\n",
      "iteration 3 | self-play\n",
      "saving memory position_memory_connectfour_50_ep_3\n",
      "iteration 3 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 6, 7, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 7)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 2s 453us/step - loss: 6.5256 - value_loss: 0.8633 - policy_loss: 1.9692 - val_loss: 6.4280 - val_value_loss: 0.6735 - val_policy_loss: 1.9641\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 2s 450us/step - loss: 6.3962 - value_loss: 0.6194 - policy_loss: 1.9548 - val_loss: 6.4040 - val_value_loss: 0.6299 - val_policy_loss: 1.9604\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.3933 - value_loss: 0.6271 - policy_loss: 1.9420 - val_loss: 6.5325 - val_value_loss: 0.8919 - val_policy_loss: 1.9560\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.3981 - value_loss: 0.6486 - policy_loss: 1.9307 - val_loss: 6.4096 - val_value_loss: 0.6492 - val_policy_loss: 1.9535\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.3179 - value_loss: 0.4965 - policy_loss: 1.9230 - val_loss: 6.4490 - val_value_loss: 0.7328 - val_policy_loss: 1.9495\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.3315 - value_loss: 0.5334 - policy_loss: 1.9139 - val_loss: 6.4848 - val_value_loss: 0.8076 - val_policy_loss: 1.9467\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.2709 - value_loss: 0.4204 - policy_loss: 1.9065 - val_loss: 6.3626 - val_value_loss: 0.5656 - val_policy_loss: 1.9452\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.2443 - value_loss: 0.3742 - policy_loss: 1.9001 - val_loss: 6.3710 - val_value_loss: 0.5859 - val_policy_loss: 1.9423\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.2511 - value_loss: 0.3938 - policy_loss: 1.8948 - val_loss: 6.4055 - val_value_loss: 0.6576 - val_policy_loss: 1.9402\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.2344 - value_loss: 0.3680 - policy_loss: 1.8878 - val_loss: 6.3647 - val_value_loss: 0.5789 - val_policy_loss: 1.9380\n",
      "Saved model  connectfour_50_3\n",
      "iteration 3 | evaluation\n",
      "agent vs random - win ratio 0.72 - draw ratio 0.0\n",
      "Number of seen trajectories: 400\n",
      "Number of unique trajectories: 400\n",
      "iteration 4 | self-play\n",
      "saving memory position_memory_connectfour_50_ep_4\n",
      "iteration 4 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 6, 7, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 7)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 2s 453us/step - loss: 6.5580 - value_loss: 0.9202 - policy_loss: 1.9834 - val_loss: 6.6131 - val_value_loss: 1.0325 - val_policy_loss: 1.9817\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 2s 450us/step - loss: 6.4508 - value_loss: 0.7221 - policy_loss: 1.9679 - val_loss: 6.5190 - val_value_loss: 0.8483 - val_policy_loss: 1.9784\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 2s 450us/step - loss: 6.4451 - value_loss: 0.7219 - policy_loss: 1.9573 - val_loss: 6.5820 - val_value_loss: 0.9778 - val_policy_loss: 1.9755\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.4384 - value_loss: 0.7203 - policy_loss: 1.9461 - val_loss: 6.4616 - val_value_loss: 0.7406 - val_policy_loss: 1.9727\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 2s 450us/step - loss: 6.3221 - value_loss: 0.4973 - policy_loss: 1.9371 - val_loss: 6.4552 - val_value_loss: 0.7302 - val_policy_loss: 1.9708\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.3117 - value_loss: 0.4849 - policy_loss: 1.9296 - val_loss: 6.4347 - val_value_loss: 0.6927 - val_policy_loss: 1.9680\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.2909 - value_loss: 0.4521 - policy_loss: 1.9213 - val_loss: 6.4467 - val_value_loss: 0.7206 - val_policy_loss: 1.9648\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.2690 - value_loss: 0.4152 - policy_loss: 1.9151 - val_loss: 6.4067 - val_value_loss: 0.6440 - val_policy_loss: 1.9620\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.2534 - value_loss: 0.3913 - policy_loss: 1.9084 - val_loss: 6.4095 - val_value_loss: 0.6516 - val_policy_loss: 1.9606\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.2966 - value_loss: 0.4839 - policy_loss: 1.9028 - val_loss: 6.4327 - val_value_loss: 0.6994 - val_policy_loss: 1.9598\n",
      "Saved model  connectfour_50_4\n",
      "iteration 4 | evaluation\n",
      "agent vs random - win ratio 0.7 - draw ratio 0.0\n",
      "Number of seen trajectories: 500\n",
      "Number of unique trajectories: 500\n",
      "iteration 5 | self-play\n",
      "saving memory position_memory_connectfour_50_ep_5\n",
      "iteration 5 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 6, 7, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 7)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 2s 455us/step - loss: 6.4999 - value_loss: 0.8122 - policy_loss: 1.9817 - val_loss: 6.4961 - val_value_loss: 0.8206 - val_policy_loss: 1.9659\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 2s 449us/step - loss: 6.4455 - value_loss: 0.7126 - policy_loss: 1.9728 - val_loss: 6.4779 - val_value_loss: 0.7862 - val_policy_loss: 1.9641\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.4093 - value_loss: 0.6464 - policy_loss: 1.9670 - val_loss: 6.4679 - val_value_loss: 0.7675 - val_policy_loss: 1.9632\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.3818 - value_loss: 0.5978 - policy_loss: 1.9607 - val_loss: 6.4567 - val_value_loss: 0.7474 - val_policy_loss: 1.9611\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.3602 - value_loss: 0.5598 - policy_loss: 1.9558 - val_loss: 6.4458 - val_value_loss: 0.7273 - val_policy_loss: 1.9599\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.3410 - value_loss: 0.5275 - policy_loss: 1.9502 - val_loss: 6.4397 - val_value_loss: 0.7177 - val_policy_loss: 1.9577\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.3259 - value_loss: 0.5026 - policy_loss: 1.9453 - val_loss: 6.4338 - val_value_loss: 0.7076 - val_policy_loss: 1.9562\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.3104 - value_loss: 0.4769 - policy_loss: 1.9402 - val_loss: 6.4337 - val_value_loss: 0.7093 - val_policy_loss: 1.9545\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.2985 - value_loss: 0.4577 - policy_loss: 1.9359 - val_loss: 6.4249 - val_value_loss: 0.6946 - val_policy_loss: 1.9521\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 2s 453us/step - loss: 6.2896 - value_loss: 0.4448 - policy_loss: 1.9314 - val_loss: 6.4338 - val_value_loss: 0.7138 - val_policy_loss: 1.9509\n",
      "Saved model  connectfour_50_5\n",
      "iteration 5 | evaluation\n",
      "agent vs random - win ratio 0.74 - draw ratio 0.0\n",
      "Number of seen trajectories: 600\n",
      "Number of unique trajectories: 600\n",
      "iteration 6 | self-play\n",
      "saving memory position_memory_connectfour_50_ep_6\n",
      "iteration 6 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 6, 7, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 7)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 2s 455us/step - loss: 6.5204 - value_loss: 0.8529 - policy_loss: 1.9852 - val_loss: 6.4881 - val_value_loss: 0.8032 - val_policy_loss: 1.9705\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 2s 450us/step - loss: 6.4665 - value_loss: 0.7537 - policy_loss: 1.9769 - val_loss: 6.4787 - val_value_loss: 0.7864 - val_policy_loss: 1.9687\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.4239 - value_loss: 0.6760 - policy_loss: 1.9697 - val_loss: 6.4766 - val_value_loss: 0.7849 - val_policy_loss: 1.9665\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.3952 - value_loss: 0.6249 - policy_loss: 1.9638 - val_loss: 6.4640 - val_value_loss: 0.7614 - val_policy_loss: 1.9651\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.3711 - value_loss: 0.5822 - policy_loss: 1.9586 - val_loss: 6.4573 - val_value_loss: 0.7499 - val_policy_loss: 1.9634\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.3479 - value_loss: 0.5413 - policy_loss: 1.9535 - val_loss: 6.4492 - val_value_loss: 0.7364 - val_policy_loss: 1.9612\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.3324 - value_loss: 0.5151 - policy_loss: 1.9490 - val_loss: 6.4485 - val_value_loss: 0.7364 - val_policy_loss: 1.9601\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 2s 450us/step - loss: 6.3148 - value_loss: 0.4855 - policy_loss: 1.9438 - val_loss: 6.4487 - val_value_loss: 0.7386 - val_policy_loss: 1.9585\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.3051 - value_loss: 0.4707 - policy_loss: 1.9394 - val_loss: 6.4351 - val_value_loss: 0.7131 - val_policy_loss: 1.9571\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.2916 - value_loss: 0.4478 - policy_loss: 1.9355 - val_loss: 6.4388 - val_value_loss: 0.7225 - val_policy_loss: 1.9555\n",
      "Saved model  connectfour_50_6\n",
      "iteration 6 | evaluation\n",
      "agent vs random - win ratio 0.64 - draw ratio 0.0\n",
      "Number of seen trajectories: 700\n",
      "Number of unique trajectories: 700\n",
      "iteration 7 | self-play\n",
      "saving memory position_memory_connectfour_50_ep_7\n",
      "iteration 7 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 6, 7, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 7)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.4984 - value_loss: 0.8210 - policy_loss: 1.9764 - val_loss: 6.5242 - val_value_loss: 0.8576 - val_policy_loss: 1.9916\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 2s 450us/step - loss: 6.4390 - value_loss: 0.7111 - policy_loss: 1.9677 - val_loss: 6.5077 - val_value_loss: 0.8261 - val_policy_loss: 1.9903\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.4018 - value_loss: 0.6436 - policy_loss: 1.9612 - val_loss: 6.5009 - val_value_loss: 0.8143 - val_policy_loss: 1.9890\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.3748 - value_loss: 0.5963 - policy_loss: 1.9547 - val_loss: 6.4959 - val_value_loss: 0.8081 - val_policy_loss: 1.9855\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.3524 - value_loss: 0.5574 - policy_loss: 1.9493 - val_loss: 6.4937 - val_value_loss: 0.8048 - val_policy_loss: 1.9846\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.3346 - value_loss: 0.5267 - policy_loss: 1.9446 - val_loss: 6.4858 - val_value_loss: 0.7903 - val_policy_loss: 1.9837\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.3199 - value_loss: 0.5026 - policy_loss: 1.9397 - val_loss: 6.4668 - val_value_loss: 0.7559 - val_policy_loss: 1.9804\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.3041 - value_loss: 0.4762 - policy_loss: 1.9348 - val_loss: 6.4717 - val_value_loss: 0.7681 - val_policy_loss: 1.9783\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.2979 - value_loss: 0.4688 - policy_loss: 1.9301 - val_loss: 6.4603 - val_value_loss: 0.7463 - val_policy_loss: 1.9776\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.2878 - value_loss: 0.4522 - policy_loss: 1.9269 - val_loss: 6.4558 - val_value_loss: 0.7396 - val_policy_loss: 1.9757\n",
      "Saved model  connectfour_50_7\n",
      "iteration 7 | evaluation\n",
      "agent vs random - win ratio 0.71 - draw ratio 0.0\n",
      "Number of seen trajectories: 800\n",
      "Number of unique trajectories: 800\n",
      "iteration 8 | self-play\n",
      "saving memory position_memory_connectfour_50_ep_8\n",
      "iteration 8 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 6, 7, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 7)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 2s 455us/step - loss: 6.4920 - value_loss: 0.8096 - policy_loss: 1.9783 - val_loss: 6.4531 - val_value_loss: 0.7370 - val_policy_loss: 1.9732\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 2s 449us/step - loss: 6.4414 - value_loss: 0.7171 - policy_loss: 1.9698 - val_loss: 6.4385 - val_value_loss: 0.7103 - val_policy_loss: 1.9710\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.4002 - value_loss: 0.6414 - policy_loss: 1.9635 - val_loss: 6.4260 - val_value_loss: 0.6880 - val_policy_loss: 1.9686\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 2s 450us/step - loss: 6.3752 - value_loss: 0.5967 - policy_loss: 1.9585 - val_loss: 6.4187 - val_value_loss: 0.6740 - val_policy_loss: 1.9683\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.3472 - value_loss: 0.5479 - policy_loss: 1.9517 - val_loss: 6.4179 - val_value_loss: 0.6766 - val_policy_loss: 1.9645\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.3321 - value_loss: 0.5221 - policy_loss: 1.9474 - val_loss: 6.4025 - val_value_loss: 0.6477 - val_policy_loss: 1.9628\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.3188 - value_loss: 0.5015 - policy_loss: 1.9417 - val_loss: 6.4029 - val_value_loss: 0.6511 - val_policy_loss: 1.9606\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.3024 - value_loss: 0.4732 - policy_loss: 1.9376 - val_loss: 6.3928 - val_value_loss: 0.6320 - val_policy_loss: 1.9598\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.3005 - value_loss: 0.4734 - policy_loss: 1.9339 - val_loss: 6.3872 - val_value_loss: 0.6221 - val_policy_loss: 1.9588\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.2799 - value_loss: 0.4368 - policy_loss: 1.9298 - val_loss: 6.3839 - val_value_loss: 0.6187 - val_policy_loss: 1.9560\n",
      "Saved model  connectfour_50_8\n",
      "iteration 8 | evaluation\n",
      "agent vs random - win ratio 0.71 - draw ratio 0.0\n",
      "Number of seen trajectories: 900\n",
      "Number of unique trajectories: 900\n",
      "iteration 9 | self-play\n",
      "saving memory position_memory_connectfour_50_ep_9\n",
      "iteration 9 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 6, 7, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 7)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.4749 - value_loss: 0.7839 - policy_loss: 1.9729 - val_loss: 6.4621 - val_value_loss: 0.7655 - val_policy_loss: 1.9659\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 2s 450us/step - loss: 6.4242 - value_loss: 0.6909 - policy_loss: 1.9649 - val_loss: 6.4408 - val_value_loss: 0.7259 - val_policy_loss: 1.9632\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.3812 - value_loss: 0.6118 - policy_loss: 1.9583 - val_loss: 6.4323 - val_value_loss: 0.7103 - val_policy_loss: 1.9622\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.3533 - value_loss: 0.5625 - policy_loss: 1.9520 - val_loss: 6.4252 - val_value_loss: 0.6978 - val_policy_loss: 1.9608\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.3326 - value_loss: 0.5269 - policy_loss: 1.9467 - val_loss: 6.4185 - val_value_loss: 0.6858 - val_policy_loss: 1.9596\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.3360 - value_loss: 0.5392 - policy_loss: 1.9415 - val_loss: 6.4350 - val_value_loss: 0.7209 - val_policy_loss: 1.9579\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.3201 - value_loss: 0.5120 - policy_loss: 1.9372 - val_loss: 6.4134 - val_value_loss: 0.6788 - val_policy_loss: 1.9570\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.2977 - value_loss: 0.4734 - policy_loss: 1.9314 - val_loss: 6.4211 - val_value_loss: 0.6962 - val_policy_loss: 1.9554\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.2804 - value_loss: 0.4424 - policy_loss: 1.9279 - val_loss: 6.4021 - val_value_loss: 0.6603 - val_policy_loss: 1.9537\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.2636 - value_loss: 0.4138 - policy_loss: 1.9234 - val_loss: 6.3945 - val_value_loss: 0.6466 - val_policy_loss: 1.9525\n",
      "Saved model  connectfour_50_9\n",
      "iteration 9 | evaluation\n",
      "agent vs random - win ratio 0.77 - draw ratio 0.0\n",
      "Number of seen trajectories: 1000\n",
      "Number of unique trajectories: 1000\n",
      "iteration 10 | self-play\n",
      "saving memory position_memory_connectfour_50_ep_10\n",
      "iteration 10 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 6, 7, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 7)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 2s 454us/step - loss: 6.4803 - value_loss: 0.7998 - policy_loss: 1.9710 - val_loss: 6.4644 - val_value_loss: 0.7719 - val_policy_loss: 1.9671\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 2s 450us/step - loss: 6.4511 - value_loss: 0.7452 - policy_loss: 1.9674 - val_loss: 6.4503 - val_value_loss: 0.7447 - val_policy_loss: 1.9664\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 2s 450us/step - loss: 6.4264 - value_loss: 0.6995 - policy_loss: 1.9637 - val_loss: 6.4427 - val_value_loss: 0.7305 - val_policy_loss: 1.9654\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.4082 - value_loss: 0.6656 - policy_loss: 1.9614 - val_loss: 6.4361 - val_value_loss: 0.7189 - val_policy_loss: 1.9640\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.3895 - value_loss: 0.6319 - policy_loss: 1.9579 - val_loss: 6.4296 - val_value_loss: 0.7067 - val_policy_loss: 1.9635\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.3736 - value_loss: 0.6031 - policy_loss: 1.9551 - val_loss: 6.4255 - val_value_loss: 0.6994 - val_policy_loss: 1.9627\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.3597 - value_loss: 0.5775 - policy_loss: 1.9529 - val_loss: 6.4209 - val_value_loss: 0.6910 - val_policy_loss: 1.9620\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.3471 - value_loss: 0.5554 - policy_loss: 1.9500 - val_loss: 6.4179 - val_value_loss: 0.6861 - val_policy_loss: 1.9611\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.3393 - value_loss: 0.5417 - policy_loss: 1.9483 - val_loss: 6.4152 - val_value_loss: 0.6809 - val_policy_loss: 1.9610\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.3290 - value_loss: 0.5243 - policy_loss: 1.9453 - val_loss: 6.4093 - val_value_loss: 0.6708 - val_policy_loss: 1.9595\n",
      "Saved model  connectfour_50_10\n",
      "iteration 10 | evaluation\n",
      "agent vs random - win ratio 0.71 - draw ratio 0.0\n",
      "Number of seen trajectories: 1100\n",
      "Number of unique trajectories: 1100\n",
      "iteration 11 | self-play\n",
      "saving memory position_memory_connectfour_50_ep_11\n",
      "iteration 11 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 6, 7, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 7)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 2s 455us/step - loss: 6.4992 - value_loss: 0.8363 - policy_loss: 1.9739 - val_loss: 6.4654 - val_value_loss: 0.7672 - val_policy_loss: 1.9754\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 2s 449us/step - loss: 6.4656 - value_loss: 0.7735 - policy_loss: 1.9696 - val_loss: 6.4554 - val_value_loss: 0.7484 - val_policy_loss: 1.9744\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.4364 - value_loss: 0.7198 - policy_loss: 1.9651 - val_loss: 6.4518 - val_value_loss: 0.7428 - val_policy_loss: 1.9730\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.4140 - value_loss: 0.6786 - policy_loss: 1.9617 - val_loss: 6.4450 - val_value_loss: 0.7304 - val_policy_loss: 1.9720\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 2s 450us/step - loss: 6.3960 - value_loss: 0.6461 - policy_loss: 1.9584 - val_loss: 6.4383 - val_value_loss: 0.7180 - val_policy_loss: 1.9711\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.3804 - value_loss: 0.6187 - policy_loss: 1.9548 - val_loss: 6.4373 - val_value_loss: 0.7178 - val_policy_loss: 1.9696\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.3660 - value_loss: 0.5929 - policy_loss: 1.9518 - val_loss: 6.4288 - val_value_loss: 0.7018 - val_policy_loss: 1.9687\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 2s 450us/step - loss: 6.3525 - value_loss: 0.5694 - policy_loss: 1.9486 - val_loss: 6.4264 - val_value_loss: 0.6984 - val_policy_loss: 1.9674\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.3413 - value_loss: 0.5495 - policy_loss: 1.9462 - val_loss: 6.4268 - val_value_loss: 0.6997 - val_policy_loss: 1.9671\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.3335 - value_loss: 0.5375 - policy_loss: 1.9428 - val_loss: 6.4217 - val_value_loss: 0.6907 - val_policy_loss: 1.9661\n",
      "Saved model  connectfour_50_11\n",
      "iteration 11 | evaluation\n",
      "agent vs random - win ratio 0.79 - draw ratio 0.0\n",
      "Number of seen trajectories: 1200\n",
      "Number of unique trajectories: 1200\n",
      "iteration 12 | self-play\n",
      "saving memory position_memory_connectfour_50_ep_12\n",
      "iteration 12 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 6, 7, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 7)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 2s 453us/step - loss: 6.4984 - value_loss: 0.8373 - policy_loss: 1.9728 - val_loss: 6.4873 - val_value_loss: 0.8135 - val_policy_loss: 1.9745\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 2s 450us/step - loss: 6.4635 - value_loss: 0.7726 - policy_loss: 1.9680 - val_loss: 6.4758 - val_value_loss: 0.7911 - val_policy_loss: 1.9741\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 2s 453us/step - loss: 6.4371 - value_loss: 0.7236 - policy_loss: 1.9644 - val_loss: 6.4659 - val_value_loss: 0.7717 - val_policy_loss: 1.9739\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 2s 456us/step - loss: 6.4143 - value_loss: 0.6817 - policy_loss: 1.9607 - val_loss: 6.4600 - val_value_loss: 0.7614 - val_policy_loss: 1.9726\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 2s 453us/step - loss: 6.3934 - value_loss: 0.6440 - policy_loss: 1.9569 - val_loss: 6.4548 - val_value_loss: 0.7514 - val_policy_loss: 1.9723\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.3793 - value_loss: 0.6189 - policy_loss: 1.9540 - val_loss: 6.4435 - val_value_loss: 0.7298 - val_policy_loss: 1.9716\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 2s 455us/step - loss: 6.3667 - value_loss: 0.5961 - policy_loss: 1.9516 - val_loss: 6.4393 - val_value_loss: 0.7222 - val_policy_loss: 1.9710\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 2s 454us/step - loss: 6.3503 - value_loss: 0.5668 - policy_loss: 1.9484 - val_loss: 6.4402 - val_value_loss: 0.7248 - val_policy_loss: 1.9702\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.3403 - value_loss: 0.5496 - policy_loss: 1.9457 - val_loss: 6.4304 - val_value_loss: 0.7062 - val_policy_loss: 1.9693\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 2s 453us/step - loss: 6.3306 - value_loss: 0.5326 - policy_loss: 1.9435 - val_loss: 6.4225 - val_value_loss: 0.6916 - val_policy_loss: 1.9685\n",
      "Saved model  connectfour_50_12\n",
      "iteration 12 | evaluation\n",
      "agent vs random - win ratio 0.64 - draw ratio 0.0\n",
      "Number of seen trajectories: 1300\n",
      "Number of unique trajectories: 1300\n",
      "iteration 13 | self-play\n",
      "saving memory position_memory_connectfour_50_ep_13\n",
      "iteration 13 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 6, 7, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 7)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 2s 453us/step - loss: 6.4900 - value_loss: 0.8179 - policy_loss: 1.9771 - val_loss: 6.5114 - val_value_loss: 0.8641 - val_policy_loss: 1.9739\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 2s 449us/step - loss: 6.4557 - value_loss: 0.7538 - policy_loss: 1.9729 - val_loss: 6.4993 - val_value_loss: 0.8406 - val_policy_loss: 1.9732\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.4317 - value_loss: 0.7099 - policy_loss: 1.9689 - val_loss: 6.4934 - val_value_loss: 0.8295 - val_policy_loss: 1.9729\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.4081 - value_loss: 0.6669 - policy_loss: 1.9648 - val_loss: 6.4815 - val_value_loss: 0.8063 - val_policy_loss: 1.9723\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.3880 - value_loss: 0.6305 - policy_loss: 1.9612 - val_loss: 6.4786 - val_value_loss: 0.8015 - val_policy_loss: 1.9714\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.3742 - value_loss: 0.6058 - policy_loss: 1.9584 - val_loss: 6.4684 - val_value_loss: 0.7823 - val_policy_loss: 1.9704\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.3581 - value_loss: 0.5773 - policy_loss: 1.9548 - val_loss: 6.4637 - val_value_loss: 0.7740 - val_policy_loss: 1.9695\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.3448 - value_loss: 0.5539 - policy_loss: 1.9518 - val_loss: 6.4560 - val_value_loss: 0.7600 - val_policy_loss: 1.9682\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.3347 - value_loss: 0.5364 - policy_loss: 1.9493 - val_loss: 6.4508 - val_value_loss: 0.7504 - val_policy_loss: 1.9676\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.3242 - value_loss: 0.5186 - policy_loss: 1.9463 - val_loss: 6.4450 - val_value_loss: 0.7404 - val_policy_loss: 1.9662\n",
      "Saved model  connectfour_50_13\n",
      "iteration 13 | evaluation\n",
      "agent vs random - win ratio 0.65 - draw ratio 0.0\n",
      "Number of seen trajectories: 1400\n",
      "Number of unique trajectories: 1400\n",
      "iteration 14 | self-play\n",
      "saving memory position_memory_connectfour_50_ep_14\n",
      "iteration 14 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 6, 7, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 7)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 2s 454us/step - loss: 6.4804 - value_loss: 0.8086 - policy_loss: 1.9689 - val_loss: 6.4685 - val_value_loss: 0.7766 - val_policy_loss: 1.9771\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 2s 450us/step - loss: 6.4483 - value_loss: 0.7472 - policy_loss: 1.9662 - val_loss: 6.4557 - val_value_loss: 0.7532 - val_policy_loss: 1.9752\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.4204 - value_loss: 0.6957 - policy_loss: 1.9621 - val_loss: 6.4434 - val_value_loss: 0.7300 - val_policy_loss: 1.9739\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.3990 - value_loss: 0.6557 - policy_loss: 1.9594 - val_loss: 6.4343 - val_value_loss: 0.7136 - val_policy_loss: 1.9723\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.3806 - value_loss: 0.6215 - policy_loss: 1.9569 - val_loss: 6.4280 - val_value_loss: 0.7019 - val_policy_loss: 1.9714\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.3624 - value_loss: 0.5881 - policy_loss: 1.9541 - val_loss: 6.4212 - val_value_loss: 0.6899 - val_policy_loss: 1.9700\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.3517 - value_loss: 0.5691 - policy_loss: 1.9519 - val_loss: 6.4154 - val_value_loss: 0.6795 - val_policy_loss: 1.9690\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.3357 - value_loss: 0.5402 - policy_loss: 1.9489 - val_loss: 6.4106 - val_value_loss: 0.6714 - val_policy_loss: 1.9678\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.3269 - value_loss: 0.5246 - policy_loss: 1.9472 - val_loss: 6.4049 - val_value_loss: 0.6614 - val_policy_loss: 1.9665\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.3185 - value_loss: 0.5093 - policy_loss: 1.9457 - val_loss: 6.3997 - val_value_loss: 0.6525 - val_policy_loss: 1.9651\n",
      "Saved model  connectfour_50_14\n",
      "iteration 14 | evaluation\n",
      "agent vs random - win ratio 0.73 - draw ratio 0.0\n",
      "Number of seen trajectories: 1500\n",
      "Number of unique trajectories: 1500\n",
      "iteration 15 | self-play\n",
      "saving memory position_memory_connectfour_50_ep_15\n",
      "iteration 15 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 6, 7, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 7)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 2s 454us/step - loss: 6.4870 - value_loss: 0.8243 - policy_loss: 1.9681 - val_loss: 6.4630 - val_value_loss: 0.7709 - val_policy_loss: 1.9734\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 2s 449us/step - loss: 6.4708 - value_loss: 0.7923 - policy_loss: 1.9676 - val_loss: 6.4589 - val_value_loss: 0.7637 - val_policy_loss: 1.9726\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.4520 - value_loss: 0.7577 - policy_loss: 1.9647 - val_loss: 6.4545 - val_value_loss: 0.7556 - val_policy_loss: 1.9718\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.4358 - value_loss: 0.7271 - policy_loss: 1.9630 - val_loss: 6.4524 - val_value_loss: 0.7522 - val_policy_loss: 1.9710\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.4237 - value_loss: 0.7052 - policy_loss: 1.9608 - val_loss: 6.4485 - val_value_loss: 0.7452 - val_policy_loss: 1.9704\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.4107 - value_loss: 0.6806 - policy_loss: 1.9593 - val_loss: 6.4462 - val_value_loss: 0.7413 - val_policy_loss: 1.9698\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.4009 - value_loss: 0.6632 - policy_loss: 1.9573 - val_loss: 6.4450 - val_value_loss: 0.7397 - val_policy_loss: 1.9691\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.3905 - value_loss: 0.6437 - policy_loss: 1.9561 - val_loss: 6.4425 - val_value_loss: 0.7353 - val_policy_loss: 1.9685\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.3813 - value_loss: 0.6272 - policy_loss: 1.9543 - val_loss: 6.4396 - val_value_loss: 0.7302 - val_policy_loss: 1.9680\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.3717 - value_loss: 0.6092 - policy_loss: 1.9532 - val_loss: 6.4370 - val_value_loss: 0.7255 - val_policy_loss: 1.9675\n",
      "Saved model  connectfour_50_15\n",
      "iteration 15 | evaluation\n",
      "agent vs random - win ratio 0.7 - draw ratio 0.0\n",
      "Number of seen trajectories: 1600\n",
      "Number of unique trajectories: 1600\n",
      "iteration 16 | self-play\n",
      "saving memory position_memory_connectfour_50_ep_16\n",
      "iteration 16 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 6, 7, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 7)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 2s 453us/step - loss: 6.5060 - value_loss: 0.8571 - policy_loss: 1.9741 - val_loss: 6.4937 - val_value_loss: 0.8314 - val_policy_loss: 1.9750\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 2s 450us/step - loss: 6.4877 - value_loss: 0.8220 - policy_loss: 1.9726 - val_loss: 6.4881 - val_value_loss: 0.8210 - val_policy_loss: 1.9744\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.4707 - value_loss: 0.7899 - policy_loss: 1.9708 - val_loss: 6.4845 - val_value_loss: 0.8145 - val_policy_loss: 1.9739\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.4559 - value_loss: 0.7627 - policy_loss: 1.9684 - val_loss: 6.4799 - val_value_loss: 0.8058 - val_policy_loss: 1.9734\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.4417 - value_loss: 0.7357 - policy_loss: 1.9670 - val_loss: 6.4760 - val_value_loss: 0.7987 - val_policy_loss: 1.9728\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.4280 - value_loss: 0.7106 - policy_loss: 1.9648 - val_loss: 6.4725 - val_value_loss: 0.7921 - val_policy_loss: 1.9723\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.4176 - value_loss: 0.6914 - policy_loss: 1.9633 - val_loss: 6.4694 - val_value_loss: 0.7867 - val_policy_loss: 1.9718\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.4094 - value_loss: 0.6763 - policy_loss: 1.9622 - val_loss: 6.4660 - val_value_loss: 0.7804 - val_policy_loss: 1.9712\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.3987 - value_loss: 0.6569 - policy_loss: 1.9603 - val_loss: 6.4640 - val_value_loss: 0.7769 - val_policy_loss: 1.9708\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 2s 453us/step - loss: 6.3888 - value_loss: 0.6382 - policy_loss: 1.9592 - val_loss: 6.4610 - val_value_loss: 0.7715 - val_policy_loss: 1.9702\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model  connectfour_50_16\n",
      "iteration 16 | evaluation\n",
      "agent vs random - win ratio 0.73 - draw ratio 0.0\n",
      "Number of seen trajectories: 1700\n",
      "Number of unique trajectories: 1700\n",
      "iteration 17 | self-play\n",
      "saving memory position_memory_connectfour_50_ep_17\n",
      "iteration 17 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 6, 7, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 7)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 2s 455us/step - loss: 6.5234 - value_loss: 0.8850 - policy_loss: 1.9818 - val_loss: 6.5050 - val_value_loss: 0.8529 - val_policy_loss: 1.9769\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.5046 - value_loss: 0.8489 - policy_loss: 1.9803 - val_loss: 6.5003 - val_value_loss: 0.8443 - val_policy_loss: 1.9762\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.4851 - value_loss: 0.8126 - policy_loss: 1.9777 - val_loss: 6.4930 - val_value_loss: 0.8307 - val_policy_loss: 1.9754\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 2s 453us/step - loss: 6.4700 - value_loss: 0.7838 - policy_loss: 1.9763 - val_loss: 6.4870 - val_value_loss: 0.8196 - val_policy_loss: 1.9746\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.4561 - value_loss: 0.7577 - policy_loss: 1.9747 - val_loss: 6.4812 - val_value_loss: 0.8084 - val_policy_loss: 1.9741\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.4430 - value_loss: 0.7333 - policy_loss: 1.9730 - val_loss: 6.4808 - val_value_loss: 0.8085 - val_policy_loss: 1.9734\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.4307 - value_loss: 0.7105 - policy_loss: 1.9713 - val_loss: 6.4740 - val_value_loss: 0.7957 - val_policy_loss: 1.9727\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.4213 - value_loss: 0.6926 - policy_loss: 1.9704 - val_loss: 6.4698 - val_value_loss: 0.7881 - val_policy_loss: 1.9720\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 2s 453us/step - loss: 6.4102 - value_loss: 0.6727 - policy_loss: 1.9682 - val_loss: 6.4664 - val_value_loss: 0.7820 - val_policy_loss: 1.9714\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 2s 453us/step - loss: 6.4004 - value_loss: 0.6547 - policy_loss: 1.9667 - val_loss: 6.4606 - val_value_loss: 0.7712 - val_policy_loss: 1.9706\n",
      "Saved model  connectfour_50_17\n",
      "iteration 17 | evaluation\n",
      "agent vs random - win ratio 0.7 - draw ratio 0.0\n",
      "Number of seen trajectories: 1800\n",
      "Number of unique trajectories: 1800\n",
      "iteration 18 | self-play\n",
      "saving memory position_memory_connectfour_50_ep_18\n",
      "iteration 18 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 6, 7, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 7)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5207 - value_loss: 0.8884 - policy_loss: 1.9737 - val_loss: 6.5035 - val_value_loss: 0.8572 - val_policy_loss: 1.9705\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 2s 449us/step - loss: 6.5014 - value_loss: 0.8521 - policy_loss: 1.9715 - val_loss: 6.4975 - val_value_loss: 0.8460 - val_policy_loss: 1.9698\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.4837 - value_loss: 0.8191 - policy_loss: 1.9691 - val_loss: 6.4896 - val_value_loss: 0.8308 - val_policy_loss: 1.9692\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 2s 453us/step - loss: 6.4708 - value_loss: 0.7953 - policy_loss: 1.9672 - val_loss: 6.4856 - val_value_loss: 0.8234 - val_policy_loss: 1.9688\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.4549 - value_loss: 0.7652 - policy_loss: 1.9657 - val_loss: 6.4813 - val_value_loss: 0.8154 - val_policy_loss: 1.9682\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 2s 453us/step - loss: 6.4408 - value_loss: 0.7389 - policy_loss: 1.9637 - val_loss: 6.4774 - val_value_loss: 0.8081 - val_policy_loss: 1.9677\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.4293 - value_loss: 0.7178 - policy_loss: 1.9620 - val_loss: 6.4735 - val_value_loss: 0.8010 - val_policy_loss: 1.9672\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 2s 453us/step - loss: 6.4186 - value_loss: 0.6982 - policy_loss: 1.9603 - val_loss: 6.4693 - val_value_loss: 0.7933 - val_policy_loss: 1.9666\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 2s 453us/step - loss: 6.4060 - value_loss: 0.6747 - policy_loss: 1.9585 - val_loss: 6.4662 - val_value_loss: 0.7878 - val_policy_loss: 1.9660\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 2s 453us/step - loss: 6.3974 - value_loss: 0.6593 - policy_loss: 1.9569 - val_loss: 6.4626 - val_value_loss: 0.7809 - val_policy_loss: 1.9657\n",
      "Saved model  connectfour_50_18\n",
      "iteration 18 | evaluation\n",
      "agent vs random - win ratio 0.64 - draw ratio 0.0\n",
      "Number of seen trajectories: 1900\n",
      "Number of unique trajectories: 1900\n",
      "iteration 19 | self-play\n",
      "saving memory position_memory_connectfour_50_ep_19\n",
      "iteration 19 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 6, 7, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 7)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 2s 455us/step - loss: 6.5193 - value_loss: 0.8868 - policy_loss: 1.9733 - val_loss: 6.5300 - val_value_loss: 0.9066 - val_policy_loss: 1.9750\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 2s 450us/step - loss: 6.5003 - value_loss: 0.8518 - policy_loss: 1.9705 - val_loss: 6.5220 - val_value_loss: 0.8915 - val_policy_loss: 1.9742\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.4812 - value_loss: 0.8153 - policy_loss: 1.9688 - val_loss: 6.5154 - val_value_loss: 0.8789 - val_policy_loss: 1.9736\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.4675 - value_loss: 0.7896 - policy_loss: 1.9671 - val_loss: 6.5099 - val_value_loss: 0.8687 - val_policy_loss: 1.9728\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 2s 454us/step - loss: 6.4517 - value_loss: 0.7608 - policy_loss: 1.9645 - val_loss: 6.5038 - val_value_loss: 0.8571 - val_policy_loss: 1.9723\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 2s 453us/step - loss: 6.4373 - value_loss: 0.7340 - policy_loss: 1.9624 - val_loss: 6.4988 - val_value_loss: 0.8480 - val_policy_loss: 1.9716\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 2s 453us/step - loss: 6.4289 - value_loss: 0.7186 - policy_loss: 1.9612 - val_loss: 6.4936 - val_value_loss: 0.8382 - val_policy_loss: 1.9710\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 2s 458us/step - loss: 6.4179 - value_loss: 0.6985 - policy_loss: 1.9594 - val_loss: 6.4894 - val_value_loss: 0.8306 - val_policy_loss: 1.9702\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 2s 463us/step - loss: 6.4064 - value_loss: 0.6776 - policy_loss: 1.9574 - val_loss: 6.4843 - val_value_loss: 0.8210 - val_policy_loss: 1.9698\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 2s 462us/step - loss: 6.3966 - value_loss: 0.6600 - policy_loss: 1.9555 - val_loss: 6.4805 - val_value_loss: 0.8142 - val_policy_loss: 1.9691\n",
      "Saved model  connectfour_50_19\n",
      "iteration 19 | evaluation\n",
      "agent vs random - win ratio 0.75 - draw ratio 0.0\n",
      "Number of seen trajectories: 2000\n",
      "Number of unique trajectories: 2000\n",
      "iteration 20 | self-play\n",
      "saving memory position_memory_connectfour_50_ep_20\n",
      "iteration 20 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 6, 7, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 7)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 2s 453us/step - loss: 6.4979 - value_loss: 0.8435 - policy_loss: 1.9747 - val_loss: 6.5046 - val_value_loss: 0.8570 - val_policy_loss: 1.9745\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 2s 450us/step - loss: 6.4858 - value_loss: 0.8209 - policy_loss: 1.9730 - val_loss: 6.5022 - val_value_loss: 0.8527 - val_policy_loss: 1.9741\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.4769 - value_loss: 0.8045 - policy_loss: 1.9716 - val_loss: 6.4985 - val_value_loss: 0.8457 - val_policy_loss: 1.9738\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.4693 - value_loss: 0.7904 - policy_loss: 1.9706 - val_loss: 6.4947 - val_value_loss: 0.8384 - val_policy_loss: 1.9735\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.4625 - value_loss: 0.7768 - policy_loss: 1.9707 - val_loss: 6.4919 - val_value_loss: 0.8331 - val_policy_loss: 1.9732\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.4541 - value_loss: 0.7615 - policy_loss: 1.9692 - val_loss: 6.4881 - val_value_loss: 0.8257 - val_policy_loss: 1.9730\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 2s 453us/step - loss: 6.4459 - value_loss: 0.7466 - policy_loss: 1.9678 - val_loss: 6.4851 - val_value_loss: 0.8201 - val_policy_loss: 1.9727\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 2s 453us/step - loss: 6.4379 - value_loss: 0.7312 - policy_loss: 1.9671 - val_loss: 6.4822 - val_value_loss: 0.8145 - val_policy_loss: 1.9725\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.4332 - value_loss: 0.7227 - policy_loss: 1.9664 - val_loss: 6.4794 - val_value_loss: 0.8090 - val_policy_loss: 1.9723\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.4281 - value_loss: 0.7130 - policy_loss: 1.9658 - val_loss: 6.4770 - val_value_loss: 0.8047 - val_policy_loss: 1.9721\n",
      "Saved model  connectfour_50_20\n",
      "iteration 20 | evaluation\n",
      "agent vs random - win ratio 0.69 - draw ratio 0.0\n",
      "Number of seen trajectories: 2100\n",
      "Number of unique trajectories: 2100\n",
      "iteration 21 | self-play\n",
      "saving memory position_memory_connectfour_50_ep_21\n",
      "iteration 21 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 6, 7, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 7)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5620 - value_loss: 0.9706 - policy_loss: 1.9760 - val_loss: 6.5482 - val_value_loss: 0.9486 - val_policy_loss: 1.9706\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 2s 450us/step - loss: 6.5493 - value_loss: 0.9465 - policy_loss: 1.9748 - val_loss: 6.5422 - val_value_loss: 0.9371 - val_policy_loss: 1.9702\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.5380 - value_loss: 0.9252 - policy_loss: 1.9735 - val_loss: 6.5379 - val_value_loss: 0.9288 - val_policy_loss: 1.9698\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5280 - value_loss: 0.9065 - policy_loss: 1.9725 - val_loss: 6.5335 - val_value_loss: 0.9204 - val_policy_loss: 1.9695\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5196 - value_loss: 0.8905 - policy_loss: 1.9717 - val_loss: 6.5294 - val_value_loss: 0.9124 - val_policy_loss: 1.9692\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5102 - value_loss: 0.8728 - policy_loss: 1.9706 - val_loss: 6.5262 - val_value_loss: 0.9064 - val_policy_loss: 1.9689\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5026 - value_loss: 0.8585 - policy_loss: 1.9697 - val_loss: 6.5221 - val_value_loss: 0.8985 - val_policy_loss: 1.9687\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.4935 - value_loss: 0.8411 - policy_loss: 1.9690 - val_loss: 6.5189 - val_value_loss: 0.8923 - val_policy_loss: 1.9685\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 2s 453us/step - loss: 6.4850 - value_loss: 0.8252 - policy_loss: 1.9678 - val_loss: 6.5167 - val_value_loss: 0.8882 - val_policy_loss: 1.9683\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 2s 454us/step - loss: 6.4796 - value_loss: 0.8153 - policy_loss: 1.9670 - val_loss: 6.5139 - val_value_loss: 0.8829 - val_policy_loss: 1.9680\n",
      "Saved model  connectfour_50_21\n",
      "iteration 21 | evaluation\n",
      "agent vs random - win ratio 0.67 - draw ratio 0.0\n",
      "Number of seen trajectories: 2200\n",
      "Number of unique trajectories: 2200\n",
      "iteration 22 | self-play\n",
      "saving memory position_memory_connectfour_50_ep_22\n",
      "iteration 22 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 6, 7, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 7)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 2s 454us/step - loss: 6.5522 - value_loss: 0.9522 - policy_loss: 1.9754 - val_loss: 6.5510 - val_value_loss: 0.9311 - val_policy_loss: 1.9940\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 2s 450us/step - loss: 6.5408 - value_loss: 0.9302 - policy_loss: 1.9745 - val_loss: 6.5450 - val_value_loss: 0.9198 - val_policy_loss: 1.9934\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.5314 - value_loss: 0.9114 - policy_loss: 1.9746 - val_loss: 6.5403 - val_value_loss: 0.9109 - val_policy_loss: 1.9928\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5196 - value_loss: 0.8901 - policy_loss: 1.9724 - val_loss: 6.5361 - val_value_loss: 0.9032 - val_policy_loss: 1.9923\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5106 - value_loss: 0.8720 - policy_loss: 1.9725 - val_loss: 6.5319 - val_value_loss: 0.8954 - val_policy_loss: 1.9918\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5015 - value_loss: 0.8552 - policy_loss: 1.9712 - val_loss: 6.5290 - val_value_loss: 0.8901 - val_policy_loss: 1.9914\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 2s 453us/step - loss: 6.4931 - value_loss: 0.8395 - policy_loss: 1.9700 - val_loss: 6.5249 - val_value_loss: 0.8823 - val_policy_loss: 1.9910\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 2s 453us/step - loss: 6.4852 - value_loss: 0.8246 - policy_loss: 1.9693 - val_loss: 6.5215 - val_value_loss: 0.8758 - val_policy_loss: 1.9906\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 2s 453us/step - loss: 6.4767 - value_loss: 0.8084 - policy_loss: 1.9685 - val_loss: 6.5198 - val_value_loss: 0.8728 - val_policy_loss: 1.9903\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.4692 - value_loss: 0.7948 - policy_loss: 1.9671 - val_loss: 6.5161 - val_value_loss: 0.8658 - val_policy_loss: 1.9899\n",
      "Saved model  connectfour_50_22\n",
      "iteration 22 | evaluation\n",
      "agent vs random - win ratio 0.73 - draw ratio 0.0\n",
      "Number of seen trajectories: 2300\n",
      "Number of unique trajectories: 2299\n",
      "iteration 23 | self-play\n",
      "saving memory position_memory_connectfour_50_ep_23\n",
      "iteration 23 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 6, 7, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 7)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.5605 - value_loss: 0.9584 - policy_loss: 1.9862 - val_loss: 6.5506 - val_value_loss: 0.9418 - val_policy_loss: 1.9829\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 2s 450us/step - loss: 6.5479 - value_loss: 0.9342 - policy_loss: 1.9851 - val_loss: 6.5457 - val_value_loss: 0.9325 - val_policy_loss: 1.9825\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5364 - value_loss: 0.9122 - policy_loss: 1.9842 - val_loss: 6.5412 - val_value_loss: 0.9241 - val_policy_loss: 1.9820\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5258 - value_loss: 0.8928 - policy_loss: 1.9824 - val_loss: 6.5370 - val_value_loss: 0.9163 - val_policy_loss: 1.9814\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5170 - value_loss: 0.8768 - policy_loss: 1.9810 - val_loss: 6.5337 - val_value_loss: 0.9103 - val_policy_loss: 1.9808\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5084 - value_loss: 0.8602 - policy_loss: 1.9803 - val_loss: 6.5299 - val_value_loss: 0.9033 - val_policy_loss: 1.9803\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.4980 - value_loss: 0.8406 - policy_loss: 1.9793 - val_loss: 6.5261 - val_value_loss: 0.8961 - val_policy_loss: 1.9798\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.4901 - value_loss: 0.8256 - policy_loss: 1.9784 - val_loss: 6.5214 - val_value_loss: 0.8874 - val_policy_loss: 1.9793\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 2s 453us/step - loss: 6.4846 - value_loss: 0.8150 - policy_loss: 1.9780 - val_loss: 6.5190 - val_value_loss: 0.8831 - val_policy_loss: 1.9788\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.4772 - value_loss: 0.8008 - policy_loss: 1.9774 - val_loss: 6.5162 - val_value_loss: 0.8780 - val_policy_loss: 1.9784\n",
      "Saved model  connectfour_50_23\n",
      "iteration 23 | evaluation\n",
      "agent vs random - win ratio 0.68 - draw ratio 0.0\n",
      "Number of seen trajectories: 2400\n",
      "Number of unique trajectories: 2399\n",
      "iteration 24 | self-play\n",
      "saving memory position_memory_connectfour_50_ep_24\n",
      "iteration 24 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 6, 7, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 7)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5513 - value_loss: 0.9451 - policy_loss: 1.9816 - val_loss: 6.5374 - val_value_loss: 0.9160 - val_policy_loss: 1.9827\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 2s 449us/step - loss: 6.5398 - value_loss: 0.9230 - policy_loss: 1.9805 - val_loss: 6.5318 - val_value_loss: 0.9055 - val_policy_loss: 1.9822\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.5266 - value_loss: 0.8982 - policy_loss: 1.9789 - val_loss: 6.5269 - val_value_loss: 0.8961 - val_policy_loss: 1.9817\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.5176 - value_loss: 0.8813 - policy_loss: 1.9779 - val_loss: 6.5220 - val_value_loss: 0.8868 - val_policy_loss: 1.9812\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.5082 - value_loss: 0.8636 - policy_loss: 1.9768 - val_loss: 6.5176 - val_value_loss: 0.8784 - val_policy_loss: 1.9808\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.4994 - value_loss: 0.8466 - policy_loss: 1.9764 - val_loss: 6.5134 - val_value_loss: 0.8707 - val_policy_loss: 1.9804\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.4915 - value_loss: 0.8321 - policy_loss: 1.9750 - val_loss: 6.5095 - val_value_loss: 0.8633 - val_policy_loss: 1.9800\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.4806 - value_loss: 0.8117 - policy_loss: 1.9738 - val_loss: 6.5058 - val_value_loss: 0.8563 - val_policy_loss: 1.9796\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.4742 - value_loss: 0.7996 - policy_loss: 1.9730 - val_loss: 6.5025 - val_value_loss: 0.8500 - val_policy_loss: 1.9793\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.4679 - value_loss: 0.7876 - policy_loss: 1.9725 - val_loss: 6.4991 - val_value_loss: 0.8436 - val_policy_loss: 1.9789\n",
      "Saved model  connectfour_50_24\n",
      "iteration 24 | evaluation\n",
      "agent vs random - win ratio 0.79 - draw ratio 0.0\n",
      "Number of seen trajectories: 2500\n",
      "Number of unique trajectories: 2499\n",
      "iteration 25 | self-play\n",
      "saving memory position_memory_connectfour_50_ep_25\n",
      "iteration 25 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 6, 7, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 7)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 2s 455us/step - loss: 6.5490 - value_loss: 0.9403 - policy_loss: 1.9822 - val_loss: 6.5342 - val_value_loss: 0.9134 - val_policy_loss: 1.9793\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 2s 450us/step - loss: 6.5404 - value_loss: 0.9246 - policy_loss: 1.9806 - val_loss: 6.5319 - val_value_loss: 0.9091 - val_policy_loss: 1.9791\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5345 - value_loss: 0.9139 - policy_loss: 1.9795 - val_loss: 6.5294 - val_value_loss: 0.9044 - val_policy_loss: 1.9788\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5286 - value_loss: 0.9025 - policy_loss: 1.9791 - val_loss: 6.5271 - val_value_loss: 0.9000 - val_policy_loss: 1.9786\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5229 - value_loss: 0.8917 - policy_loss: 1.9786 - val_loss: 6.5254 - val_value_loss: 0.8968 - val_policy_loss: 1.9785\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5173 - value_loss: 0.8815 - policy_loss: 1.9776 - val_loss: 6.5233 - val_value_loss: 0.8928 - val_policy_loss: 1.9783\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5132 - value_loss: 0.8736 - policy_loss: 1.9773 - val_loss: 6.5210 - val_value_loss: 0.8884 - val_policy_loss: 1.9781\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 2s 453us/step - loss: 6.5089 - value_loss: 0.8656 - policy_loss: 1.9767 - val_loss: 6.5190 - val_value_loss: 0.8845 - val_policy_loss: 1.9780\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5029 - value_loss: 0.8544 - policy_loss: 1.9759 - val_loss: 6.5176 - val_value_loss: 0.8819 - val_policy_loss: 1.9778\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5012 - value_loss: 0.8504 - policy_loss: 1.9764 - val_loss: 6.5156 - val_value_loss: 0.8780 - val_policy_loss: 1.9777\n",
      "Saved model  connectfour_50_25\n",
      "iteration 25 | evaluation\n",
      "agent vs random - win ratio 0.8 - draw ratio 0.0\n",
      "Number of seen trajectories: 2600\n",
      "Number of unique trajectories: 2598\n",
      "iteration 26 | self-play\n",
      "saving memory position_memory_connectfour_50_ep_26\n",
      "iteration 26 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 6, 7, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 7)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 2s 455us/step - loss: 6.5531 - value_loss: 0.9575 - policy_loss: 1.9734 - val_loss: 6.5520 - val_value_loss: 0.9466 - val_policy_loss: 1.9820\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 2s 450us/step - loss: 6.5472 - value_loss: 0.9457 - policy_loss: 1.9732 - val_loss: 6.5498 - val_value_loss: 0.9422 - val_policy_loss: 1.9819\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5427 - value_loss: 0.9376 - policy_loss: 1.9725 - val_loss: 6.5476 - val_value_loss: 0.9380 - val_policy_loss: 1.9817\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5364 - value_loss: 0.9260 - policy_loss: 1.9715 - val_loss: 6.5454 - val_value_loss: 0.9338 - val_policy_loss: 1.9816\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5333 - value_loss: 0.9195 - policy_loss: 1.9718 - val_loss: 6.5436 - val_value_loss: 0.9303 - val_policy_loss: 1.9815\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5267 - value_loss: 0.9072 - policy_loss: 1.9709 - val_loss: 6.5415 - val_value_loss: 0.9263 - val_policy_loss: 1.9814\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.5231 - value_loss: 0.9002 - policy_loss: 1.9707 - val_loss: 6.5394 - val_value_loss: 0.9222 - val_policy_loss: 1.9813\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.5178 - value_loss: 0.8902 - policy_loss: 1.9701 - val_loss: 6.5378 - val_value_loss: 0.9191 - val_policy_loss: 1.9812\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 2s 453us/step - loss: 6.5127 - value_loss: 0.8810 - policy_loss: 1.9692 - val_loss: 6.5358 - val_value_loss: 0.9152 - val_policy_loss: 1.9811\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5101 - value_loss: 0.8760 - policy_loss: 1.9690 - val_loss: 6.5343 - val_value_loss: 0.9123 - val_policy_loss: 1.9810\n",
      "Saved model  connectfour_50_26\n",
      "iteration 26 | evaluation\n",
      "agent vs random - win ratio 0.75 - draw ratio 0.0\n",
      "Number of seen trajectories: 2700\n",
      "Number of unique trajectories: 2698\n",
      "iteration 27 | self-play\n",
      "saving memory position_memory_connectfour_50_ep_27\n",
      "iteration 27 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 6, 7, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 7)\n",
      "Train on 4096 samples, validate on 1024 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 2s 454us/step - loss: 6.5444 - value_loss: 0.9363 - policy_loss: 1.9772 - val_loss: 6.5602 - val_value_loss: 0.9721 - val_policy_loss: 1.9731\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 2s 449us/step - loss: 6.5391 - value_loss: 0.9260 - policy_loss: 1.9770 - val_loss: 6.5576 - val_value_loss: 0.9670 - val_policy_loss: 1.9730\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.5306 - value_loss: 0.9100 - policy_loss: 1.9759 - val_loss: 6.5549 - val_value_loss: 0.9619 - val_policy_loss: 1.9728\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5247 - value_loss: 0.8990 - policy_loss: 1.9753 - val_loss: 6.5525 - val_value_loss: 0.9572 - val_policy_loss: 1.9727\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5195 - value_loss: 0.8892 - policy_loss: 1.9746 - val_loss: 6.5502 - val_value_loss: 0.9528 - val_policy_loss: 1.9725\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5143 - value_loss: 0.8791 - policy_loss: 1.9743 - val_loss: 6.5479 - val_value_loss: 0.9483 - val_policy_loss: 1.9724\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5103 - value_loss: 0.8715 - policy_loss: 1.9739 - val_loss: 6.5458 - val_value_loss: 0.9442 - val_policy_loss: 1.9722\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5046 - value_loss: 0.8609 - policy_loss: 1.9733 - val_loss: 6.5437 - val_value_loss: 0.9402 - val_policy_loss: 1.9721\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5008 - value_loss: 0.8532 - policy_loss: 1.9733 - val_loss: 6.5417 - val_value_loss: 0.9364 - val_policy_loss: 1.9719\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.4952 - value_loss: 0.8427 - policy_loss: 1.9726 - val_loss: 6.5398 - val_value_loss: 0.9329 - val_policy_loss: 1.9718\n",
      "Saved model  connectfour_50_27\n",
      "iteration 27 | evaluation\n",
      "agent vs random - win ratio 0.79 - draw ratio 0.0\n",
      "Number of seen trajectories: 2800\n",
      "Number of unique trajectories: 2798\n",
      "iteration 28 | self-play\n",
      "saving memory position_memory_connectfour_50_ep_28\n",
      "iteration 28 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 6, 7, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 7)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 2s 454us/step - loss: 6.5526 - value_loss: 0.9531 - policy_loss: 1.9770 - val_loss: 6.5726 - val_value_loss: 0.9815 - val_policy_loss: 1.9886\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 2s 449us/step - loss: 6.5452 - value_loss: 0.9389 - policy_loss: 1.9764 - val_loss: 6.5695 - val_value_loss: 0.9755 - val_policy_loss: 1.9885\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5390 - value_loss: 0.9271 - policy_loss: 1.9760 - val_loss: 6.5659 - val_value_loss: 0.9684 - val_policy_loss: 1.9884\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5335 - value_loss: 0.9162 - policy_loss: 1.9757 - val_loss: 6.5628 - val_value_loss: 0.9623 - val_policy_loss: 1.9883\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5270 - value_loss: 0.9047 - policy_loss: 1.9744 - val_loss: 6.5599 - val_value_loss: 0.9567 - val_policy_loss: 1.9882\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5227 - value_loss: 0.8962 - policy_loss: 1.9742 - val_loss: 6.5564 - val_value_loss: 0.9496 - val_policy_loss: 1.9882\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5172 - value_loss: 0.8860 - policy_loss: 1.9736 - val_loss: 6.5527 - val_value_loss: 0.9424 - val_policy_loss: 1.9881\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5116 - value_loss: 0.8755 - policy_loss: 1.9729 - val_loss: 6.5497 - val_value_loss: 0.9365 - val_policy_loss: 1.9880\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5066 - value_loss: 0.8658 - policy_loss: 1.9726 - val_loss: 6.5476 - val_value_loss: 0.9324 - val_policy_loss: 1.9878\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5028 - value_loss: 0.8582 - policy_loss: 1.9726 - val_loss: 6.5450 - val_value_loss: 0.9275 - val_policy_loss: 1.9877\n",
      "Saved model  connectfour_50_28\n",
      "iteration 28 | evaluation\n",
      "agent vs random - win ratio 0.76 - draw ratio 0.0\n",
      "Number of seen trajectories: 2900\n",
      "Number of unique trajectories: 2898\n",
      "iteration 29 | self-play\n",
      "saving memory position_memory_connectfour_50_ep_29\n",
      "iteration 29 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 6, 7, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 7)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 2s 453us/step - loss: 6.5670 - value_loss: 0.9796 - policy_loss: 1.9795 - val_loss: 6.5615 - val_value_loss: 0.9661 - val_policy_loss: 1.9821\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 2s 450us/step - loss: 6.5601 - value_loss: 0.9665 - policy_loss: 1.9789 - val_loss: 6.5591 - val_value_loss: 0.9616 - val_policy_loss: 1.9818\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5536 - value_loss: 0.9541 - policy_loss: 1.9783 - val_loss: 6.5569 - val_value_loss: 0.9573 - val_policy_loss: 1.9817\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.5510 - value_loss: 0.9488 - policy_loss: 1.9784 - val_loss: 6.5548 - val_value_loss: 0.9533 - val_policy_loss: 1.9815\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5445 - value_loss: 0.9367 - policy_loss: 1.9776 - val_loss: 6.5527 - val_value_loss: 0.9493 - val_policy_loss: 1.9814\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.5376 - value_loss: 0.9237 - policy_loss: 1.9768 - val_loss: 6.5508 - val_value_loss: 0.9456 - val_policy_loss: 1.9813\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5323 - value_loss: 0.9136 - policy_loss: 1.9763 - val_loss: 6.5488 - val_value_loss: 0.9418 - val_policy_loss: 1.9812\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5274 - value_loss: 0.9047 - policy_loss: 1.9755 - val_loss: 6.5471 - val_value_loss: 0.9384 - val_policy_loss: 1.9811\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5226 - value_loss: 0.8952 - policy_loss: 1.9753 - val_loss: 6.5452 - val_value_loss: 0.9347 - val_policy_loss: 1.9810\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5174 - value_loss: 0.8854 - policy_loss: 1.9749 - val_loss: 6.5434 - val_value_loss: 0.9312 - val_policy_loss: 1.9809\n",
      "Saved model  connectfour_50_29\n",
      "iteration 29 | evaluation\n",
      "agent vs random - win ratio 0.71 - draw ratio 0.0\n",
      "Number of seen trajectories: 3000\n",
      "Number of unique trajectories: 2997\n",
      "iteration 30 | self-play\n",
      "saving memory position_memory_connectfour_50_ep_30\n",
      "iteration 30 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 6, 7, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 7)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 2s 454us/step - loss: 6.5547 - value_loss: 0.9600 - policy_loss: 1.9747 - val_loss: 6.5482 - val_value_loss: 0.9457 - val_policy_loss: 1.9761\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 2s 450us/step - loss: 6.5521 - value_loss: 0.9547 - policy_loss: 1.9749 - val_loss: 6.5472 - val_value_loss: 0.9438 - val_policy_loss: 1.9760\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.5469 - value_loss: 0.9453 - policy_loss: 1.9739 - val_loss: 6.5460 - val_value_loss: 0.9416 - val_policy_loss: 1.9759\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5459 - value_loss: 0.9434 - policy_loss: 1.9737 - val_loss: 6.5449 - val_value_loss: 0.9394 - val_policy_loss: 1.9758\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.5428 - value_loss: 0.9372 - policy_loss: 1.9738 - val_loss: 6.5439 - val_value_loss: 0.9376 - val_policy_loss: 1.9756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.5400 - value_loss: 0.9323 - policy_loss: 1.9732 - val_loss: 6.5428 - val_value_loss: 0.9354 - val_policy_loss: 1.9756\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5372 - value_loss: 0.9270 - policy_loss: 1.9729 - val_loss: 6.5415 - val_value_loss: 0.9330 - val_policy_loss: 1.9755\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5350 - value_loss: 0.9227 - policy_loss: 1.9727 - val_loss: 6.5404 - val_value_loss: 0.9309 - val_policy_loss: 1.9754\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5318 - value_loss: 0.9167 - policy_loss: 1.9723 - val_loss: 6.5393 - val_value_loss: 0.9289 - val_policy_loss: 1.9753\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5292 - value_loss: 0.9119 - policy_loss: 1.9719 - val_loss: 6.5384 - val_value_loss: 0.9271 - val_policy_loss: 1.9752\n",
      "Saved model  connectfour_50_30\n",
      "iteration 30 | evaluation\n",
      "agent vs random - win ratio 0.85 - draw ratio 0.0\n",
      "Number of seen trajectories: 3100\n",
      "Number of unique trajectories: 3096\n",
      "iteration 31 | self-play\n",
      "saving memory position_memory_connectfour_50_ep_31\n",
      "iteration 31 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 6, 7, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 7)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 2s 450us/step - loss: 6.5609 - value_loss: 0.9680 - policy_loss: 1.9792 - val_loss: 6.5628 - val_value_loss: 0.9792 - val_policy_loss: 1.9719\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 2s 450us/step - loss: 6.5565 - value_loss: 0.9600 - policy_loss: 1.9785 - val_loss: 6.5614 - val_value_loss: 0.9764 - val_policy_loss: 1.9719\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 2s 450us/step - loss: 6.5533 - value_loss: 0.9537 - policy_loss: 1.9783 - val_loss: 6.5600 - val_value_loss: 0.9737 - val_policy_loss: 1.9718\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.5501 - value_loss: 0.9479 - policy_loss: 1.9778 - val_loss: 6.5585 - val_value_loss: 0.9707 - val_policy_loss: 1.9718\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 2s 450us/step - loss: 6.5472 - value_loss: 0.9423 - policy_loss: 1.9777 - val_loss: 6.5571 - val_value_loss: 0.9680 - val_policy_loss: 1.9718\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.5444 - value_loss: 0.9368 - policy_loss: 1.9776 - val_loss: 6.5556 - val_value_loss: 0.9651 - val_policy_loss: 1.9717\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.5416 - value_loss: 0.9312 - policy_loss: 1.9774 - val_loss: 6.5542 - val_value_loss: 0.9622 - val_policy_loss: 1.9717\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5404 - value_loss: 0.9289 - policy_loss: 1.9775 - val_loss: 6.5526 - val_value_loss: 0.9592 - val_policy_loss: 1.9717\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5370 - value_loss: 0.9228 - policy_loss: 1.9767 - val_loss: 6.5513 - val_value_loss: 0.9565 - val_policy_loss: 1.9716\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.5352 - value_loss: 0.9195 - policy_loss: 1.9765 - val_loss: 6.5500 - val_value_loss: 0.9539 - val_policy_loss: 1.9716\n",
      "Saved model  connectfour_50_31\n",
      "iteration 31 | evaluation\n",
      "agent vs random - win ratio 0.8 - draw ratio 0.0\n",
      "Number of seen trajectories: 3200\n",
      "Number of unique trajectories: 3194\n",
      "iteration 32 | self-play\n",
      "saving memory position_memory_connectfour_50_ep_32\n",
      "iteration 32 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 6, 7, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 7)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 2s 454us/step - loss: 6.5761 - value_loss: 1.0041 - policy_loss: 1.9737 - val_loss: 6.5427 - val_value_loss: 0.9477 - val_policy_loss: 1.9632\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 2s 450us/step - loss: 6.5735 - value_loss: 0.9986 - policy_loss: 1.9740 - val_loss: 6.5406 - val_value_loss: 0.9438 - val_policy_loss: 1.9630\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.5676 - value_loss: 0.9877 - policy_loss: 1.9732 - val_loss: 6.5388 - val_value_loss: 0.9403 - val_policy_loss: 1.9629\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 2s 450us/step - loss: 6.5655 - value_loss: 0.9837 - policy_loss: 1.9728 - val_loss: 6.5371 - val_value_loss: 0.9370 - val_policy_loss: 1.9628\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5627 - value_loss: 0.9785 - policy_loss: 1.9726 - val_loss: 6.5355 - val_value_loss: 0.9341 - val_policy_loss: 1.9626\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.5579 - value_loss: 0.9690 - policy_loss: 1.9724 - val_loss: 6.5341 - val_value_loss: 0.9313 - val_policy_loss: 1.9625\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.5562 - value_loss: 0.9656 - policy_loss: 1.9725 - val_loss: 6.5326 - val_value_loss: 0.9284 - val_policy_loss: 1.9624\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5534 - value_loss: 0.9600 - policy_loss: 1.9725 - val_loss: 6.5312 - val_value_loss: 0.9258 - val_policy_loss: 1.9623\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5502 - value_loss: 0.9539 - policy_loss: 1.9723 - val_loss: 6.5299 - val_value_loss: 0.9233 - val_policy_loss: 1.9622\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5460 - value_loss: 0.9459 - policy_loss: 1.9717 - val_loss: 6.5285 - val_value_loss: 0.9205 - val_policy_loss: 1.9621\n",
      "Saved model  connectfour_50_32\n",
      "iteration 32 | evaluation\n",
      "agent vs random - win ratio 0.8 - draw ratio 0.0\n",
      "Number of seen trajectories: 3300\n",
      "Number of unique trajectories: 3294\n",
      "iteration 33 | self-play\n",
      "saving memory position_memory_connectfour_50_ep_33\n",
      "iteration 33 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 6, 7, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 7)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 2s 454us/step - loss: 6.5664 - value_loss: 0.9802 - policy_loss: 1.9783 - val_loss: 6.5393 - val_value_loss: 0.9278 - val_policy_loss: 1.9765\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 2s 450us/step - loss: 6.5624 - value_loss: 0.9729 - policy_loss: 1.9776 - val_loss: 6.5376 - val_value_loss: 0.9247 - val_policy_loss: 1.9762\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5593 - value_loss: 0.9675 - policy_loss: 1.9769 - val_loss: 6.5361 - val_value_loss: 0.9219 - val_policy_loss: 1.9760\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5578 - value_loss: 0.9641 - policy_loss: 1.9773 - val_loss: 6.5348 - val_value_loss: 0.9195 - val_policy_loss: 1.9758\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5529 - value_loss: 0.9549 - policy_loss: 1.9766 - val_loss: 6.5335 - val_value_loss: 0.9172 - val_policy_loss: 1.9755\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5502 - value_loss: 0.9501 - policy_loss: 1.9760 - val_loss: 6.5323 - val_value_loss: 0.9151 - val_policy_loss: 1.9753\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 2s 453us/step - loss: 6.5473 - value_loss: 0.9440 - policy_loss: 1.9763 - val_loss: 6.5312 - val_value_loss: 0.9130 - val_policy_loss: 1.9752\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5436 - value_loss: 0.9374 - policy_loss: 1.9757 - val_loss: 6.5301 - val_value_loss: 0.9110 - val_policy_loss: 1.9750\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 2s 453us/step - loss: 6.5416 - value_loss: 0.9333 - policy_loss: 1.9756 - val_loss: 6.5290 - val_value_loss: 0.9089 - val_policy_loss: 1.9748\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5385 - value_loss: 0.9274 - policy_loss: 1.9754 - val_loss: 6.5279 - val_value_loss: 0.9070 - val_policy_loss: 1.9747\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model  connectfour_50_33\n",
      "iteration 33 | evaluation\n",
      "agent vs random - win ratio 0.73 - draw ratio 0.0\n",
      "Number of seen trajectories: 3400\n",
      "Number of unique trajectories: 3394\n",
      "iteration 34 | self-play\n",
      "saving memory position_memory_connectfour_50_ep_34\n",
      "iteration 34 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 6, 7, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 7)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 2s 453us/step - loss: 6.5898 - value_loss: 1.0304 - policy_loss: 1.9751 - val_loss: 6.5944 - val_value_loss: 1.0368 - val_policy_loss: 1.9779\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 2s 450us/step - loss: 6.5875 - value_loss: 1.0255 - policy_loss: 1.9754 - val_loss: 6.5930 - val_value_loss: 1.0340 - val_policy_loss: 1.9777\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 2s 454us/step - loss: 6.5837 - value_loss: 1.0185 - policy_loss: 1.9747 - val_loss: 6.5916 - val_value_loss: 1.0314 - val_policy_loss: 1.9776\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5821 - value_loss: 1.0157 - policy_loss: 1.9742 - val_loss: 6.5903 - val_value_loss: 1.0289 - val_policy_loss: 1.9775\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5772 - value_loss: 1.0062 - policy_loss: 1.9740 - val_loss: 6.5891 - val_value_loss: 1.0266 - val_policy_loss: 1.9774\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5752 - value_loss: 1.0024 - policy_loss: 1.9739 - val_loss: 6.5878 - val_value_loss: 1.0242 - val_policy_loss: 1.9773\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5731 - value_loss: 0.9975 - policy_loss: 1.9745 - val_loss: 6.5865 - val_value_loss: 1.0217 - val_policy_loss: 1.9772\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5690 - value_loss: 0.9900 - policy_loss: 1.9738 - val_loss: 6.5853 - val_value_loss: 1.0194 - val_policy_loss: 1.9770\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5663 - value_loss: 0.9852 - policy_loss: 1.9733 - val_loss: 6.5841 - val_value_loss: 1.0171 - val_policy_loss: 1.9769\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5621 - value_loss: 0.9775 - policy_loss: 1.9725 - val_loss: 6.5829 - val_value_loss: 1.0148 - val_policy_loss: 1.9769\n",
      "Saved model  connectfour_50_34\n",
      "iteration 34 | evaluation\n",
      "agent vs random - win ratio 0.86 - draw ratio 0.0\n",
      "Number of seen trajectories: 3500\n",
      "Number of unique trajectories: 3494\n",
      "iteration 35 | self-play\n",
      "saving memory position_memory_connectfour_50_ep_35\n",
      "iteration 35 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 6, 7, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 7)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5896 - value_loss: 1.0307 - policy_loss: 1.9743 - val_loss: 6.5703 - val_value_loss: 0.9887 - val_policy_loss: 1.9778\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.5856 - value_loss: 1.0227 - policy_loss: 1.9744 - val_loss: 6.5695 - val_value_loss: 0.9871 - val_policy_loss: 1.9777\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5854 - value_loss: 1.0224 - policy_loss: 1.9742 - val_loss: 6.5688 - val_value_loss: 0.9858 - val_policy_loss: 1.9776\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5814 - value_loss: 1.0153 - policy_loss: 1.9733 - val_loss: 6.5681 - val_value_loss: 0.9846 - val_policy_loss: 1.9775\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5792 - value_loss: 1.0112 - policy_loss: 1.9732 - val_loss: 6.5675 - val_value_loss: 0.9835 - val_policy_loss: 1.9775\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5791 - value_loss: 1.0104 - policy_loss: 1.9736 - val_loss: 6.5670 - val_value_loss: 0.9824 - val_policy_loss: 1.9774\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5762 - value_loss: 1.0049 - policy_loss: 1.9734 - val_loss: 6.5664 - val_value_loss: 0.9814 - val_policy_loss: 1.9773\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5753 - value_loss: 1.0035 - policy_loss: 1.9730 - val_loss: 6.5659 - val_value_loss: 0.9804 - val_policy_loss: 1.9773\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5735 - value_loss: 0.9995 - policy_loss: 1.9734 - val_loss: 6.5654 - val_value_loss: 0.9794 - val_policy_loss: 1.9772\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5718 - value_loss: 0.9959 - policy_loss: 1.9736 - val_loss: 6.5649 - val_value_loss: 0.9785 - val_policy_loss: 1.9771\n",
      "Saved model  connectfour_50_35\n",
      "iteration 35 | evaluation\n",
      "agent vs random - win ratio 0.77 - draw ratio 0.0\n",
      "Number of seen trajectories: 3600\n",
      "Number of unique trajectories: 3594\n",
      "iteration 36 | self-play\n",
      "saving memory position_memory_connectfour_50_ep_36\n",
      "iteration 36 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 6, 7, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 7)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 2s 453us/step - loss: 6.5918 - value_loss: 1.0366 - policy_loss: 1.9730 - val_loss: 6.5965 - val_value_loss: 1.0406 - val_policy_loss: 1.9783\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 2s 450us/step - loss: 6.5896 - value_loss: 1.0323 - policy_loss: 1.9727 - val_loss: 6.5955 - val_value_loss: 1.0386 - val_policy_loss: 1.9783\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5869 - value_loss: 1.0273 - policy_loss: 1.9723 - val_loss: 6.5946 - val_value_loss: 1.0368 - val_policy_loss: 1.9784\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5861 - value_loss: 1.0261 - policy_loss: 1.9721 - val_loss: 6.5938 - val_value_loss: 1.0351 - val_policy_loss: 1.9784\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5841 - value_loss: 1.0221 - policy_loss: 1.9721 - val_loss: 6.5929 - val_value_loss: 1.0335 - val_policy_loss: 1.9784\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 2s 454us/step - loss: 6.5806 - value_loss: 1.0154 - policy_loss: 1.9717 - val_loss: 6.5922 - val_value_loss: 1.0319 - val_policy_loss: 1.9784\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 2s 453us/step - loss: 6.5790 - value_loss: 1.0125 - policy_loss: 1.9714 - val_loss: 6.5914 - val_value_loss: 1.0303 - val_policy_loss: 1.9784\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5786 - value_loss: 1.0117 - policy_loss: 1.9714 - val_loss: 6.5906 - val_value_loss: 1.0288 - val_policy_loss: 1.9785\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 2s 453us/step - loss: 6.5766 - value_loss: 1.0079 - policy_loss: 1.9713 - val_loss: 6.5899 - val_value_loss: 1.0273 - val_policy_loss: 1.9785\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 2s 464us/step - loss: 6.5748 - value_loss: 1.0044 - policy_loss: 1.9711 - val_loss: 6.5892 - val_value_loss: 1.0259 - val_policy_loss: 1.9785\n",
      "Saved model  connectfour_50_36\n",
      "iteration 36 | evaluation\n",
      "agent vs random - win ratio 0.85 - draw ratio 0.0\n",
      "Number of seen trajectories: 3700\n",
      "Number of unique trajectories: 3694\n",
      "iteration 37 | self-play\n",
      "saving memory position_memory_connectfour_50_ep_37\n",
      "iteration 37 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 6, 7, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 7)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 2s 454us/step - loss: 6.5860 - value_loss: 1.0242 - policy_loss: 1.9739 - val_loss: 6.5801 - val_value_loss: 1.0158 - val_policy_loss: 1.9704\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 2s 450us/step - loss: 6.5851 - value_loss: 1.0224 - policy_loss: 1.9739 - val_loss: 6.5791 - val_value_loss: 1.0140 - val_policy_loss: 1.9703\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5829 - value_loss: 1.0185 - policy_loss: 1.9733 - val_loss: 6.5782 - val_value_loss: 1.0121 - val_policy_loss: 1.9702\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5822 - value_loss: 1.0169 - policy_loss: 1.9734 - val_loss: 6.5773 - val_value_loss: 1.0105 - val_policy_loss: 1.9701\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5801 - value_loss: 1.0127 - policy_loss: 1.9734 - val_loss: 6.5764 - val_value_loss: 1.0087 - val_policy_loss: 1.9700\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 2s 453us/step - loss: 6.5779 - value_loss: 1.0090 - policy_loss: 1.9728 - val_loss: 6.5755 - val_value_loss: 1.0071 - val_policy_loss: 1.9700\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 2s 453us/step - loss: 6.5770 - value_loss: 1.0074 - policy_loss: 1.9726 - val_loss: 6.5747 - val_value_loss: 1.0054 - val_policy_loss: 1.9699\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5749 - value_loss: 1.0031 - policy_loss: 1.9728 - val_loss: 6.5738 - val_value_loss: 1.0039 - val_policy_loss: 1.9698\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5734 - value_loss: 1.0006 - policy_loss: 1.9722 - val_loss: 6.5730 - val_value_loss: 1.0023 - val_policy_loss: 1.9698\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 2s 453us/step - loss: 6.5705 - value_loss: 0.9951 - policy_loss: 1.9720 - val_loss: 6.5721 - val_value_loss: 1.0007 - val_policy_loss: 1.9697\n",
      "Saved model  connectfour_50_37\n",
      "iteration 37 | evaluation\n",
      "agent vs random - win ratio 0.74 - draw ratio 0.0\n",
      "Number of seen trajectories: 3800\n",
      "Number of unique trajectories: 3794\n",
      "iteration 38 | self-play\n",
      "saving memory position_memory_connectfour_50_ep_38\n",
      "iteration 38 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 6, 7, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 7)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 2s 454us/step - loss: 6.5572 - value_loss: 0.9675 - policy_loss: 1.9730 - val_loss: 6.5608 - val_value_loss: 0.9704 - val_policy_loss: 1.9773\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 2s 450us/step - loss: 6.5570 - value_loss: 0.9666 - policy_loss: 1.9735 - val_loss: 6.5600 - val_value_loss: 0.9688 - val_policy_loss: 1.9773\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5557 - value_loss: 0.9642 - policy_loss: 1.9733 - val_loss: 6.5592 - val_value_loss: 0.9671 - val_policy_loss: 1.9774\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 2s 453us/step - loss: 6.5532 - value_loss: 0.9596 - policy_loss: 1.9729 - val_loss: 6.5584 - val_value_loss: 0.9655 - val_policy_loss: 1.9774\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 2s 453us/step - loss: 6.5513 - value_loss: 0.9559 - policy_loss: 1.9727 - val_loss: 6.5576 - val_value_loss: 0.9639 - val_policy_loss: 1.9774\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5506 - value_loss: 0.9544 - policy_loss: 1.9728 - val_loss: 6.5568 - val_value_loss: 0.9623 - val_policy_loss: 1.9774\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 2s 453us/step - loss: 6.5479 - value_loss: 0.9493 - policy_loss: 1.9725 - val_loss: 6.5560 - val_value_loss: 0.9607 - val_policy_loss: 1.9774\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 2s 453us/step - loss: 6.5462 - value_loss: 0.9465 - policy_loss: 1.9721 - val_loss: 6.5552 - val_value_loss: 0.9591 - val_policy_loss: 1.9775\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 2s 453us/step - loss: 6.5442 - value_loss: 0.9421 - policy_loss: 1.9724 - val_loss: 6.5544 - val_value_loss: 0.9575 - val_policy_loss: 1.9775\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 2s 454us/step - loss: 6.5443 - value_loss: 0.9419 - policy_loss: 1.9728 - val_loss: 6.5536 - val_value_loss: 0.9559 - val_policy_loss: 1.9775\n",
      "Saved model  connectfour_50_38\n",
      "iteration 38 | evaluation\n",
      "agent vs random - win ratio 0.74 - draw ratio 0.0\n",
      "Number of seen trajectories: 3900\n",
      "Number of unique trajectories: 3894\n",
      "iteration 39 | self-play\n",
      "saving memory position_memory_connectfour_50_ep_39\n",
      "iteration 39 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 6, 7, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 7)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 2s 454us/step - loss: 6.5635 - value_loss: 0.9791 - policy_loss: 1.9740 - val_loss: 6.5886 - val_value_loss: 1.0231 - val_policy_loss: 1.9802\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 2s 450us/step - loss: 6.5616 - value_loss: 0.9756 - policy_loss: 1.9737 - val_loss: 6.5881 - val_value_loss: 1.0222 - val_policy_loss: 1.9801\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5609 - value_loss: 0.9746 - policy_loss: 1.9734 - val_loss: 6.5875 - val_value_loss: 1.0211 - val_policy_loss: 1.9801\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.5576 - value_loss: 0.9677 - policy_loss: 1.9735 - val_loss: 6.5869 - val_value_loss: 1.0199 - val_policy_loss: 1.9800\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 2s 457us/step - loss: 6.5568 - value_loss: 0.9665 - policy_loss: 1.9732 - val_loss: 6.5861 - val_value_loss: 1.0184 - val_policy_loss: 1.9799\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 2s 460us/step - loss: 6.5544 - value_loss: 0.9619 - policy_loss: 1.9731 - val_loss: 6.5854 - val_value_loss: 1.0170 - val_policy_loss: 1.9799\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 2s 462us/step - loss: 6.5535 - value_loss: 0.9595 - policy_loss: 1.9737 - val_loss: 6.5845 - val_value_loss: 1.0154 - val_policy_loss: 1.9798\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 2s 467us/step - loss: 6.5510 - value_loss: 0.9551 - policy_loss: 1.9730 - val_loss: 6.5837 - val_value_loss: 1.0138 - val_policy_loss: 1.9797\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 2s 464us/step - loss: 6.5488 - value_loss: 0.9513 - policy_loss: 1.9725 - val_loss: 6.5828 - val_value_loss: 1.0120 - val_policy_loss: 1.9797\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 2s 463us/step - loss: 6.5483 - value_loss: 0.9504 - policy_loss: 1.9723 - val_loss: 6.5819 - val_value_loss: 1.0104 - val_policy_loss: 1.9796\n",
      "Saved model  connectfour_50_39\n",
      "iteration 39 | evaluation\n",
      "agent vs random - win ratio 0.68 - draw ratio 0.0\n",
      "Number of seen trajectories: 4000\n",
      "Number of unique trajectories: 3994\n",
      "iteration 40 | self-play\n",
      "saving memory position_memory_connectfour_50_ep_40\n",
      "iteration 40 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 6, 7, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 7)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 2s 453us/step - loss: 6.5693 - value_loss: 0.9901 - policy_loss: 1.9747 - val_loss: 6.5556 - val_value_loss: 0.9574 - val_policy_loss: 1.9798\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 2s 450us/step - loss: 6.5677 - value_loss: 0.9864 - policy_loss: 1.9752 - val_loss: 6.5551 - val_value_loss: 0.9566 - val_policy_loss: 1.9798\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.5663 - value_loss: 0.9843 - policy_loss: 1.9745 - val_loss: 6.5547 - val_value_loss: 0.9557 - val_policy_loss: 1.9798\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5648 - value_loss: 0.9817 - policy_loss: 1.9741 - val_loss: 6.5543 - val_value_loss: 0.9549 - val_policy_loss: 1.9798\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 2s 455us/step - loss: 6.5656 - value_loss: 0.9831 - policy_loss: 1.9744 - val_loss: 6.5539 - val_value_loss: 0.9542 - val_policy_loss: 1.9797\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 2s 454us/step - loss: 6.5641 - value_loss: 0.9799 - policy_loss: 1.9746 - val_loss: 6.5535 - val_value_loss: 0.9535 - val_policy_loss: 1.9797\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 2s 453us/step - loss: 6.5622 - value_loss: 0.9764 - policy_loss: 1.9741 - val_loss: 6.5532 - val_value_loss: 0.9529 - val_policy_loss: 1.9797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5615 - value_loss: 0.9754 - policy_loss: 1.9738 - val_loss: 6.5529 - val_value_loss: 0.9522 - val_policy_loss: 1.9797\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5608 - value_loss: 0.9735 - policy_loss: 1.9743 - val_loss: 6.5525 - val_value_loss: 0.9516 - val_policy_loss: 1.9796\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 2s 453us/step - loss: 6.5603 - value_loss: 0.9726 - policy_loss: 1.9741 - val_loss: 6.5522 - val_value_loss: 0.9510 - val_policy_loss: 1.9796\n",
      "Saved model  connectfour_50_40\n",
      "iteration 40 | evaluation\n",
      "agent vs random - win ratio 0.71 - draw ratio 0.0\n",
      "Number of seen trajectories: 4100\n",
      "Number of unique trajectories: 4094\n",
      "iteration 41 | self-play\n",
      "saving memory position_memory_connectfour_50_ep_41\n",
      "iteration 41 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 6, 7, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 7)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 2s 456us/step - loss: 6.5587 - value_loss: 0.9682 - policy_loss: 1.9754 - val_loss: 6.5821 - val_value_loss: 1.0082 - val_policy_loss: 1.9822\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.5574 - value_loss: 0.9657 - policy_loss: 1.9754 - val_loss: 6.5817 - val_value_loss: 1.0076 - val_policy_loss: 1.9821\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.5567 - value_loss: 0.9645 - policy_loss: 1.9750 - val_loss: 6.5813 - val_value_loss: 1.0068 - val_policy_loss: 1.9820\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5555 - value_loss: 0.9620 - policy_loss: 1.9753 - val_loss: 6.5810 - val_value_loss: 1.0061 - val_policy_loss: 1.9820\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 2s 453us/step - loss: 6.5552 - value_loss: 0.9616 - policy_loss: 1.9749 - val_loss: 6.5806 - val_value_loss: 1.0055 - val_policy_loss: 1.9819\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5542 - value_loss: 0.9595 - policy_loss: 1.9750 - val_loss: 6.5802 - val_value_loss: 1.0048 - val_policy_loss: 1.9818\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5535 - value_loss: 0.9585 - policy_loss: 1.9746 - val_loss: 6.5799 - val_value_loss: 1.0041 - val_policy_loss: 1.9818\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5534 - value_loss: 0.9579 - policy_loss: 1.9751 - val_loss: 6.5795 - val_value_loss: 1.0035 - val_policy_loss: 1.9817\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5522 - value_loss: 0.9558 - policy_loss: 1.9749 - val_loss: 6.5792 - val_value_loss: 1.0028 - val_policy_loss: 1.9817\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5522 - value_loss: 0.9559 - policy_loss: 1.9747 - val_loss: 6.5788 - val_value_loss: 1.0022 - val_policy_loss: 1.9816\n",
      "Saved model  connectfour_50_41\n",
      "iteration 41 | evaluation\n",
      "agent vs random - win ratio 0.72 - draw ratio 0.0\n",
      "Number of seen trajectories: 4200\n",
      "Number of unique trajectories: 4193\n",
      "iteration 42 | self-play\n",
      "saving memory position_memory_connectfour_50_ep_42\n",
      "iteration 42 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 6, 7, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 7)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 2s 455us/step - loss: 6.5518 - value_loss: 0.9581 - policy_loss: 1.9718 - val_loss: 6.5489 - val_value_loss: 0.9545 - val_policy_loss: 1.9696\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 2s 450us/step - loss: 6.5504 - value_loss: 0.9552 - policy_loss: 1.9717 - val_loss: 6.5482 - val_value_loss: 0.9530 - val_policy_loss: 1.9695\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5503 - value_loss: 0.9546 - policy_loss: 1.9722 - val_loss: 6.5474 - val_value_loss: 0.9517 - val_policy_loss: 1.9694\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 2s 453us/step - loss: 6.5498 - value_loss: 0.9539 - policy_loss: 1.9720 - val_loss: 6.5468 - val_value_loss: 0.9504 - val_policy_loss: 1.9693\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 2s 453us/step - loss: 6.5477 - value_loss: 0.9504 - policy_loss: 1.9713 - val_loss: 6.5461 - val_value_loss: 0.9492 - val_policy_loss: 1.9693\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5471 - value_loss: 0.9483 - policy_loss: 1.9720 - val_loss: 6.5455 - val_value_loss: 0.9481 - val_policy_loss: 1.9692\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 2s 453us/step - loss: 6.5458 - value_loss: 0.9464 - policy_loss: 1.9713 - val_loss: 6.5450 - val_value_loss: 0.9470 - val_policy_loss: 1.9692\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5441 - value_loss: 0.9430 - policy_loss: 1.9715 - val_loss: 6.5444 - val_value_loss: 0.9460 - val_policy_loss: 1.9691\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 2s 453us/step - loss: 6.5439 - value_loss: 0.9427 - policy_loss: 1.9713 - val_loss: 6.5439 - val_value_loss: 0.9451 - val_policy_loss: 1.9690\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5435 - value_loss: 0.9421 - policy_loss: 1.9712 - val_loss: 6.5434 - val_value_loss: 0.9441 - val_policy_loss: 1.9690\n",
      "Saved model  connectfour_50_42\n",
      "iteration 42 | evaluation\n",
      "agent vs random - win ratio 0.73 - draw ratio 0.0\n",
      "Number of seen trajectories: 4300\n",
      "Number of unique trajectories: 4293\n",
      "iteration 43 | self-play\n",
      "saving memory position_memory_connectfour_50_ep_43\n",
      "iteration 43 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 6, 7, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 7)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 2s 455us/step - loss: 6.5897 - value_loss: 1.0327 - policy_loss: 1.9729 - val_loss: 6.5837 - val_value_loss: 1.0192 - val_policy_loss: 1.9744\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 2s 453us/step - loss: 6.5889 - value_loss: 1.0316 - policy_loss: 1.9725 - val_loss: 6.5830 - val_value_loss: 1.0179 - val_policy_loss: 1.9743\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 2s 454us/step - loss: 6.5878 - value_loss: 1.0296 - policy_loss: 1.9722 - val_loss: 6.5823 - val_value_loss: 1.0167 - val_policy_loss: 1.9742\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 2s 458us/step - loss: 6.5868 - value_loss: 1.0279 - policy_loss: 1.9720 - val_loss: 6.5817 - val_value_loss: 1.0154 - val_policy_loss: 1.9742\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5860 - value_loss: 1.0261 - policy_loss: 1.9720 - val_loss: 6.5811 - val_value_loss: 1.0142 - val_policy_loss: 1.9741\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5846 - value_loss: 1.0235 - policy_loss: 1.9720 - val_loss: 6.5805 - val_value_loss: 1.0130 - val_policy_loss: 1.9741\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 2s 453us/step - loss: 6.5834 - value_loss: 1.0215 - policy_loss: 1.9717 - val_loss: 6.5799 - val_value_loss: 1.0119 - val_policy_loss: 1.9741\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5832 - value_loss: 1.0208 - policy_loss: 1.9719 - val_loss: 6.5793 - val_value_loss: 1.0108 - val_policy_loss: 1.9740\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5829 - value_loss: 1.0199 - policy_loss: 1.9721 - val_loss: 6.5787 - val_value_loss: 1.0097 - val_policy_loss: 1.9740\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 2s 453us/step - loss: 6.5814 - value_loss: 1.0173 - policy_loss: 1.9717 - val_loss: 6.5782 - val_value_loss: 1.0087 - val_policy_loss: 1.9739\n",
      "Saved model  connectfour_50_43\n",
      "iteration 43 | evaluation\n",
      "agent vs random - win ratio 0.83 - draw ratio 0.0\n",
      "Number of seen trajectories: 4400\n",
      "Number of unique trajectories: 4393\n",
      "iteration 44 | self-play\n",
      "saving memory position_memory_connectfour_50_ep_44\n",
      "iteration 44 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 6, 7, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 7)\n",
      "Train on 4096 samples, validate on 1024 samples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 2s 454us/step - loss: 6.5997 - value_loss: 1.0460 - policy_loss: 1.9796 - val_loss: 6.6144 - val_value_loss: 1.0795 - val_policy_loss: 1.9756\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 2s 450us/step - loss: 6.5985 - value_loss: 1.0435 - policy_loss: 1.9797 - val_loss: 6.6139 - val_value_loss: 1.0786 - val_policy_loss: 1.9755\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.5976 - value_loss: 1.0418 - policy_loss: 1.9798 - val_loss: 6.6134 - val_value_loss: 1.0776 - val_policy_loss: 1.9754\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 2s 453us/step - loss: 6.5965 - value_loss: 1.0401 - policy_loss: 1.9791 - val_loss: 6.6129 - val_value_loss: 1.0768 - val_policy_loss: 1.9753\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5963 - value_loss: 1.0395 - policy_loss: 1.9794 - val_loss: 6.6125 - val_value_loss: 1.0759 - val_policy_loss: 1.9753\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 2s 453us/step - loss: 6.5968 - value_loss: 1.0404 - policy_loss: 1.9795 - val_loss: 6.6120 - val_value_loss: 1.0750 - val_policy_loss: 1.9752\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 2s 453us/step - loss: 6.5940 - value_loss: 1.0350 - policy_loss: 1.9792 - val_loss: 6.6116 - val_value_loss: 1.0742 - val_policy_loss: 1.9752\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5931 - value_loss: 1.0335 - policy_loss: 1.9789 - val_loss: 6.6111 - val_value_loss: 1.0734 - val_policy_loss: 1.9751\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 2s 453us/step - loss: 6.5920 - value_loss: 1.0314 - policy_loss: 1.9789 - val_loss: 6.6107 - val_value_loss: 1.0725 - val_policy_loss: 1.9750\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 2s 453us/step - loss: 6.5927 - value_loss: 1.0328 - policy_loss: 1.9788 - val_loss: 6.6102 - val_value_loss: 1.0717 - val_policy_loss: 1.9750\n",
      "Saved model  connectfour_50_44\n",
      "iteration 44 | evaluation\n",
      "agent vs random - win ratio 0.68 - draw ratio 0.0\n",
      "Number of seen trajectories: 4500\n",
      "Number of unique trajectories: 4493\n",
      "iteration 45 | self-play\n",
      "saving memory position_memory_connectfour_50_ep_45\n",
      "iteration 45 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 6, 7, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 7)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 2s 456us/step - loss: 6.5966 - value_loss: 1.0456 - policy_loss: 1.9738 - val_loss: 6.6160 - val_value_loss: 1.0843 - val_policy_loss: 1.9740\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.5965 - value_loss: 1.0455 - policy_loss: 1.9738 - val_loss: 6.6157 - val_value_loss: 1.0838 - val_policy_loss: 1.9739\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5941 - value_loss: 1.0412 - policy_loss: 1.9733 - val_loss: 6.6155 - val_value_loss: 1.0834 - val_policy_loss: 1.9739\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 2s 453us/step - loss: 6.5927 - value_loss: 1.0381 - policy_loss: 1.9735 - val_loss: 6.6152 - val_value_loss: 1.0829 - val_policy_loss: 1.9738\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 2s 453us/step - loss: 6.5940 - value_loss: 1.0403 - policy_loss: 1.9740 - val_loss: 6.6150 - val_value_loss: 1.0824 - val_policy_loss: 1.9738\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 2s 453us/step - loss: 6.5931 - value_loss: 1.0389 - policy_loss: 1.9737 - val_loss: 6.6147 - val_value_loss: 1.0820 - val_policy_loss: 1.9737\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5923 - value_loss: 1.0377 - policy_loss: 1.9732 - val_loss: 6.6145 - val_value_loss: 1.0815 - val_policy_loss: 1.9737\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 2s 453us/step - loss: 6.5912 - value_loss: 1.0358 - policy_loss: 1.9729 - val_loss: 6.6142 - val_value_loss: 1.0811 - val_policy_loss: 1.9737\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5918 - value_loss: 1.0370 - policy_loss: 1.9729 - val_loss: 6.6140 - val_value_loss: 1.0806 - val_policy_loss: 1.9736\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5908 - value_loss: 1.0353 - policy_loss: 1.9725 - val_loss: 6.6137 - val_value_loss: 1.0801 - val_policy_loss: 1.9736\n",
      "Saved model  connectfour_50_45\n",
      "iteration 45 | evaluation\n",
      "agent vs random - win ratio 0.75 - draw ratio 0.0\n",
      "Number of seen trajectories: 4600\n",
      "Number of unique trajectories: 4592\n",
      "iteration 46 | self-play\n",
      "saving memory position_memory_connectfour_50_ep_46\n",
      "iteration 46 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 6, 7, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 7)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 2s 454us/step - loss: 6.5882 - value_loss: 1.0255 - policy_loss: 1.9771 - val_loss: 6.5869 - val_value_loss: 1.0326 - val_policy_loss: 1.9674\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 2s 450us/step - loss: 6.5890 - value_loss: 1.0273 - policy_loss: 1.9770 - val_loss: 6.5867 - val_value_loss: 1.0321 - val_policy_loss: 1.9675\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5874 - value_loss: 1.0242 - policy_loss: 1.9768 - val_loss: 6.5864 - val_value_loss: 1.0317 - val_policy_loss: 1.9675\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 2s 453us/step - loss: 6.5877 - value_loss: 1.0246 - policy_loss: 1.9770 - val_loss: 6.5862 - val_value_loss: 1.0312 - val_policy_loss: 1.9675\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5858 - value_loss: 1.0212 - policy_loss: 1.9768 - val_loss: 6.5860 - val_value_loss: 1.0308 - val_policy_loss: 1.9675\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5849 - value_loss: 1.0190 - policy_loss: 1.9771 - val_loss: 6.5858 - val_value_loss: 1.0303 - val_policy_loss: 1.9676\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5858 - value_loss: 1.0208 - policy_loss: 1.9770 - val_loss: 6.5856 - val_value_loss: 1.0299 - val_policy_loss: 1.9676\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 2s 453us/step - loss: 6.5841 - value_loss: 1.0175 - policy_loss: 1.9770 - val_loss: 6.5854 - val_value_loss: 1.0294 - val_policy_loss: 1.9676\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5841 - value_loss: 1.0176 - policy_loss: 1.9769 - val_loss: 6.5852 - val_value_loss: 1.0290 - val_policy_loss: 1.9676\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5845 - value_loss: 1.0185 - policy_loss: 1.9767 - val_loss: 6.5849 - val_value_loss: 1.0286 - val_policy_loss: 1.9676\n",
      "Saved model  connectfour_50_46\n",
      "iteration 46 | evaluation\n",
      "agent vs random - win ratio 0.7 - draw ratio 0.0\n",
      "Number of seen trajectories: 4700\n",
      "Number of unique trajectories: 4692\n",
      "iteration 47 | self-play\n",
      "saving memory position_memory_connectfour_50_ep_47\n",
      "iteration 47 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 6, 7, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 7)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 2s 455us/step - loss: 6.5582 - value_loss: 0.9696 - policy_loss: 1.9731 - val_loss: 6.5758 - val_value_loss: 1.0058 - val_policy_loss: 1.9720\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 2s 450us/step - loss: 6.5587 - value_loss: 0.9697 - policy_loss: 1.9739 - val_loss: 6.5756 - val_value_loss: 1.0055 - val_policy_loss: 1.9721\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.5575 - value_loss: 0.9681 - policy_loss: 1.9731 - val_loss: 6.5755 - val_value_loss: 1.0052 - val_policy_loss: 1.9721\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5580 - value_loss: 0.9687 - policy_loss: 1.9735 - val_loss: 6.5754 - val_value_loss: 1.0049 - val_policy_loss: 1.9721\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5565 - value_loss: 0.9660 - policy_loss: 1.9733 - val_loss: 6.5752 - val_value_loss: 1.0046 - val_policy_loss: 1.9722\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5574 - value_loss: 0.9671 - policy_loss: 1.9740 - val_loss: 6.5751 - val_value_loss: 1.0043 - val_policy_loss: 1.9722\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5559 - value_loss: 0.9651 - policy_loss: 1.9730 - val_loss: 6.5749 - val_value_loss: 1.0040 - val_policy_loss: 1.9722\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5555 - value_loss: 0.9641 - policy_loss: 1.9732 - val_loss: 6.5748 - val_value_loss: 1.0036 - val_policy_loss: 1.9723\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 2s 453us/step - loss: 6.5551 - value_loss: 0.9630 - policy_loss: 1.9735 - val_loss: 6.5746 - val_value_loss: 1.0033 - val_policy_loss: 1.9723\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5546 - value_loss: 0.9629 - policy_loss: 1.9727 - val_loss: 6.5745 - val_value_loss: 1.0030 - val_policy_loss: 1.9723\n",
      "Saved model  connectfour_50_47\n",
      "iteration 47 | evaluation\n",
      "agent vs random - win ratio 0.75 - draw ratio 0.0\n",
      "Number of seen trajectories: 4800\n",
      "Number of unique trajectories: 4791\n",
      "iteration 48 | self-play\n",
      "saving memory position_memory_connectfour_50_ep_48\n",
      "iteration 48 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 6, 7, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 7)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 2s 457us/step - loss: 6.5597 - value_loss: 0.9709 - policy_loss: 1.9748 - val_loss: 6.5523 - val_value_loss: 0.9569 - val_policy_loss: 1.9740\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 2s 462us/step - loss: 6.5595 - value_loss: 0.9703 - policy_loss: 1.9749 - val_loss: 6.5522 - val_value_loss: 0.9566 - val_policy_loss: 1.9741\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 2s 463us/step - loss: 6.5590 - value_loss: 0.9694 - policy_loss: 1.9749 - val_loss: 6.5520 - val_value_loss: 0.9562 - val_policy_loss: 1.9742\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 2s 462us/step - loss: 6.5586 - value_loss: 0.9683 - policy_loss: 1.9752 - val_loss: 6.5518 - val_value_loss: 0.9558 - val_policy_loss: 1.9742\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 2s 464us/step - loss: 6.5582 - value_loss: 0.9675 - policy_loss: 1.9752 - val_loss: 6.5517 - val_value_loss: 0.9554 - val_policy_loss: 1.9743\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 2s 463us/step - loss: 6.5591 - value_loss: 0.9693 - policy_loss: 1.9752 - val_loss: 6.5515 - val_value_loss: 0.9550 - val_policy_loss: 1.9743\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 2s 460us/step - loss: 6.5568 - value_loss: 0.9655 - policy_loss: 1.9745 - val_loss: 6.5514 - val_value_loss: 0.9547 - val_policy_loss: 1.9744\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 2s 459us/step - loss: 6.5565 - value_loss: 0.9647 - policy_loss: 1.9746 - val_loss: 6.5512 - val_value_loss: 0.9543 - val_policy_loss: 1.9744\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 2s 458us/step - loss: 6.5558 - value_loss: 0.9632 - policy_loss: 1.9747 - val_loss: 6.5510 - val_value_loss: 0.9539 - val_policy_loss: 1.9744\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 2s 458us/step - loss: 6.5559 - value_loss: 0.9632 - policy_loss: 1.9750 - val_loss: 6.5509 - val_value_loss: 0.9536 - val_policy_loss: 1.9745\n",
      "Saved model  connectfour_50_48\n",
      "iteration 48 | evaluation\n",
      "agent vs random - win ratio 0.7 - draw ratio 0.0\n",
      "Number of seen trajectories: 4900\n",
      "Number of unique trajectories: 4891\n",
      "iteration 49 | self-play\n",
      "saving memory position_memory_connectfour_50_ep_49\n",
      "iteration 49 | optimization\n",
      "num_positions: 5120\n",
      "model_X shape: (5120, 6, 7, 3)\n",
      "model_y_outcomes: (5120,)\n",
      "model_y_probabilities: (5120, 7)\n",
      "Train on 4096 samples, validate on 1024 samples\n",
      "Epoch 1/10\n",
      "4096/4096 [==============================] - 2s 454us/step - loss: 6.5671 - value_loss: 0.9885 - policy_loss: 1.9720 - val_loss: 6.5402 - val_value_loss: 0.9368 - val_policy_loss: 1.9700\n",
      "Epoch 2/10\n",
      "4096/4096 [==============================] - 2s 451us/step - loss: 6.5670 - value_loss: 0.9879 - policy_loss: 1.9723 - val_loss: 6.5402 - val_value_loss: 0.9365 - val_policy_loss: 1.9701\n",
      "Epoch 3/10\n",
      "4096/4096 [==============================] - 2s 453us/step - loss: 6.5664 - value_loss: 0.9875 - policy_loss: 1.9717 - val_loss: 6.5401 - val_value_loss: 0.9362 - val_policy_loss: 1.9702\n",
      "Epoch 4/10\n",
      "4096/4096 [==============================] - 2s 453us/step - loss: 6.5662 - value_loss: 0.9862 - policy_loss: 1.9726 - val_loss: 6.5400 - val_value_loss: 0.9360 - val_policy_loss: 1.9703\n",
      "Epoch 5/10\n",
      "4096/4096 [==============================] - 2s 453us/step - loss: 6.5665 - value_loss: 0.9874 - policy_loss: 1.9720 - val_loss: 6.5399 - val_value_loss: 0.9357 - val_policy_loss: 1.9704\n",
      "Epoch 6/10\n",
      "4096/4096 [==============================] - 2s 453us/step - loss: 6.5663 - value_loss: 0.9866 - policy_loss: 1.9724 - val_loss: 6.5398 - val_value_loss: 0.9355 - val_policy_loss: 1.9705\n",
      "Epoch 7/10\n",
      "4096/4096 [==============================] - 2s 453us/step - loss: 6.5645 - value_loss: 0.9831 - policy_loss: 1.9721 - val_loss: 6.5398 - val_value_loss: 0.9352 - val_policy_loss: 1.9706\n",
      "Epoch 8/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5645 - value_loss: 0.9835 - policy_loss: 1.9718 - val_loss: 6.5397 - val_value_loss: 0.9350 - val_policy_loss: 1.9707\n",
      "Epoch 9/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5644 - value_loss: 0.9833 - policy_loss: 1.9719 - val_loss: 6.5396 - val_value_loss: 0.9347 - val_policy_loss: 1.9707\n",
      "Epoch 10/10\n",
      "4096/4096 [==============================] - 2s 452us/step - loss: 6.5636 - value_loss: 0.9817 - policy_loss: 1.9719 - val_loss: 6.5395 - val_value_loss: 0.9345 - val_policy_loss: 1.9708\n",
      "Saved model  connectfour_50_49\n",
      "iteration 49 | evaluation\n",
      "agent vs random - win ratio 0.77 - draw ratio 0.0\n",
      "Number of seen trajectories: 5000\n",
      "Number of unique trajectories: 4990\n"
     ]
    }
   ],
   "source": [
    "wins, draws, seen_trajectories, unique_trajectories = az_pipeline.run(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wins: [0.66, 0.66, 0.69, 0.72, 0.7, 0.74, 0.64, 0.71, 0.71, 0.77, 0.71, 0.79, 0.64, 0.65, 0.73, 0.7, 0.73, 0.7, 0.64, 0.75, 0.69, 0.67, 0.73, 0.68, 0.79, 0.8, 0.75, 0.79, 0.76, 0.71, 0.85, 0.8, 0.8, 0.73, 0.86, 0.77, 0.85, 0.74, 0.74, 0.68, 0.71, 0.72, 0.73, 0.83, 0.68, 0.75, 0.7, 0.75, 0.7, 0.77]\n",
      "draws: [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "seen_trajectories: [ 100.  200.  300.  400.  500.  600.  700.  800.  900. 1000. 1100. 1200.\n",
      " 1300. 1400. 1500. 1600. 1700. 1800. 1900. 2000. 2100. 2200. 2300. 2400.\n",
      " 2500. 2600. 2700. 2800. 2900. 3000. 3100. 3200. 3300. 3400. 3500. 3600.\n",
      " 3700. 3800. 3900. 4000. 4100. 4200. 4300. 4400. 4500. 4600. 4700. 4800.\n",
      " 4900. 5000.]\n",
      "unique_trajectories: [ 100.  200.  300.  400.  500.  600.  700.  800.  900. 1000. 1100. 1200.\n",
      " 1300. 1400. 1500. 1600. 1700. 1800. 1900. 2000. 2100. 2200. 2299. 2399.\n",
      " 2499. 2598. 2698. 2798. 2898. 2997. 3096. 3194. 3294. 3394. 3494. 3594.\n",
      " 3694. 3794. 3894. 3994. 4094. 4193. 4293. 4393. 4493. 4592. 4692. 4791.\n",
      " 4891. 4990.]\n"
     ]
    }
   ],
   "source": [
    "print(\"wins: {}\" .format(wins))\n",
    "print(\"draws: {}\" .format(draws))\n",
    "print(\"seen_trajectories: {}\" .format(seen_trajectories))\n",
    "print(\"unique_trajectories: {}\" .format(unique_trajectories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model connectfour_50_34\n"
     ]
    }
   ],
   "source": [
    "best_version = 34\n",
    "\n",
    "position_memory = memory.PositionMemory(variant=\"Connect4\")\n",
    "best_model = model.AZModel(\n",
    "    memory=position_memory,\n",
    "    input_shape=[6,7,3],\n",
    "    num_possible_moves=7,\n",
    "    model_id=\"connectfour_50\"\n",
    ")\n",
    "best_model.load(best_version)\n",
    "best_agent = agent.AlphaZeroAgent(variant=\"Connect4\", model=best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========\n",
      " 0 | 0 | 0 | 0 | 0 | 0 | 0\n",
      " 0 | 0 | 0 | 0 | 0 | 0 | 0\n",
      " 0 | 0 | 0 | 0 | 0 | 0 | 0\n",
      " 0 | 0 | 0 | 0 | 0 | 0 | 0\n",
      " 0 | 0 | 0 | 0 | 0 | 0 | 0\n",
      " 0 | 0 | 1 | 0 | 0 | 0 | 0\n",
      "-----------\n",
      "value: 0.3631991446018219\n",
      "policy:\n",
      " 0.14375802874565125 | 0.12425162643194199 | 0.12682002782821655 | 0.15876691043376923 | 0.15969540178775787 | 0.15325532853603363 | 0.1334526240825653\n",
      "===========\n",
      "choose from [1 1 1 1 1 1 1] :3\n",
      "===========\n",
      " 0 | 0 | 0 | 0 | 0 | 0 | 0\n",
      " 0 | 0 | 0 | 0 | 0 | 0 | 0\n",
      " 0 | 0 | 0 | 0 | 0 | 0 | 0\n",
      " 0 | 0 | 0 | 0 | 0 | 0 | 0\n",
      " 0 | 0 | 0 | 0 | 0 | 0 | 0\n",
      " 0 | 0 | 1 | 2 | 0 | 0 | 0\n",
      "-----------\n",
      "value: -0.09269339591264725\n",
      "policy:\n",
      " 0.12907880544662476 | 0.1395495980978012 | 0.1397915482521057 | 0.12950707972049713 | 0.16295909881591797 | 0.15916188061237335 | 0.13995197415351868\n",
      "===========\n",
      "===========\n",
      " 0 | 0 | 0 | 0 | 0 | 0 | 0\n",
      " 0 | 0 | 0 | 0 | 0 | 0 | 0\n",
      " 0 | 0 | 0 | 0 | 0 | 0 | 0\n",
      " 0 | 0 | 0 | 0 | 0 | 0 | 0\n",
      " 0 | 0 | 0 | 0 | 0 | 0 | 0\n",
      " 0 | 0 | 1 | 2 | 0 | 1 | 0\n",
      "-----------\n",
      "value: -0.06487923115491867\n",
      "policy:\n",
      " 0.1358793079853058 | 0.11912351101636887 | 0.14296263456344604 | 0.13913631439208984 | 0.151005357503891 | 0.15601448714733124 | 0.15587842464447021\n",
      "===========\n",
      "choose from [1 1 1 1 1 1 1] :3\n",
      "===========\n",
      " 4 | 0 | 0 | 0 | 0 | 0 | 0\n",
      " 0 | 0 | 0 | 0 | 0 | 0 | 0\n",
      " 0 | 0 | 0 | 0 | 0 | 0 | 0\n",
      " 0 | 0 | 0 | 0 | 0 | 0 | 0\n",
      " 0 | 0 | 0 | 2 | 0 | 0 | 0\n",
      " 0 | 0 | 1 | 2 | 0 | 1 | 0\n",
      "-----------\n",
      "value: 0.30451500415802\n",
      "policy:\n",
      " 0.08110073208808899 | 0.2240019142627716 | 0.07834719121456146 | 0.15547601878643036 | 0.141428142786026 | 0.15472237765789032 | 0.16492362320423126\n",
      "===========\n",
      "===========\n",
      " 4 | 0 | 0 | 0 | 0 | 0 | 0\n",
      " 0 | 0 | 0 | 0 | 0 | 0 | 0\n",
      " 0 | 0 | 0 | 0 | 0 | 0 | 0\n",
      " 0 | 0 | 0 | 0 | 0 | 0 | 0\n",
      " 0 | 0 | 1 | 2 | 0 | 0 | 0\n",
      " 0 | 0 | 1 | 2 | 0 | 1 | 0\n",
      "-----------\n",
      "value: 0.4527851939201355\n",
      "policy:\n",
      " 0.10703794658184052 | 0.07414735108613968 | 0.11433795839548111 | 0.1929786503314972 | 0.07973656058311462 | 0.20426321029663086 | 0.22749823331832886\n",
      "===========\n",
      "choose from [1 1 1 1 1 1 1] :3\n",
      "===========\n",
      " 4 | 0 | 0 | 0 | 0 | 0 | 0\n",
      " 0 | 0 | 0 | 0 | 0 | 0 | 0\n",
      " 0 | 0 | 0 | 0 | 0 | 0 | 0\n",
      " 0 | 0 | 0 | 2 | 0 | 0 | 0\n",
      " 0 | 0 | 1 | 2 | 0 | 0 | 0\n",
      " 0 | 0 | 1 | 2 | 0 | 1 | 0\n",
      "-----------\n",
      "value: 0.27419593930244446\n",
      "policy:\n",
      " 0.05971667915582657 | 0.3696768283843994 | 0.04531409963965416 | 0.3297809660434723 | 0.08038751780986786 | 0.03600728139281273 | 0.07911662012338638\n",
      "===========\n",
      "===========\n",
      " 4 | 0 | 0 | 0 | 0 | 0 | 0\n",
      " 0 | 0 | 0 | 0 | 0 | 0 | 0\n",
      " 0 | 0 | 0 | 1 | 0 | 0 | 0\n",
      " 0 | 0 | 0 | 2 | 0 | 0 | 0\n",
      " 0 | 0 | 1 | 2 | 0 | 0 | 0\n",
      " 0 | 0 | 1 | 2 | 0 | 1 | 0\n",
      "-----------\n",
      "value: 0.3137027323246002\n",
      "policy:\n",
      " 0.09684579074382782 | 0.10122561454772949 | 0.095303475856781 | 0.4845963418483734 | 0.035771287977695465 | 0.09782591462135315 | 0.08843150734901428\n",
      "===========\n",
      "choose from [1 1 1 1 1 1 1] :2\n",
      "===========\n",
      " 4 | 0 | 0 | 0 | 0 | 0 | 0\n",
      " 0 | 0 | 0 | 0 | 0 | 0 | 0\n",
      " 0 | 0 | 0 | 1 | 0 | 0 | 0\n",
      " 0 | 0 | 2 | 2 | 0 | 0 | 0\n",
      " 0 | 0 | 1 | 2 | 0 | 0 | 0\n",
      " 0 | 0 | 1 | 2 | 0 | 1 | 0\n",
      "-----------\n",
      "value: -0.1461183875799179\n",
      "policy:\n",
      " 0.049753058701753616 | 0.3082996606826782 | 0.03216436505317688 | 0.36835941672325134 | 0.07480739057064056 | 0.1450871378183365 | 0.021529005840420723\n",
      "===========\n",
      "===========\n",
      " 4 | 0 | 0 | 0 | 0 | 0 | 0\n",
      " 0 | 0 | 0 | 1 | 0 | 0 | 0\n",
      " 0 | 0 | 0 | 1 | 0 | 0 | 0\n",
      " 0 | 0 | 2 | 2 | 0 | 0 | 0\n",
      " 0 | 0 | 1 | 2 | 0 | 0 | 0\n",
      " 0 | 0 | 1 | 2 | 0 | 1 | 0\n",
      "-----------\n",
      "value: -0.7510396242141724\n",
      "policy:\n",
      " 0.13531780242919922 | 0.08240801095962524 | 0.16255079209804535 | 0.14580921828746796 | 0.12270783632993698 | 0.08347608149051666 | 0.26773032546043396\n",
      "===========\n",
      "choose from [1 1 1 1 1 1 1] :4\n",
      "===========\n",
      " 4 | 0 | 0 | 0 | 0 | 0 | 0\n",
      " 0 | 0 | 0 | 1 | 0 | 0 | 0\n",
      " 0 | 0 | 0 | 1 | 0 | 0 | 0\n",
      " 0 | 0 | 2 | 2 | 0 | 0 | 0\n",
      " 0 | 0 | 1 | 2 | 0 | 0 | 0\n",
      " 0 | 0 | 1 | 2 | 2 | 1 | 0\n",
      "-----------\n",
      "value: -0.01486621517688036\n",
      "policy:\n",
      " 0.03084270842373371 | 0.2914087772369385 | 0.01795629784464836 | 0.39711248874664307 | 0.03621784970164299 | 0.19753915071487427 | 0.028922664001584053\n",
      "===========\n",
      "===========\n",
      " 4 | 0 | 0 | 0 | 0 | 0 | 0\n",
      " 0 | 0 | 0 | 1 | 0 | 0 | 0\n",
      " 0 | 0 | 0 | 1 | 0 | 0 | 0\n",
      " 0 | 0 | 2 | 2 | 0 | 0 | 0\n",
      " 0 | 0 | 1 | 2 | 0 | 0 | 0\n",
      " 1 | 0 | 1 | 2 | 2 | 1 | 0\n",
      "-----------\n",
      "value: -0.896108090877533\n",
      "policy:\n",
      " 0.22118344902992249 | 0.08943185955286026 | 0.18683059513568878 | 0.12711387872695923 | 0.10095945000648499 | 0.1125282421708107 | 0.16195252537727356\n",
      "===========\n",
      "choose from [1 1 1 1 1 1 1] :4\n",
      "===========\n",
      " 6 | 0 | 0 | 0 | 0 | 0 | 0\n",
      " 0 | 0 | 0 | 1 | 0 | 0 | 0\n",
      " 0 | 0 | 0 | 1 | 0 | 0 | 0\n",
      " 0 | 0 | 2 | 2 | 0 | 0 | 0\n",
      " 0 | 0 | 1 | 2 | 2 | 0 | 0\n",
      " 1 | 0 | 1 | 2 | 2 | 1 | 0\n",
      "-----------\n",
      "value: 0.2498520314693451\n",
      "policy:\n",
      " 0.08459263294935226 | 0.2736569046974182 | 0.011977851390838623 | 0.12286945432424545 | 0.025290202349424362 | 0.4009174704551697 | 0.0806954950094223\n",
      "===========\n",
      "===========\n",
      " 6 | 0 | 0 | 0 | 0 | 0 | 0\n",
      " 0 | 0 | 0 | 1 | 0 | 0 | 0\n",
      " 0 | 0 | 1 | 1 | 0 | 0 | 0\n",
      " 0 | 0 | 2 | 2 | 0 | 0 | 0\n",
      " 0 | 0 | 1 | 2 | 2 | 0 | 0\n",
      " 1 | 0 | 1 | 2 | 2 | 1 | 0\n",
      "-----------\n",
      "value: -0.8333279490470886\n",
      "policy:\n",
      " 0.08281993120908737 | 0.19201040267944336 | 0.1395990550518036 | 0.11393748223781586 | 0.07240036875009537 | 0.27136409282684326 | 0.12786865234375\n",
      "===========\n",
      "choose from [1 1 1 1 1 1 1] :4\n",
      "===========\n",
      " 6 | 0 | 0 | 0 | 0 | 0 | 0\n",
      " 0 | 0 | 0 | 1 | 0 | 0 | 0\n",
      " 0 | 0 | 1 | 1 | 0 | 0 | 0\n",
      " 0 | 0 | 2 | 2 | 2 | 0 | 0\n",
      " 0 | 0 | 1 | 2 | 2 | 0 | 0\n",
      " 1 | 0 | 1 | 2 | 2 | 1 | 0\n",
      "-----------\n",
      "value: 1.0\n",
      "policy:\n",
      " 1.0 | 5.025530021301632e-18 | 1.1615816404963365e-33 | 1.6973872246427936e-39 | 2.0391567199872536e-29 | 2.5158960891319726e-25 | 1.9044892329844743e-09\n",
      "===========\n",
      "===========\n",
      " 6 | 0 | 0 | 0 | 0 | 0 | 0\n",
      " 0 | 0 | 0 | 1 | 0 | 0 | 0\n",
      " 0 | 0 | 1 | 1 | 0 | 0 | 0\n",
      " 0 | 0 | 2 | 2 | 2 | 0 | 0\n",
      " 1 | 0 | 1 | 2 | 2 | 0 | 0\n",
      " 1 | 0 | 1 | 2 | 2 | 1 | 0\n",
      "-----------\n",
      "value: -0.0862157791852951\n",
      "policy:\n",
      " 1.0 | 1.1860471310448709e-18 | 2.2730550511956702e-23 | 0.0 | 3.502924019110676e-36 | 1.3906343323330706e-18 | 1.172757993986941e-24\n",
      "===========\n",
      "choose from [1 1 1 1 1 1 1] :4\n",
      "===========\n",
      " 6 | 0 | 0 | 0 | 0 | 0 | 0\n",
      " 0 | 0 | 0 | 1 | 0 | 0 | 0\n",
      " 0 | 0 | 1 | 1 | 2 | 0 | 0\n",
      " 0 | 0 | 2 | 2 | 2 | 0 | 0\n",
      " 1 | 0 | 1 | 2 | 2 | 0 | 0\n",
      " 1 | 0 | 1 | 2 | 2 | 1 | 0\n",
      "-----------\n",
      "value: 1.0\n",
      "policy:\n",
      " 1.0 | 6.941047055041704e-18 | 6.76485703132199e-34 | 1.510493245858864e-39 | 7.642186315825771e-30 | 1.2148455475213253e-25 | 1.1824025092366242e-09\n",
      "===========\n",
      "Player -1 won the game after 16 turns.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_vs_player(best_agent, variant=\"Connect4\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
